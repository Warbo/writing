\documentclass{article}
\usepackage{cite}
\begin{document}
The problem of making machine-generated artefacts more understandable to human
users is quite clear in the field of \emph{automated theorem
  proving}~\cite{sutcliffe2001evaluating}: the task of deciding whether a given
formal statement follows from a given set of premeses.  The least informative
approaches, such algorithms based on the popular \emph{resolution}
rule~\cite{robinson1965machine}, produce nothing more than a ``yes'', ``no'' or
``unknown'' response. Not only is this devoid of \emph{explanation}, but it also
hides the effects of any bugs; requiring the user to either trust the results,
or verify the implementation.

Both of these issues can be mitigated by having the system instead generate a
\emph{proof object}: a formal argument for \emph{why} a given statement follows
or does not follow. Once generated, a proof object's validity can be checked
without requiring any knowledge of how it was created, thus avoiding the need to
trust or verify the (potentially complicated) search and generation
procedures. The \emph{de Bruijn criterion}~\cite{barendregt2005challenge}
describes theorem provers which produce proof objects that are trivial to check
by independent \emph{proof checker} programs (which are themselves easily
verified, due to their simplicity); examples are Coq~\cite{barras1997coq} and
Isabelle~\cite{nipkow2002isabelle}.

Proof objects are not a complete solution to understandability, since they can
still be quite inscrutable to human users. This often depends on how closely the
chosen formal system is able to encode the user's ideas: for example, the formal
proof of the Kepler Conjecture was performed using a system of Higher Order
Logic (HOL)~\cite{hales2015formal} whose proof objects (natural-deduction style
derivations), whilst tedious, are in principle understandable to a user
experienced with both the software and problem domain. The same cannot be said
of the Boolean Pythagorean Triples problem, a statement of Ramsey theory
involving the structure of the natural numbers. Rather than taking a high-level
approach like HOL, the authors of~\cite{heule2016solving} analysed sets
$\{0..n\}$ for larger and larger $n$, encoding these restricted versions of the
problem into the language of boolean satisfiability (SAT), and found that the
problem is unsatisfiable for $n= 7825$, and hence for the natural numbers as a
whole. In this case, the proof object demonstrates this unsatisfiability using
200 terabytes of propositional logic clauses (compressable to 68 gigabytes).
Not only is this far too much for any human to comprehend, but the concepts used
in the proof (boolean formulae) are several layers removed from the actual
problem statement (natural numbers, subsets and pythagorean triples).

Whilst ``low level'' formalisms like SAT are less understandable or explanatory
for users, they are far more amenable to automation than more expressive logics.
Despite the proof for the Boolean Pythagorean Triples problem being many orders
of magnitude larger than that of the Kepler Conjecture, the latter is well
beyond the ability of today's automated theorem provers due to its encoding in
HOL. Instead, it took 22 collaborators 9 years just to formalise the proof
(Hales had previously produced a less formal proof, hundreds of pages long and
accompanied by unverified software; a reminder that human-generated artefacts
are not necessarily understandable either!).

A similar problem of understandability has emerged in the field of \emph{machine
  learning} (ML), where statistical methods are used to make predictions or
decisions about data, often without human
involvement~\cite{alpaydin2014introduction}. In particular there is a tradeoff
between the generality of an approach and how easily its resulting behaviour can
be understood.

Many ML approaches can be characterised as constructing a computer program (or
``model'') consisting of two parts: an overall structure or \emph{architecture},
which remains fixed; and a set of adjustable \emph{parameters}, which are
inferred or ``learned'' from data (e.g. \emph{training data} of desired
input/output examples). One particularly simple architecture is the
\emph{decision tree}: nested boolean queries of the input, often used for
\emph{classification}~\cite{safavian1991survey}. These queries are parameters,
and are chosen based on how efficiently they separate the classes given in the
training data.  Decision trees are nominally understandable, since their
behaviour on a given input traces a single path through these queries; yet they
perform poorly compared to other ML algorithms. The \emph{random forest}
approach yields better results by combining many decision trees and having them
vote on the overall outcome~\cite{randomforests}, although such ensemble
behaviour is more difficult to reason about than that of a single tree.

Recent research has focused on highly expressive classes of models such as
\emph{differentiable programming}~\cite{wang2018demystifying}, whose
architectures output not only a (numerical) answer, but also partial derivatives
with respect to the parameters; and \emph{probabilistic programming}, which
samples from a \emph{distribution} conditioned on the training data. Both
frameworks allow arbitrary architectures, specified via Turing-complete
languages, and provide efficient, composable methods for optimising the
parameters (e.g. Stochastic Gradient Descent and Markov Chain Monte Carlo,
respectively) to minimise arbitrary loss functions (e.g. the squared output
error on a training data set).

With such expressive formalisms, the conflict between the generality of a model
and its understandability becomes clear. Task-specific architectures require
fewer parameters than general-purpose approaches and perform well with little
training data, for example hand-written characters can be classified based on a
single example if we allow our model to assume the given images are generated by
pen strokes~\cite{lake2015human}. Likewise we can infer parameters of a 3D scene
(such as object position and lighting) from images if we embed a ray-tracer into
our model~\cite{li2018differentiable}; embedding a physics engine can provide
predictions about these scenes, which are useful e.g. for robot
controllers~\cite{degrave2016differentiable}.

The parameters of such programs are easily understood: they describe the pen
strokes, geometric and physical properties of the simulated system, inferred
from the given images. However, such specificity makes these implementations
unsuitable for anything else. The choice of such high-level, task-specific
components is performed by the user, and encodes some of their domain knowledge
into the structure of the solution, such that it doesn't have to be learned.
This is similar to how logics like HOL can directly represent high-level
concepts (e.g. natural numbers and sets) but whose proof methods have limited
reusability due to the difficulty of automating such high-level reasoning.

At the other end of the spectrum we have differentiable programs with general
purpose architectures, like \emph{neural networks} (capable of universal
function approximation~\cite{funahashi1989approximate}). These are compositions
of a large number of identical sub-expressions (``neurons''), whose parameters
(``weights'') scale the input values, and hence the contribution of each
sub-expression to the whole. Such architectures encode essentially no domain
knowledge, requiring much larger training sets than task-specific algorithms in
order to ``learn'' these details.  The behaviour of general purpose models
depends so heavily on their (many) parameters, that understanding their
behaviour becomes difficult and they are often treated as inscrutable ``black
boxes''; akin to the large (un)SAT proof described above.

Full generality is seldom required: even if we don't want to constrain a model's
internal operation, the input and output data formats will always be known. Such
knowledge can be encoded in the architecture through the use of
\emph{convolutional layers}, for example forcing a 2D grid structure when
dealing with images~\cite{krizhevsky2012imagenet}. Introducing task-specific
components like this can aid in understanding the resulting models, since they
are constrained to operate on 2D patches of image content (like shapes or
textures), rather than potentially arbitrary numerical relationships between
pixels.

Understanding exactly how these programs make their decisions is an active area
of research, known as \emph{explainable artificial intelligence}
(XAI)~\cite{dovsilovic2018explainable,doshi2017towards,molnar2018interpretable},
with popular approaches such as saliency maps~\cite{simonyan2013deep} tending to
reverse-engineer the factors which lead to a model's decisions (judging the
saliency of each pixel based on how strongly it would effect the output
prediction if adjusted). These methods appear intuitive, e.g. producing
visualisations highlighting a particular object in a scene as the reason for its
classification; akin to the proof objects emitted by theorem provers. Yet this
can obscure the difficulty of interpreting such high-dimensional decision
boundaries. In particular, these explanations or justifications can be
fundamentally altered by imperceptible adjustments to the
input~\cite{ghorbani2017interpretation}. Similar adjustments can also change a
model's output, leading to the field of \emph{adversarial machine
  learning}~\cite{goodfellow2014explaining}; adjustments to even a single pixel
can not only cause a system to mislabel an input, but to give high confidence to
its erroneous result~\cite{su2019one}. Such fragility and opaqueness has
important implications as these techniques begin to have decision making
applications in life-critical domains such as automotive
control~\cite{bojarski2016end} and medical
diagnosis~\cite{esteva2017dermatologist}.

\bibliographystyle{plain}
\bibliography{./bibtex}

\end{document}

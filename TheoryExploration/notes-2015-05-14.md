---
title: Meeting notes 2015-05-14
---

# Participants #

 - Katya
 - Chris
 - Moa

# Discussion #

The current state of QuickSpec 2 and "ML4HS" (machine learning for Haskell code) were discussed.

## QuickSpec 2 ##

 - The use of the Knuth-Bendix algorithm has sped up the generation of conjectures, so the system scales much better than QuickSpec 1
 - There are now "too many" conjectures!
    - We may want to filter out some "less interesting" conjectures
    - We may be missing some generalisation
     - Perhaps it's conditional, eg. 'when x is non-empty...'?
     - Perhaps a generalisation has been missed? This can happen due to a timeout in running Knuth-Bendix

It was suggested that machine learning could be applied to these problems; perhaps even in a supervised way, either by asking the user to label the equations as interesting or not, or (as has previously been suggested), by data mining existing QuickCheck tests.

It was noted that QS2 is harder to set up than QS1 at the moment, since it involves checking out non-master branches of GitHub repos and chasing down dependencies. QS1 is on Hackage, so `cabal install quickspec` does the job.

## "ML4HS" ##

The current state of the project is the following:

### HS2AST ###

This uses GHC to parse and uniquely-rename Haskell code. Haskell filenames are provided on the commandline and s-expressions of their ASTs are written to files, one per definition, in a directory hierarchy of the form `packageName/moduleName/valueName`.

This is working fine, although might require some tweaking as everything's plugged together. In particular, the output can be altered by editing `Sexpr.hs` in the following ways:

 - To remove specific types of sub-tree, add the root's type to `excludedTypes`
 - To remove nodes from the tree whilst keeping their children, add the type to `unwrapTypes`
 - To change the way a particular type of node is processed, extend the `strConstr` function using `extQ` and a type-specific specialisation

### TreeFeatures ###

This is an existing tree-based feature extraction program, which has been ported from using XML to using s-expressions. Data is read from stdin and a binary feature vector is written comma-separated to stdout.

The current algorithm has no knowledge of Haskell whatsoever. It assigns feature vectors to leaves based on the MD5 hash of their contents, and assigns feature vectors to nodes based on the circular convolution of their children. The output of the program is the feature vector of the root node.

This algorithm has the effect of sending identical trees to identical feature vectors, whilst sending different trees to orthogonal feature vectors with high probability. We could achieve a similar outcome by just taking the MD5 hash of the s-expression directly (modulo the desired feature vector length), although the accumulation of errors in the tree-based method should make collisions more likely for large trees which only differ far from the root.

This should be the first component to replace once the whole pipeline is running.

### Clustering ###

Here the existing calls to Weka can be used from ML4PG, we just need to write the CSV data appropriately.

### Controlling QuickSpec ###

The clusters we get from Weka need to be turned into theories for *Spec. It's been suggested to look at the way Hipster does this, sending the data to HipSpec which sends it on to QuickSpec.

### Evaluation ###

The current plan is to evaluate the combined running time of ML4HS (across all clusters) and compare it to that of QuickSpec. The proportion of equations discovered by ML4HS will be strictly less than QuickSpec, since QS is complete; hence we can obtain an objective measure of the tradeoff between time and completeness. For now, QS 1 will be used, since that's easier to install and it's the version that's known to work with Hipster/HipSpec.

It was pointed out that if the problem with QS2 is that we get *too many* conjectures, we might want to consider evaluating the *quality* of the generated equations, to see if the "less interesting" ones are being skipped. This is more difficult to measure, as it is subjective, so some further thought needs to be given if we're to apply machine learning to equations rather than theories.

## Immediate Work ##

 - Finish off the skeletal ML4HS implementation
    - Show to Katya and Moa, so everyone is up to speed on what we have and what it does
    - Demo at the Dundee SoC PhD Symposium on 2015-05-25
 - Start keeping these detailed notes of meeting outcomes, including technical details, etc.
 - Document a "best case" for the ML4HS system, ie. *exactly* what we're trying to do. That way, we make tradeoffs and decisions explicit, and we can evaluate whether we've achieved what we set out to.

## Future Work ##

It seems sensible to present something at the AI4FM conference in Edinburgh, on 2015-08-31. When the pipeline is working we can decide what that will be.

Other, more substantial collaboration with Chalmers would probably aim for early 2016 deadlines.

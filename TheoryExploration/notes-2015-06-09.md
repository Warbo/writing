---
title: Theory Exploration notes 2015-06-09
---

What are our aims?

 - To increase the output rate of QuickSpec, ie. equations/time
 - To increase the average "quality" of equations produced, ie. if we sacrifice completeness, we prefer to lose "uninteresting" equations

What are our hypotheses?

 - Performing exhaustive search on sub-sets of definitions has a higher total output rate than on whole libraries
 - Structural similarity between functions (eg. both recursive on their first argument) makes them more likely to be related by equational properties.
 - Properties relating functions are more likely to be found when "similar" functions are known to be related (eg. "depth myTree = depth (fmap f myTree)" given "length myList = length (map f myList)")

What can we measure?

 - The output rate of QuickSpec over time. Does it tail off? Does it go up?
 - The output rate of QuickSpec versus library size. Is there a "sweet spot"?
 - Profile the time spent by QuickSpec. Is it mainly generation, equality, ...?
 - The output rate of QuickSpec versus search depth. Does generation tail off, remain constant, etc.?
 - QuickSpec's memory usage versus time, search depth and library size. Are we forced to stop by hardware limitations, or could we let it keep going forever?

How can we determine "quality"? Perhaps by *using* the equations, eg. by providing them to an SMT solver.

For example, SMT-Lib provides many equational theories. We could model some of these as Haskell libraries (CHECK: has anyone done this aready?) and see which equations are found. This isn't *directly* comparable to the known properties of the SMT-Lib theory, since QuickSpec tries to produce a minimal set.

Instead, we can give the equations we've discovered to an SMT solver like z3, and see how many of the SMT-Lib properties it can prove given our equations. We can measure the "quality" of each equation by how if affects the z3 results (what is proven, how long it takes) when that equation is removed.

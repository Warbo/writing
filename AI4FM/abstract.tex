\documentclass[]{eceasst}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% \ifxetex
%   \usepackage[setpagesize=false, % page size defined by xetex
%               unicode=false, % unicode breaks when used with xetex
%               xetex]{hyperref}
% \else
%   \usepackage[unicode=true]{hyperref}
% \fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Chris Warburton},
            pdftitle={Scaling Automated Theory Exploration},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

\title{Scaling Automated Theory Exploration}
\author{Chris Warburton}
\date{}
\usepackage[T1]{fontenc}
\usepackage{upquote}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\begin{document}
\maketitle

\begin{abstract}

In this paper we investigate the \textbf{theory exploration} (TE)
paradigm for computer-assisted Mathematics and identify limitations and
improvements of current approaches. Unlike the theorem-proving paradigm,
which requires user-provided conjectures, TE performs an open-ended
search for theorems satisfying given criteria. We see promise in TE for
identifying new abstractions and connections in libraries of software
and proofs, but realising this potential requires more scalable
algorithms than presently used.

\end{abstract}

\section{Introduction}\label{introduction}

The \emph{theory exploration} (TE) paradigm provides software support
for traditional Mathematical workflows; namely, deriving ``interesting''
consequences from formal definitions \cite{RISC1482}. Early
implementations like \textsc{Theorema} \cite{buchberger2000theory} emphasised
interactivity, in a similar way to computer algebra systems like
\textsc{Mathematica} (in which \textsc{Theorema} is implemented) or
interactive theorem provers. Subsequent systems have investigated
\emph{automated} theory exploration, for tasks such as lemma discovery
\cite{Hipster}.

By removing user interaction, automated TE systems require an algorithm
for deciding whether to inform the user of a particular theorem or not; we refer
to this search criterion as the theorem being ``interesting''.

In existing systems, this criterion is intimately connected to the
search algorithm; this coupling improves efficiency, even for brute-force
search. As an example, \textsc{IsaCoSy} \cite{johansson2009isacosy} discovers
equations, which are defined as ``interesting'' if they cannot be simplified
using previously discovered equations. The intuition for such criteria is to
avoid special cases of known theorems, such as \(0 + 0 = 0\), \(0 + 1 = 1\),
etc. when we already know \(\forall x. 0 + x = x\). To work effectively, this
requires general forms to be found \emph{before} special cases.

More generally, automated TE is an \emph{optimisation problem}; an area which
has been studied extensively in the fields of Artificial Intelligence and
Machine Learning. For a more thorough treatment, \cite{geng2006interestingness}
surveys recent work on search algorithms and \cite{blum2011hybrid} surveys
interestingness criteria in the context of data mining.

\section{Tackling Complexity}\label{tackling-complexity}

Even with a slow search algorithm, we can use a divide and conquor
approach to limit the number of allowed combinations, either by using
stricter types to prevent composition, or by partitioning the theory
into small independently-searched sub-theories. Of course, such
restrictions should strike a balance between the efficiency gained and
the potential to forbid some interesting theorems.

\section{Existing Work}\label{existing-work}

Automated theory exploration has been applied to libraries in Isabelle
and Haskell, although we focus on the latter as its implementations are
the most mature (demonstrated by \textsc{Hipster}, which explores Isabelle by
translating code to Haskell first). Haskell is interesting to target,
since its emphasis on pure functions and algebraic datatypes make equational
properties common; however, Haskell is not a theorem prover, and hence it cannot
represent such as universally quantified equations. Also, the widespread use of
recursion and higher-order functions makes automated deduction non-trivial
effort is spent discovering and proving these properties compared to
proof-oriented systems like Isabelle.

Due to Haskell's relative popularity, there are large code repositories
such as \textsc{Hackage} available to explore, with the potential to
benefit existing library authors and users in comprehending and
maintaining their code \cite{QuickSpec}.

Currently, the most powerful TE system for Haskell is \textsc{HipSpec}.
This uses \textsc{QuickSpec} to search through \emph{expressions}
(combinations of the Haskell terms given by the theory), rather than
searching through the space of equations or proofs directly. Expressions
are grouped into equivalence classes, such that the \textsc{QuickCheck}
counterexample finder cannot distinguish between the elements; equations
relating the members of these classes are then conjectured, and sent to
existing automated theorem provers to try and prove \cite{rosen2012proving}. This
approach works well as a lemma generation system, making
\textsc{HipSpec} a capable inductive theorem prover as well as a theory
exploration system \cite{claessen2013automating}.

\section{Going Forward}\label{going-forward}

Given this state of the art, we identify the following as potential
areas for improvement:

\begin{itemize}
\tightlist
\item
  Expression enumeration is brute-force; this could scale to larger
  terms and theories using a heuristic algorithm.
\item
  ``Interestingness'' is a fixed part of the algorithm: an equation is
  interesting if it cannot be derived from previous equations. As we
  increase the size of our theory, this becomes unsatisfying in two
  ways:

  \begin{itemize}
  \tightlist
  \item
    The number of irreducible equations grows, making it desirable to
    impose extra conditions of a more subjective nature.
  \item
    Surprising, insightful equations may be discarded, if they are
    actually reducible in some complex, non-obvious way. A more
    subjective interestingness measure could be used to veto such
    rejections.
  \end{itemize}
\item
  The system does not propose candidate equations by data mining
  previous results; generalisation methods like anti-unification could
  do this, and at the same time remove the requirement that general
  forms must be enumerated early.
\item
  All type-safe combinations of the given expressions are tried, whilst
  it may be discernable a priori that some combinations are not worth
  considering (either because they are never related, or because their
  relations are never interesting). A pre-processor could make large
  theories more tractable by selecting combinations which are likely to
  be related, similar to premise selection in automated theorem proving
  \cite{kuhlwein2012overview}.
\end{itemize}

We are implementing a system called \textsc{ML4HS} to investigate these
ideas. Our initial hypothesis that expressions with similar definitions
are more likely to be related by equational properties than those
without, and hence a similarity-based clustering method such as that of
\textsc{ML4PG} \cite{journals/corr/abs-1302-6421} can be used to implement
a divide and conquor pre-processor.

Since our aim is to scale up theory exploration, we treat entire
\textsc{Hackage} packages as our theories. \textsc{ML4HS} will manage
downloading, compiling, managing dependencies, etc. automatically. This
both eliminates the need to define theories manually, and may be useful
in its own right as a mechanism to execute arbitrary Haskell code from
arbitrary modules in arbitrary packages.

\section{Acknowlegements}\label{acknowlegements}

I am grateful for those who have helped formulate these ideas through
conversation, especially the HipSpec team at Chalmers University (Moa
Johansson, Koen Claessen, Nick Smallbone and Dan Rosén) and my
supervisor Katya Komendantskaya. I also wish to thank the implementors
of the systems we are building on, including HipSpec, QuickSpec and
QuickCheck on the theory exploration side, as well as GHC, Cabal and Nix
on the infrastructure side.

\bibliographystyle{plain} \bibliography{../Bibtex}

\end{document}

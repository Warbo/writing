\section{Background}
\label{sec:background}

\subsection{Haskell}
\label{sec:haskell}

\newcommand{\CVar}{\texttt{Var}}
\newcommand{\CLit}{\texttt{Lit}}
\newcommand{\CApp}{\texttt{App}}
\newcommand{\CLam}{\texttt{Lam}}
\newcommand{\CLet}{\texttt{Let}}
\newcommand{\CCase}{\texttt{Case}}
\newcommand{\CType}{\texttt{Type}}
\newcommand{\CLocal}{\texttt{Local}}
\newcommand{\CGlobal}{\texttt{Global}}
\newcommand{\CConstructor}{\texttt{Constructor}}
\newcommand{\CLitNum}{\texttt{LitNum}}
\newcommand{\CLitStr}{\texttt{LitStr}}
\newcommand{\CAlt}{\texttt{Alt}}
\newcommand{\CDataAlt}{\texttt{DataAlt}}
\newcommand{\CLitAlt}{\texttt{LitAlt}}
\newcommand{\CDefault}{\texttt{Default}}
\newcommand{\CNonRec}{\texttt{NonRec}}
\newcommand{\CRec}{\texttt{Rec}}
\newcommand{\CBind}{\texttt{Bind}}

We decided to focus on theory exploration in the Haskell programming language as it has mature, state-of-the-art implementations (\qspec{} \cite{QuickSpec} and \hspec{} \cite{claessen2013automating}). This is evident from the fact that the state-of-the-art equivalent for Isabelle/HOL, the \textsc{Hipster} \cite{Hipster} system, is actually implemented by translating to Haskell and invoking \hspec{}.

Like most functional programming languages, Haskell builds upon $\lambda$-calculus, with extra features such as a strong type system and ``syntactic sugar'' to improve readability. For simplicity, we will focus on an intermediate representation of the \textsc{GHC} compiler, known as \emph{GHC Core}, rather than the relatively large and complex syntax of Haskell proper. Core is based on \fc{}, but for our machine learning purposes we are mostly interested in its syntax; for a more thorough treatment of \fc{} and its use in GHC, see \cite[Appendix C]{sulzmann2007system}.

The sub-set of Core we consider is shown in Figure \ref{fig:coresyntax}; compared to the full language \footnote{As of GHC version 7.10.2, the latest at the time of writing.} our major restriction is to ignore type hints (such as explicit casts, and differences between types/kinds/coercions). For brevity, we also omit several other forms of literal (machine words of various sizes, individual characters, etc.), as their treatment is similar to those of strings and numerals. We will use quoted strings to denote names and literals, e.g. \texttt{Local "foo"}, \texttt{Global "bar"}, \texttt{Constructor "Baz"}, \texttt{LitStr "quux"} and \texttt{LitNum "42"}, and require only that they can be compared for equality.

\begin{figure}
  \begin{haskell}\begin{verbatim}
data Nat = Z
         | S Nat

plus :: Nat -> Nat -> Nat
plus    Z  y = y
plus (S x) y = S (plus x y)

mult :: Nat -> Nat -> Nat
mult    Z  y = Z
mult (S x) y = plus y (mult x y)

odd :: Nat -> Bool
odd    Z  = False
odd (S n) = even n

even :: Nat -> Bool
even    Z  = True
even (S n) = odd n\end{verbatim}
  \end{haskell}
  \caption{A Haskell datatype for Peano numerals with some simple arithmetic functions, including mutually-recursive definitions for \hs{odd} and \hs{even}. \hs{Bool} is Haskell's built in boolean type, which can be regarded as \hs{data Bool \equal{} True | False}.}
  \label{fig:haskellexample}
\end{figure}

\begin{figure}
  \begin{small}
    \underline{\texttt{plus}}
    \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Local "y")))
                       (Alt (DataAlt "S") (App (Var (Constructor "S"))
                                               (App (App (Var (Global "plus"))
                                                         (Var (Local  "x")))
                                                    (Var (Local "y"))))
                                          "x")))
    \end{verbatim}

    \underline{\texttt{mult}}
    \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Constructor "Z")))
                       (Alt (DataAlt "S") (App (App (Var (Global "plus"))
                                                    (Var (Local  "y")))
                                               (App (App (Var (Global "mult"))
                                                         (Var (Local  "x")))
                                                    (Var (Local  "y"))))
                                          "x")))
    \end{verbatim}

    \underline{\texttt{odd}}
    \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "False")))
              (Alt (DataAlt "S") (App (Var (Global "even"))
                                      (Var (Local  "n")))
                                 "n"))
    \end{verbatim}

    \underline{\texttt{even}}
    \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "True")))
              (Alt (DataAlt "S") (App (Var (Global "odd"))
                                      (Var (Local  "n")))
                                 "n"))
    \end{verbatim}
  \end{small}
  \caption{Translations of functions in Figure \ref{fig:haskellexample} into the Core syntax of Figure \ref{fig:coresyntax}. Notice the introduction of explicit $\lambda$ abstractions (\texttt{Lam}) and the use of \texttt{Case} to represent piecewise definitions. Fresh variables are chosen arbitrarily as \texttt{"a"}, \texttt{"b"}, etc.}
  \label{fig:coreexample}
\end{figure}

Figure \ref{fig:haskellexample} shows some simple Haskell function definitions, along with a custom datatype for Peano numerals. The translation to our Core syntax is routine, and shown in Figure \ref{fig:coreexample}. Although the Core is more verbose, we can see that similar structure in the Haskell definitions gives rise to similar structure in the Core; for example, the definitions of \hs{odd} and \hs{even} are identical in both languages, except for the particular identifiers used. It is this close correspondence which allows us to analyse Core expressions in place of their more complicated Haskell source.

Note that we exclude representations for type-level entities, including datatype definitions like that of \hs{Nat}. GHC can represent these, but in this work we only consider reducible expressions (i.e. value-level bindings of the form \mbox{\hs{f a b ... = ...}}).

\subsection{Theory Exploration}
\label{sec:theoryexploration}

We consider the problem of \emph{(automated) theory exploration}, which includes the ability to \emph{generate} conjectures about code, to \emph{prove} those conjectures, and hence output \emph{novel} theorems without guidance from the user. The method of conjecture generation is a key characteristic of any theory exploration system, although all existing implementations rely on brute force enumeration to some degree.

We focus on \qspec{} \cite{QuickSpec}, which conjectures equations about Haskell code. These may be fed into another tool, such as \hspec{}, for subsequent proving. These conjectures are arrived at through the following stages:

\iffalse TODO: Make this more formal?
 V \in Var
 F \in Fun
 T \in Term
 T ::= V | F | T1 T2

 Term ::= VAR | Const | Fun (Term)
or
 Term t ::= x | f | t t'
\fi

\begin{enumerate}
  \item Given a typed signature $\Sigma$ and set of variables $V$, \qspec{} generates a list $terms$ containing the constants (including functions) from $\Sigma$, the variables from $V$ and type-correct function applications $f(x)$, where $f$ and $x$ are elements of $terms$ \iffalse TODO: A little awkward; maybe use the above notation? \fi. To ensure the list is finite, function applications are only nested up to a specified depth (by default, 3).
  \item The elements of $terms$ are grouped into equivalence classes, based on their type.
  \item The equivalence of terms in each class is tested using \qcheck{}: variables are instantiated to particular values, generated randomly, and the resulting closed expressions are evaluated and compared for equality.
  \item If a class is found to have non-equal members, it is split up to separate those members.
  \item The previous steps of testing and splitting are repeated until the classes stabilise (i.e. no differences have been observed for some specified number of repetitions).
  \item For each class, one member is selected and equations are conjectured that it is equal to each of the other members.
\end{enumerate}

Such conjectures can be used in several ways: they can be simplified for direct presentation to the user (by removing any equation which can be derived from the others by rewriting), sent to a more rigorous system like \hspec{} for proving, or even serve as a background theory for an automated theorem prover \cite{claessen2013automating}.

\iffalse
As an example, we can consider a simple signature containing the expressions from Figure \ref{fig:haskellexample}:

\begin{align*}
  \Sigma_{\texttt{Nat}} = \{\texttt{Z}, \texttt{S}, \texttt{plus}, \texttt{mult}, \texttt{odd}, \texttt{even}\}
\end{align*}

Together with a set of variables, say $V_{\texttt{Nat}} = \{a, b, c\}$, \qspec{}'s enumeration will resemble the following:

\begin{align*}
  terms_{\texttt{Nat}} = [& \texttt{Z},\ \texttt{S},\ \texttt{plus},\ \texttt{mult},\ \texttt{odd},\ \texttt{even},\ a,\ b,\ c,\ \texttt{S Z},\ \texttt{S}\ a,\ \texttt{S}\ b, \\
                     & \texttt{S}\ c,\ \texttt{plus Z},\ \texttt{plus}\ a,\ \dots ]
\end{align*}

Notice that functions such as \hs{plus} and \hs{mult} are valid terms, despite not being applied to any arguments. In addition, Haskell curries multi-argument functions, allowing them to be applied to one argument at a time, as used in the construction of $terms$.

\begin{figure}
  % To reproduce, run 'quickSpec nat' in haskell_example/src/QuickSpecExample.hs
  \begin{verbatim}
                  plus a b = plus b a
                  plus a Z = a
         plus a (plus b c) = plus b (plus a c)
                  mult a b = mult b a
                  mult a Z = Z
         mult a (mult b c) = mult b (mult a c)
              plus a (S b) = S (plus a b)
              mult a (S b) = plus a (mult a b)
         mult a (plus b b) = mult b (plus a a)
                 odd (S a) = even a
            odd (plus a a) = odd Z
           odd (times a a) = odd a
                even (S a) = odd a
           even (plus a a) = even Z
          even (times a a) = even a
plus (mult a b) (mult a c) = mult a (plus b c)
  \end{verbatim}
  \caption{Equations conjectured by \qspec{} for the functions in Figure \ref{fig:haskellexample}; after simplification.}
  \label{fig:qspecresult}
\end{figure}

These terms will be grouped into five classes, one each for \hs{Nat}, \hs{Nat -> Nat}, \hs{Nat -> Nat -> Nat}, \hs{Nat -> Bool} and \hs{Bool}. As the variables $a$, $b$ and $c$ are instantiated to various randomly-generated numbers, these equivalence classes will be divided, until eventually the equations in Figure \ref{fig:qspecresult} are conjectured.
\fi

Although complete, this enumeration approach is wasteful: many terms are unlikely to appear in theorems, which requires careful choice by the user of what to include in the signature. For example, we know that addition and multiplication are closely related, and hence obey many algebraic laws. Our machine learning technique aims to predict these kinds of relations between functions, so we can create small signatures which nevertheless have the potential to give rise to many equations.

\iffalse
TODO
In fact, there are similarities between the way a TE system like \qspec{} can generalise from checking \emph{particular} properties to \emph{inventing} new ones, and the way counterexample finders like \qcheck{} can generalise from testing \emph{particular} expressions to \emph{inventing} expressions to test. One of our aims is to understand the implications of this generalisation, the lessons that each can learn from the other's approach to term generation, and the consequences for testing and QA in general.
\fi

\subsection{Clustering}
\label{sec:clustering}

Our approach to scaling up \qspec{} takes inspiration from two sources. The first is relevance filtering, which makes expensive algorithms used in theorem proving more practical by limiting the size of their inputs. We describe this approach in more details in \S \ref{sec:relevance}. Relevance filtering is a practical tool which has existing applications in software, such as the \emph{Sledgehammer} component of the Isabelle/HOL theorem prover.

Despite the idea's promise, we cannot simply invoke existing relevance filter algorithms in our theory exploration setting. The reason is that relevance filtering is a supervised learning method, i.e. it would require a distinguished expression to compare everything against. Theory exploration does not have such a distinguished expression; instead, we are interested in relationships between \emph{all} terms in a signature, and hence we must consider the relevance of \emph{all terms} to \emph{all other terms}.

A natural fit for this task is \emph{clustering}, which attempts to group similar inputs together in an unsupervised way. Based on their success in discovering relationships and patterns between expressions in Coq and ACL2 (in the ML4PG and ACL2(ml) tools respectively), we hypothesise that clustering methods can fulfil the role of relevance filters for theory exploration: intelligently breaking up large signatures into smaller ones more amenable to brute force enumeration, such that related expressions are explored together.

We use \emph{k-means} clustering, implemented in the Weka tool \cite{Holmes.Donkin.Witten:1994} by Lloyd's algorithm \cite{lloyd1982least}, as this setup is used by ML4PG and ACL2(ml). For simplicity, we will cluster $n$ points into $k = \lceil \sqrt{\frac{n}{2}} \rceil$ clusters, following the ``rule of thumb'' given in \cite[pp. 365]{mardia1979multivariate}, and will use randomly-selected elements of our data set to determine the initial clusters (known as the Forgy method).

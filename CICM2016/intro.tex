\section{Introduction}

As computers and software become more sophisticated, and as our reliance on them increases, the importance of \emph{understanding}, \emph{predicting} and \emph{verifying} these systems grows; which is undermined by their ever-increasing complexity. The \emph{functional programming} paradigm has been proposed for addressing these issues \cite{hughes1989functional}, by constructing programs which are more amenable to mathematical analysis. For example, in pure functional programming all values are \emph{immutable}: defined once and never changed. Hence there is no way for a value to be altered between the point it is introduced and the point it is used, unlike in many \emph{imperative} languages where we may have to search the whole program to ensure the value is not altered by any intermediate code. Similarly, the results of pure functions cannot depend on any state other than their arguments, and hence will always produce repeatable results. By making state implicit in this way, powerful type systems can be used to constrain the behaviour of programs, and to give a rich, composable structure to data.

Whilst use of pure functional programming languages, like Haskell and Idris, is relatively rare, their features are well suited to common software engineering practices like \emph{unit testing}; where tasks are broken down into small, easily-specified ``units'', and tested in isolation for a variety of use-cases. Functional ideas are thus spreading to mainstream software engineering in a more dilute form; seen, for example, in the recent inclusion of first-class functions in Java \cite{gosling2015java} and C++ \cite{willcock2006lambda}.

Functional programming is also well suited to less-widespread practices, such as \emph{property checking} (as popularised by \qcheck{}) and \emph{theorem proving}, which are promising methods for increasing confidence in software, yet can be prohibitively expensive. Here we investigate how the recent \emph{theory exploration} approach can lower the effort required to pursue these goals, and in particular how machine learning techniques can mitigate the costs of the combinatorial algorithms involved.

Our contributions are:

\begin{enumerate}
  \item The application of machine learning algorithms to theory exploration, for intelligently discovering interesting sub-sets of Haskell libraries, which are more tractable to explore.
  \item A novel feature extraction method for transforming Haskell expressions into a form amenable to off-the-shelf learning algorithms.
  \item An implementation of these feature extraction and theory exploration approaches.
  \item A comparison of our methods with existing approaches, both for theory exploration in Haskell, and for machine learning in other languages.
\end{enumerate}

We begin in \S \ref{sec:background} by providing a formal context for analysing Haskell expressions (\S \ref{sec:haskell}) and describe the \qspec{} theory exploration system (\S \ref{sec:theoryexploration}). We give a brief overview of testing approaches and how they relate to Haskell (\S \ref{sec:quickcheck}), as well as the machine learning approaches we are building on (\S \ref{sec:featureextraction}). We discuss our contributions in more depth in \S \ref{sec:contributions}, and provide implementation details in \S \ref{sec:implementation}. A variety of related work is surveyed in \S \ref{sec:related}, we briefly evaluate our implementations in \S \ref{sec:evaluation} and give several potential directions for future research in \S \ref{sec:future}.

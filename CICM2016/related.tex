\section{Related Work}
\label{sec:related}

\subsection{Theory Exploration}

We briefly described theory exploration in \S \ref{sec:theoryexploration}, as the task of discovering \emph{new} theorems in a software or proof library, rather than proving/disproving user-provided statements. This general idea is quite old, with . The current generation of theory exploration tools trace their roots back to the interactive \textsc{Theorema} \cite{buchberger2000theory} system of Buchberger. In such a setting, most of the automation focuses on bookkeeping (such as type-checking, managing scope, etc.) and more traditional automated theorem proving.

Subsequent systems have increased the level of automation, and have shown promise in tasks such as lemma discovery. As well as \qspec{} and \hspec{} in Haskell, automated theory exploration has been applied to Isabelle \cite{Montano-Rivas.McCasland.Dixon.ea:2012} \cite{johansson2009isacosy} \cite{Hipster}. Since \qspec{} is used for conjecture generation in many of those systems, they can also benefit from our work with machine learning.

\subsection{Relevance Filtering}
\label{sec:relevance}

The combinatorial nature of formal systems causes many proof search methods, such as resolution, to have exponential complexity \cite{haken1985intractability}. The idea of \emph{relevance filtering} (or, \emph{premise selection}) is to reduce the input size of a task by removing redundant or irrelevant information from the input. Our restriction of theory exploration to intelligently-selected clusters of symbols, rather than whole libraries at a time, is the essentially the same idea being applied in a novel setting.

Relevance filtering has mostly been used in automated proof search, where it simplifies problems by removing from consideration those clauses (axioms, definitions, lemmas, etc.) which are deemed \emph{irrelevant}. The technique is used in Isabelle's Sledgehammer tool, during its translation of Isabelle/HOL theories to statements in first order logic: rather than translating the entire theory, only a sub-set of relevant clauses are included. This reduces the size of the problem and speeds up the proof search, but it creates the new problem of determining when a clause is relevant: how do we know what will be required, before we have the proof?

Various approaches have been proposed in the literature \cite{kuhlwein2013mash} \cite{kuhlwein2012overview} \cite{alama2014premise}, though the original approach is based on the proportion of symbols which a clause has in common with the problem statement \cite{meng2009lightweight} (plus various heuristics).

\subsection{Recurrent Clustering}
\label{sec:clusteringexpressions}

Our recurrent clustering approach takes inspiration from the ML4PG \cite{journals/corr/abs-1212-3618} and ACL2(ml) \cite{heras2013proof} tools, used for analysing proofs in Coq and ACL2, respectively. Whilst both transform syntax trees into matrices, the algorithm of ML4PG most closely resembles ours as it assigns tokens directly to matrix elements. In contrast, the matrices produced by ACL2(ml) \emph{summarise} information about the tree; for example, one column counts the number of variables appearing at each tree level, others count the number of function symbols which are nullary, unary, binary, etc. Whilst it may be interesting to contrast our current algorithm with an alternative based on that of ACL2(ml), it is unclear how such summaries could be extended to include types, which seems the next logical step for our approach. The ML4PG algorithm extends trivially, by using (term, type) pairs instead of just terms.

The way we \emph{use} our clusters to inform theory exploration is actually more similar to that of ACL2(ml) than ML4PG. ML4PG can either present clusters to the user for inspection, or produce automata for recreating proofs. In ACL2(ml), the clusters are used to restrict the search space of a proof search, much like we restrict the scope of theory exploration.

ACL2(ml) reasons by analogy: finding theorem statements which are similar to the current goal, and attempting to prove the goal in a similar way. In particular, the lemmas used to prove a theorem are mutated by substituting symbols for those which appear in the same cluster. For example, if \texttt{plus} and \texttt{multiply} are clustered together, and we are trying to prove a goal involving \texttt{multiply}, then ACL2(ml) might consider an existing theorem involving \texttt{plus}. The lemmas used to prove that theorem will be mutated, for example replacing occurrences of \texttt{plus} with \texttt{mult}, in an attempt to prove the goal.

Whilst we do not currently reason by analogy, this is an interesting area for future work in theory exploration: given a set of theorems relating particular terms, we might form conjectures regarding similar terms found through clustering.

\subsection{Feature Extraction}

One major difficulty when applying statistical machine learning algorithms to \emph{languages}, such as Haskell, is the appearance of recursive structures. This can lead to nested expressions of arbitrary depth, which are difficult to compare in numerical ways. One common approach to this problem is to represent such structures as \emph{sequences}. \emph{Recurrent neural networks} (RNNs) are a popular choice for processing sequences, especially when combined with mechanisms such as \emph{long short-term memory} (LSTM) for preserving information across long sequences \cite{hochreiter1997long}. Such systems have been used, for example, to parse and execute computer programs \cite{zaremba2014learning}. However, learning to parse sequences seems inefficient considering that we already have correctly-parsed ASTs.

Whilst neural networks have been applied directly to recursive structures \cite{goller1996learning}, including using LSTM \cite{zhu2015long}, a more popular approach is to use \emph{kernel methods} \cite{bakir2007predicting}. These are promising as a more principled alternative to our current hand-crafted translation of ASTs to vectors.

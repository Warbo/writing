\section{Implementation}
\label{sec:implementation}

We provide an implementation of our recurrent clustering algorithm in a tool called \textsc{ML4HS}. We obtain Core ASTs from Haskell packages using a plugin for the GHC compiler, which emits a serialised version of each Core definition as it is being compiled. This approach is more robust than, for example, parsing source files, since it avoids the complications of preprocessors and language extensions altering the syntax.

A post-processing stage determines which Core definitions can be used by \qspec{}, based on their type, visibility (whether they are encapsulated inside their module or visible to importers), etc. For each definition, we simply use the GHCi interpreter to check if calling \qspec{} with that argument is well-typed.

These definitions are then sorted topologically, based on the non-local identifiers appearing in their ASTs, and feature vectors are constructed using a Haskell implementation of the approach described in \S \ref{sec:recurrentclustering}. Since the features associated with each identifier may vary between iterations, we leave the raw identifiers in the vector so their features can be extracted in the correct context.

The iterative algorithm \ref{alg:recurrent} is a simple shell script, which calls the Weka machine learning system to perform k-means clustering and associate each Core definition with a cluster number. These numbers are used to finish the deferred feature extraction of identifiers, the resulting feature vectors are clustered, and so on until all SCCs have been processed.

Once the recurrent clustering is complete, we split the definitions based on their final clusters, and use these clusters to generate strings of Haskell code for invoking \qspec{}. To construct a \qspec{} signature from these definitions, we must:

\begin{itemize}
  \item{Monomorphise}: If a value has polymorphic type, e.g. \hs{equal :: forall t. t -> t -> Bool}, we must choose a concrete representation to use (in this case for \hs{t}), in order to know which random generator to use. We take the approach used in \qcheck{} by attempting to instantiate all type variables to \hs{Integer}. Any values where this is invalid, e.g. if there are incompatible class constraints (e.g. \hs{forall t. IsString t => t}, where \hs{Integer} does not implement \hs{IsString}) will have already been discarded in our post-processing stage after extracting the ASTs.

  \item{Qualification}: All names are \emph{qualified} (prefixed by their module's name), to avoid most ambiguity. There is still the possibility that multiple packages will declare modules of the same name, although this is rare as it causes problems for any Haskell programmer trying to use those modules. In such cases the exploration process simply aborts.

  \item{Variable definition}: Once a \qspec{} theory has been defined containing all of the given terms, we inspect the types it references and append three variables for each to the theory (enough to discover laws such as associativity, but not too many to overflow the limit of \qspec{}'s exhaustive search).

  \item{Sandboxing}: One difficulty with Haskell's packaging infrastructure is that all required packages and modules must be provided up-front, usually by specification in a Cabal file. Since \textsc{MLSpec} builds signatures \emph{dynamically}, depending on the cluster information it is given, we do not know what packages it may need. To work around this problem, we evaluate these strings of Haskell using a custom library called \texttt{nix-eval}. This uses the Nix package manager to obtain all of the required packages and make them available to GHC.

\end{itemize}

% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{multirow}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Journal of Automated Reasoning}

\begin{document}

\title{Quantitative Benchmarking for Automatically Generated Conjectures%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton \and
        Alison Pease
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
           University of Dundee \\
           \email{c.m.warburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           A. Pease \at
           University of Dundee \\
           \email{a.pease@dundee.ac.uk}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle
%IGNORE ISABELLE THEORIES FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%IGNORE CLUSTERING FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

%COMPARE COLTON ET AL.

% Evaluation of conjectures, sets of conjectures, systems
% what constitutes a good/desired output (in each author's terms)? This combines
% their evaluation with their (system-specific) assumptions (e.g. IsaCoSy doesn't
% generate redundant terms: non-redundancy is clearly a goal, but it's not measured
% in eval since it's assumed based on system's construction)

% Look carefully at each system's (paper's) eval section
% Make a table like tabl 1 of Colton et al

% Make it clear that methodology is the point (it's not just a detail, like in a
% chemistry paper for example)

\begin{abstract}
  We propose a benchmark suite for evaluating the efficiency and effectiveness
  of automated tools for \emph{theory exploration} or
  \emph{conjecture formation} in higher-order, inductive theories; a domain
  especially suited for analysing software. By providing standard tools and
  metrics, we hope to encourage innovation and comparison between the disparate
  approaches currently being pursued, and spur improvements similar to those
  seen in the competitive field of automated theorem proving.
%\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% Lay out the field of TE: motivation, systems, evaluation, problems...

\emph{Automated theory exploration} (ATE), also known as
\emph{conjecture generation/formation}, is the open-ended problem of producing
conjectures about a given logical theory which are somehow ``interesting''.

This has applications wherever we find use for formal statements: in proof
assistants and their libraries, in mathematics education and research, and
(of special concern for the authors) in the specification, verification,
optimisation and testing of software.

Existing attempts at tackling this problem have been found difficult to compare
and study, due partially to the variety of approaches taken, but also because of
the inherent ambiguity of the task and the different goals emphasised by their
authors and evaluation methods.

We attempt to solve this discrepancy, at least for the foreseeable future, by
defining a standard, unambiguous benchmarking approach with which to compare
ATE systems. Our contributions include:

\begin{itemize}
\item A general methodology for benchmarking ATE systems.
\item Resolving the issue of ``interestingness'' through the use theorem-proving
  benchmarks as a ground-truth.
\item A specific instantiation of this methodology, using the Tons of Inductive
  Problems benchmark as a corpus.
\item Automated tooling to produce this benchmark and analyse its output.
\item Application of this methodology to the QuickSpec and IsaCoSy ATE systems,
  and a discussion of the results.
\end{itemize}

Section \ref{sec:background} describes the ATE problem in more detail, along
with existing approaches and their evaluation methodologies. We explain our
proposal for a more general benchmark in section \ref{sec:proposal} and
section \ref{sec:application} shows the results when applied to existing ATE tools.
Analysis of these results is given in section \ref{sec:discussion} and
concluding remarks in \ref{sec:conclusion}.

%- Existing approaches are difficult to compare
% - They want different things
% - They're evaluated differently
% - They're evaluated ``locally'' (as white boxen)

% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

\section{Background}
\label{sec:background}

\subsection{Motivation}
\label{sec:motivation}

% more detail about evaluation, interestingness, etc. Reference Simon/Alan
% interestingness, e.g. examples from textbooks, etc.

% split into sub-sections, e.g. motivation, existing approaches, etc.

\begin{figure}
  \begin{equation*}
    \begin{split}
      \forall a. \text{Nil} &: \text{List} \  a \\
      \forall a. \text{Cons} &: a \rightarrow \text{List} \  a \rightarrow \text{List} \ a \\
      \text{head}(\text{Cons}(x, xs)) &= x \\
      \text{tail}(\text{Cons}(x, xs)) &= xs \\
      \text{append}(xs, \text{Nil}) &= xs \\
      \text{append}(\text{Cons}(x, xs), ys) &= \text{Cons}(x, \text{append}(xs, ys)) \\
      \text{reverse}(\text{Nil}) &= \text{Nil} \\
      \text{reverse}(\text{Cons}(x, xs)) &= \text{append}(\text{reverse}(xs), \text{Cons}(x, \text{Nil})) \\
      \text{length}(\text{Nil}) &= \text{Z} \\
      \text{length}(\text{Cons}(x, xs)) &= \text{S} (\text{length}(xs)) \\
      \text{map}(f, \text{Nil}) &= \text{Nil} \\
      \text{map}(f, \text{Cons}(x, xs)) &= \text{Cons}(f(x), \text{map}(f, xs)) \\
      \text{foldl}(f, x, \text{Nil}) &= x \\
      \text{foldl}(f, x, \text{Cons}(y, ys)) &= \text{foldl}(f, f(x, y), xs) \\
      \text{foldr}(f, \text{Nil}, y) &= y \\
      \text{foldr}(f, \text{Cons}(x, xs), y) &= f(x, \text{foldr}(f, xs, y))
    \end{split}
  \end{equation*}
  \caption{A simple theory defining a $\text{List}$ type and some associated
    operations. $\text{Z}$ and $\text{S}$ are from a Peano encoding of the natural numbers.
    Taken from \cite{Johansson.Dixon.Bundy:conjecture-generation}}
  \label{figure:list_theory}
\end{figure}

If we have a logical theory, for example the theory of lists shown in figure
\ref{figure:list_theory}, we may want to find theorems which describe its
behaviour. For a software library we may want to verify that certain
(un)desirable properties do (not) hold; we may also want to \emph{optimise}
programs using this library, rewriting expressions into a form which requires
less time, memory, network usage, etc. To avoid altering the program's result,
it is important to prove particular rewrite rules correct. Consider the
following theorem:

\begin{equation} \label{eq:mapreduce}
  \forall f. \forall xs. \forall ys. \text{map}(f, \text{append}(xs, ys)) = \text{append}(\text{map}(f, xs), \text{map}(f, ys))
\end{equation}

This justifies a rewrite rule for splitting a single $\text{map}$ call into
multiple independent calls dealing with different sections of the list. This is
useful for the ``map/reduce'' programming paradigm, as each call can be
evaluated by a separate machine in parallel.

\subsection{Theory Exploration}
\label{sec:te}

Discovering theorems from an arbitrary theory requires two steps:

\begin{itemize}
\item \emph{Exploring} the theory, to find patterns suitable for posing as
  conjectures.
\item \emph{Proving} that those conjectures are theorems.
\end{itemize}

Lots of research has been devoted to the second step, resulting in the field of
Automated Theorem Proving (ATP) and its spinoff applications. Less attention has
been paid to the first step, that of Automated Theory Exploration (ATE).

A major challenge in ATE is the requirement for conjectures to be
``interesting'', since this is an imprecise term with many different
interpretations, which is reflected in the variety of disparate approaches taken
so far.

A survey of these systems and their notions of ``interestingness'' is given by
Colton et al~\cite{colton2000notion}, for both conjectures and \emph{concepts}
(the contents of a theory, which for ATE we consider to be provided as input,
rather than part of the generating process). We summarise those aspects relevant
to conjecture formation in table \ref{table:colton}, along with later systems
we have studied.

\begin{table}
  \begin{center}
    \begin{tabular}{ |l|l|c|c|c|c|c|c| }
      \hline
      \multirow{2}{*}{\textbf{Program}}                 &
      \multirow{2}{*}{\textbf{Conjecture Types}}        &
      \multicolumn{6}{c}{\textbf{Interestingness Measures}} \\
      \hhline{~~------}
         & & \textbf{E} & \textbf{N} & \textbf{S} & \textbf{A} & \textbf{C} & \textbf{U} \\

      \hline
      AM          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      GT          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      Graffiti    & inequalities                        & X & X & X &   & X & X \\ \hline
      Bagai et al & non-exists                          &   & X &   & X & X &   \\ \hline
      HR          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      QuickSpec   & equalities                          & X & X &   &   & X &   \\ \hline
      IsaCoSy     & equalities                          &   & X &   &   & X &   \\ \hline
      IsaScheme   & equalities                          &   &   &   &   & X &   \\ \hline
    \end{tabular}
  \end{center}
  \caption{Classification of ATE systems from \cite{colton2000notion}, extended
    to those compared in \cite{claessen2013automating} (QuickSpec is the
    conjecture generation component of HipSpec). The interestingness measures
    are \textbf{E}mpirical plausibility, \textbf{N}ovelty,
    \textbf{S}urprisingness, \textbf{A}pplicability, \textbf{C}omprehensibility
    (low complexity) and \textbf{U}tility.}
  \label{table:colton}
\end{table}

One aspect mentioned in the survey is that ``interestingness'' is not just a
measure of output quality, but can also impact the core design of a system. For
example, the use of constraint solving in IsaCoSy is to prevent forming
conjectures which are simple substitutions of previous ones; this is because the
designers consider such conjectures to be uninteresting. This further
complicates comparison of systems based on standalone evaluations, since
assumptions about such behaviour may be implicit in the interpretation (e.g. an
analysis of IsaCoSy conjectures does not need to consider substitutions), and
each methodology will be using different assumptions.

\subsection{Existing Evaluations}
\label{sec:existing}

The variety of existing ATE systems, their different domains, and their varying
interpretations of what is interesting makes it difficult to directly compare
their performance.

Whilst all offer insights into the ATE problem, we will limit ourselves to those
operating in higher-order logic and inductive theories, since this domain is the
most relevant for our interest in analysing software. The relevant systems in
this domain are QuickSpec, operating on functions in the Haskell programming
language; and IsaCoSy and IsaScheme, which operate in the Isabelle proof
assistant.

There are three aspects to consider when we evaluate an ATE system:

\begin{itemize}
\item The quality of each conjecture, which is the ``interestingness'' discussed
  above.
\item The \emph{set} of conjectures produced, to assess things like consistency
  and redundancy.
\item Performance of the system, to find the balance struck between output
  quality and time taken.
\end{itemize}

Existing evaluation of these three systems have assessed the interestingness of
their output by operating in well-studied theories, and taking a set of existing
theorems as a \emph{ground truth} to compare against.

A common choice of ground truth is the standard library of a theorem prover,
such as Isabelle/HOL; the library authors have gone to the effort of stating,
proving and including these theorems in every copy of their software, which is a
good indication that they are useful or important.

One problem with this choice is the small size of these libraries. For example,
the benchmark based on Isabelle/HOL's theory of natural numbers given
in~\cite{Johansson.Dixon.Bundy:conjecture-generation} contains only 4
definitions and a ground truth of 12 theorems. Whilst such benchmarks allow
objective comparisons between different approaches, their narrow scope doesn't
provide much indication of performance in different, especially \emph{novel}, domains.

\section{Theory Exploration Benchmark}
\label{sec:proposal}

We propose a larger, more ambitious method for evaluating ATE systems, which we
call the Theory Exploration Benchmark (TEB). We take a similar approach to prior
evaluations, but our ground truth comes from the extensive test suites which are
already used to benchmark automated \emph{theorem provers}; which we repurpose
for use in the theory \emph{exploration} setting.

To do this we can take a set of ATP tasks, for example proving the equation
\ref{eq:mapreduce} from the theory in figure \ref{figure:list_theory}, and we
split apart the definitions (to form a theory) from the theorem statements (to
form our ground truth).

We focus on the Tons of Inductive Problems (TIP) theorem proving
benchmark~\cite{claessen2015tip}, since it has several desirable properties:

\begin{itemize}
\item Each problem provides separate definitions and a theorem statement; hence
  it's not difficult to tease them apart for our purposes.
\item Higher-order functions and inductively defined types are provided, which
  corresponds to our desired domain.
\item All together, TIP provides 219 distinct function definitions and 343
  theorem statements, which is enough to fully exercise current ATE systems.
\item Benchmark problems include examples from the theorem proving literature
  as well as common program verification tasks, ensuring the resulting corpus is
  relevant to researchers and practitioners.
\item TIP's maintainers provide a set of tools, including translators from TIP's
  native format (based on SMT-Lib~\cite{BarFT-SMTLIB}) to Haskell and Isabelle
  (which are used by ATE systems).
\end{itemize}

From TIP, we automatically derive our Theory Exploration Benchmark (TEB), and
provide tools to benchmark ATE systems and compare their results. Our
methodology is as follows:

% Better presentation, flowchart?
\begin{itemize}
\item Collect together all definitions found in TIP into one large theory,
  removing $\alpha$-equivalent duplicates.
\item For each type, provide extra functions which act as constructors and
  destructors. For example, the $\text{List}$ type would have extra constructor
  functions $\text{constructor-Nil}$ and $\text{constructor-Cons}$, and
  destructor functions $\text{destructor-head}$ and $\text{destructor-tail}$.
  This allows us to ignore constructors and destructors, and concentrate solely
  on functions.
\item Collect together all theorem statements found in TIP. Update any
  references to removed definitions, and replace references to constructors with
  the associated constructor function from above.
\item \emph{Sample} a sub-set of functions from the large theory uniformly, such
  that we choose all of the names referenced by at least one of the theorems
  (for example, theorem \ref{eq:mapreduce} references the names
  $\text{map}$ and $\text{append}$).
\item Provide this sub-set as an input to the ATE system, timing how long it
  takes to complete.
\item Perform precision/recall analysis on the resulting conjectures, against
  the ground truth of TIP theorems which only reference names in this sample.
\end{itemize}

\section{Application}
\label{sec:application}

% evaluation

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy theory
exploration systems. To assess the scalability of their algorithms, we sampled
theories of various sizes: from theories containing only a single definition, up
to theories containing 20 definitions.

To provide a more robust comparison of these two systems, we also applied a
paired difference test: measuring, for each sampled theory, the difference in
time, precision and recall between the two systems.

\section{Discussion}
\label{sec:discussion}

% philosophical bits

\section{Conclusion}
\label{sec:conclusion}

% future work, etc.

Our benchmark suite provides a unifying goal for the diverse approaches of
theory exploration, whilst avoiding some of the philosophical complications of
the field. Whilst the current implementation is quite limited, we welcome
additions from other researchers, to more closely align the benchmark with their
goals, and the strenghts and weaknesses of their systems.

``Solving'' this benchmark suite would not solve the problem of theory exploration in general,
so more ambitious goals must be set in the future; but we have found that there is still a
long way to go until that problem arises.

\begin{acknowledgements}
  We are grateful to Jianguo Zhang for help with our statistical analysis.

  EPSRC
\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

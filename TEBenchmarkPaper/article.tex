% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{multirow}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Journal of Automated Reasoning}

\begin{document}

\title{Quantitative Benchmarking for Automatically Generated Conjectures%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton \and
        Alison Pease
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
           University of Dundee \\
           \email{c.m.warburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           A. Pease \at
           University of Dundee \\
           \email{a.pease@dundee.ac.uk}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle
%IGNORE ISABELLE THEORIES FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%IGNORE CLUSTERING FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

%COMPARE COLTON ET AL.

% Evaluation of conjectures, sets of conjectures, systems
% what constitutes a good/desired output (in each author's terms)? This combines
% their evaluation with their (system-specific) assumptions (e.g. IsaCoSy doesn't
% generate redundant terms: non-redundancy is clearly a goal, but it's not measured
% in eval since it's assumed based on system's construction)

% Look carefully at each system's (paper's) eval section
% Make a table like tabl 1 of Colton et al

% Make it clear that methodology is the point (it's not just a detail, like in a
% chemistry paper for example)

\begin{abstract}
  We propose a benchmark suite for evaluating the efficiency and effectiveness
  of automated tools for \emph{theory exploration} or
  \emph{conjecture formation} in higher-order, inductive theories; a domain
  especially suited for analysing software. By providing standard tools and
  metrics, we hope to encourage innovation and comparison between the disparate
  approaches currently being pursued, and spur improvements similar to those
  seen in the competitive field of automated theorem proving.
%\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% Lay out the field of TE: motivation, systems, evaluation, problems...

\emph{Automated theory exploration} (ATE), also known as
\emph{conjecture generation/formation}, is the open-ended problem of producing
conjectures about a given logical theory which are somehow ``interesting''.

This has applications wherever we find use for formal statements: in proof
assistants and their libraries, in mathematics education and research, and
(of special concern for the authors) in the specification, verification,
optimisation and testing of software.

Existing attempts at tackling this problem are difficult to compare, due
partially to the variety of approaches taken, but also because of the inherent
ambiguity of the task and the different goals emphasised by their designers and
their choice of evaluation method.

We attempt to solve this discrepancy, at least for the foreseeable future, by
defining a standard, unambiguous benchmarking approach with which to compare
ATE systems. Our contributions include:

\begin{itemize}
\item A general methodology for benchmarking ATE systems.
\item Resolving the issue of ``interestingness'' through the use of
  theorem-proving benchmarks as a ground-truth.
\item A specific instantiation of this methodology, using the Tons of Inductive
  Problems benchmark as a corpus.
\item Automated tooling to perform this benchmarking.
\item Application of our methodology to the QuickSpec and IsaCoSy ATE systems,
  and a discussion of the results.
\end{itemize}

Section \ref{sec:background} describes the ATE problem in more detail, along
with existing approaches and their evaluation methodologies. We explain our
proposal for a more general benchmark in section \ref{sec:proposal} and
section \ref{sec:application} shows the results when applied to existing ATE tools.
Analysis of these results is given in section \ref{sec:discussion} and
concluding remarks in \ref{sec:conclusion}.

%- Existing approaches are difficult to compare
% - They want different things
% - They're evaluated differently
% - They're evaluated ``locally'' (as white boxen)

% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

\section{Background}
\label{sec:background}

\subsection{Motivation}
\label{sec:motivation}

% more detail about evaluation, interestingness, etc. Reference Simon/Alan
% interestingness, e.g. examples from textbooks, etc.

% split into sub-sections, e.g. motivation, existing approaches, etc.

\begin{figure}
  \begin{equation*}
    \begin{split}
      \forall a. \text{Nil} &: \text{List} \  a \\
      \forall a. \text{Cons} &: a \rightarrow \text{List} \  a \rightarrow \text{List} \ a \\
      \text{head}(\text{Cons}(x, xs)) &= x \\
      \text{tail}(\text{Cons}(x, xs)) &= xs \\
      \text{append}(xs, \text{Nil}) &= xs \\
      \text{append}(\text{Cons}(x, xs), ys) &= \text{Cons}(x, \text{append}(xs, ys)) \\
      \text{reverse}(\text{Nil}) &= \text{Nil} \\
      \text{reverse}(\text{Cons}(x, xs)) &= \text{append}(\text{reverse}(xs), \text{Cons}(x, \text{Nil})) \\
      \text{length}(\text{Nil}) &= \text{Z} \\
      \text{length}(\text{Cons}(x, xs)) &= \text{S} (\text{length}(xs)) \\
      \text{map}(f, \text{Nil}) &= \text{Nil} \\
      \text{map}(f, \text{Cons}(x, xs)) &= \text{Cons}(f(x), \text{map}(f, xs)) \\
      \text{foldl}(f, x, \text{Nil}) &= x \\
      \text{foldl}(f, x, \text{Cons}(y, ys)) &= \text{foldl}(f, f(x, y), xs) \\
      \text{foldr}(f, \text{Nil}, y) &= y \\
      \text{foldr}(f, \text{Cons}(x, xs), y) &= f(x, \text{foldr}(f, xs, y))
    \end{split}
  \end{equation*}
  \caption{A simple theory defining a $\text{List}$ type and some associated
    operations. $\text{Z}$ and $\text{S}$ are from a Peano encoding of the natural numbers.
    Taken from \cite{Johansson.Dixon.Bundy:conjecture-generation}}
  \label{figure:list_theory}
\end{figure}

Given a logical theory, like the theory of lists shown in figure
\ref{figure:list_theory}, we may want to find theorems which describe its
behaviour. This could be for mathematical curiosity, or due to the theory's
importance in some domain. In particular, for theories which capture the
semantics of some software library, we may want to verify that certain
(un)desirable properties do (not) hold; we might also want to \emph{optimise}
programs using this library, rewriting expressions into a form which requires
less time, memory, network usage, etc. To avoid altering a program's result,
such rewrites should come with theorems proving their correctness, such as the
following theorem for our theory of lists:

\begin{equation} \label{eq:mapreduce}
  \forall f. \forall xs. \forall ys. \text{map}(f, \text{append}(xs, ys)) = \text{append}(\text{map}(f, xs), \text{map}(f, ys))
\end{equation}

This justifies a rewrite rule for splitting a single $\text{map}$ call into
multiple independent calls dealing with different sections of a list. Such rules
are useful optimisations since each call can be evaluated in parallel, leading
to the ``map/reduce'' programming paradigm.

All of these example use cases require the ability to discover theorems about
some particular theory. This is a hard problem in general, but presents
opportunities for automation due to the precise, symbolic nature of the domain.

\subsection{Theory Exploration}
\label{sec:te}

The task of discovering theorems in an arbitrary theory can be broken down into
two steps:

\begin{itemize}
\item \emph{Exploring} the theory, to find patterns suitable for posing as
  conjectures.
\item \emph{Proving} that those conjectures are theorems.
\end{itemize}

This second step is studied extensively in the field of Automated Theorem
Proving (ATP). Less attention has been paid to the first step, that of
Automated Theory Exploration (ATE).

A major challenge in ATE is choosing how to narrow down the set of generated
conjectures to those deemed ``interesting'', since this is an imprecise term
with many different interpretations. For example, all existing approaches agree
that simple tautologies are ``uninteresting'', but differ when it comes to more
complex statements.

A survey of ATE systems and their notions of ``interestingness'' is given by
Colton et al~\cite{colton2000notion}. Their survey also includes \emph{concept
  formation}, which corresponds to the automatic creation of theories, although
We ignore this as a separate concern: the ATE task assumes that a theory is
provided. We summarise those aspects relevant to conjecture formation in table
\ref{table:colton}, and extend the comparison with three more recent systems.

\begin{table}
  \begin{center}
    \begin{tabular}{ |l|l|c|c|c|c|c|c| }
      \hline
      \multirow{2}{*}{\textbf{Program}}                 &
      \multirow{2}{*}{\textbf{Conjecture Types}}        &
      \multicolumn{6}{c}{\textbf{Interestingness Measures}} \\
      \hhline{~~------}
         & & \textbf{E} & \textbf{N} & \textbf{S} & \textbf{A} & \textbf{C} & \textbf{U} \\

      \hline
      AM          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      GT          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      Graffiti    & inequalities                        & X & X & X &   & X & X \\ \hline
      Bagai et al & non-exists                          &   & X &   & X & X &   \\ \hline
      HR          & if-and-only-if, implies, non-exists & X & X & X & X & X & X \\ \hline
      QuickSpec   & equalities                          & X & X &   &   & X &   \\ \hline
      IsaCoSy     & equalities                          &   & X &   &   & X &   \\ \hline
      IsaScheme   & equalities                          &   &   &   &   & X &   \\ \hline
    \end{tabular}
  \end{center}
  \caption{Classification of ATE systems from \cite{colton2000notion}, extended
    to those compared in \cite{claessen2013automating} (QuickSpec is the
    conjecture generation component of HipSpec). The interestingness measures
    are \textbf{E}mpirical plausibility, \textbf{N}ovelty,
    \textbf{S}urprisingness, \textbf{A}pplicability, \textbf{C}omprehensibility
    (low complexity) and \textbf{U}tility.}
  \label{table:colton}
\end{table}

\subsection{Existing Evaluations}
\label{sec:existing}

The criteria for ``interestingness'' in table \ref{table:colton} not only allow
the results of ATE systems to be evaluated, but they also affect the choice of
algorithm used by those systems in the first place. This coupling between
implementation detail and assessment method makes it difficult to compare
existing approaches, since the assumptions made by each evaluation methodology
may lead to incompatible results. For example, the design of IsaCoSy treats
simple substitutions of existing conjectures as uninteresting; yet this
criterion is absent from its evaluation, since the algorithm uses a constraint
solver to entirely avoid such results.

If we limit ourselves to exploring higher-order theories with inductive types
(a domain closely matching our interest in analysing software), we do find a
direct comparison of three systems~\cite{claessen2013automating}: QuickSpec (the
conjecture-generating component of HipSpec), IsaCoSy and IsaScheme. This is a
\emph{precision/recall} analysis, providing the same theories to each system and
comparing the set of generated conjectures against a \emph{ground truth}.

There are three aspects to consider when we evaluate an ATE system:

\begin{itemize}
\item The quality of each conjecture, which is the ``interestingness'' discussed
  above.
\item The \emph{set} of conjectures produced, to assess things like consistency
  and redundancy.
\item Performance of the system, to find the balance struck between output
  quality and time taken.
\end{itemize}

Existing evaluation of these three systems have assessed the interestingness of
their output by operating in well-studied theories, and taking a set of existing
theorems as a \emph{ground truth} to compare against.

A common choice of ground truth is the standard library of a theorem prover,
such as Isabelle/HOL; the library authors have gone to the effort of stating,
proving and including these theorems in every copy of their software, which is a
good indication that they are useful or important.

One problem with this choice is the small size of these libraries. For example,
the benchmark based on Isabelle/HOL's theory of natural numbers given
in~\cite{Johansson.Dixon.Bundy:conjecture-generation} contains only 4
definitions and a ground truth of 12 theorems. Whilst such benchmarks allow
objective comparisons between different approaches, their narrow scope doesn't
provide much indication of performance in different, especially \emph{novel}, domains.

\section{Theory Exploration Benchmark}
\label{sec:proposal}

We propose a larger, more ambitious method for evaluating ATE systems, which we
call the Theory Exploration Benchmark (TEB). We take a similar approach to prior
evaluations, but our ground truth comes from the extensive test suites which are
already used to benchmark automated \emph{theorem provers}; which we repurpose
for use in the theory \emph{exploration} setting.

To do this we can take a set of ATP tasks, for example proving the equation
\ref{eq:mapreduce} from the theory in figure \ref{figure:list_theory}, and we
split apart the definitions (to form a theory) from the theorem statements (to
form our ground truth).

We focus on the Tons of Inductive Problems (TIP) theorem proving
benchmark~\cite{claessen2015tip}, since it has several desirable properties:

\begin{itemize}
\item Each problem provides separate definitions and a theorem statement; hence
  it's not difficult to tease them apart for our purposes.
\item Higher-order functions and inductively defined types are provided, which
  corresponds to our desired domain.
\item All together, TIP provides 219 distinct function definitions and 343
  theorem statements, which is enough to fully exercise current ATE systems.
\item Benchmark problems include examples from the theorem proving literature
  as well as common program verification tasks, ensuring the resulting corpus is
  relevant to researchers and practitioners.
\item TIP's maintainers provide a set of tools, including translators from TIP's
  native format (based on SMT-Lib~\cite{BarFT-SMTLIB}) to Haskell and Isabelle
  (which are used by ATE systems).
\end{itemize}

From TIP, we automatically derive our Theory Exploration Benchmark (TEB), and
provide tools to benchmark ATE systems and compare their results. Our
methodology is as follows:

% Better presentation, flowchart?
\begin{itemize}
\item Collect together all definitions found in TIP into one large theory,
  removing $\alpha$-equivalent duplicates.
\item For each type, provide extra functions which act as constructors and
  destructors. For example, the $\text{List}$ type would have extra constructor
  functions $\text{constructor-Nil}$ and $\text{constructor-Cons}$, and
  destructor functions $\text{destructor-head}$ and $\text{destructor-tail}$.
  This allows us to ignore constructors and destructors, and concentrate solely
  on functions.
\item Collect together all theorem statements found in TIP. Update any
  references to removed definitions, and replace references to constructors with
  the associated constructor function from above.
\item \emph{Sample} a sub-set of functions from the large theory uniformly, such
  that we choose all of the names referenced by at least one of the theorems
  (for example, theorem \ref{eq:mapreduce} references the names
  $\text{map}$ and $\text{append}$).
\item Provide this sub-set as an input to the ATE system, timing how long it
  takes to complete.
\item Perform precision/recall analysis on the resulting conjectures, against
  the ground truth of TIP theorems which only reference names in this sample.
\end{itemize}

\section{Application}
\label{sec:application}

% evaluation

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy theory
exploration systems. To assess the scalability of their algorithms, we sampled
theories of various sizes: from theories containing only a single definition, up
to theories containing 20 definitions.

To provide a more robust comparison of these two systems, we also applied a
paired difference test: measuring, for each sampled theory, the difference in
time, precision and recall between the two systems.

\section{Discussion}
\label{sec:discussion}

% philosophical bits

\section{Conclusion}
\label{sec:conclusion}

% future work, etc.

Our benchmark suite provides a unifying goal for the diverse approaches of
theory exploration, whilst avoiding some of the philosophical complications of
the field. Whilst the current implementation is quite limited, we welcome
additions from other researchers, to more closely align the benchmark with their
goals, and the strenghts and weaknesses of their systems.

``Solving'' this benchmark suite would not solve the problem of theory exploration in general,
so more ambitious goals must be set in the future; but we have found that there is still a
long way to go until that problem arises.

\begin{acknowledgements}
  We are grateful to Jianguo Zhang for help with our statistical analysis.

  EPSRC
\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{csvsimple}
\usepackage{float}
\restylefloat{table}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{arrows, calc, decorations.pathreplacing, positioning,
  shapes.geometric}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}

\newcommand{\etal}{{\em et al.}}
\newcommand{\Bagai}{Bagai \etal{}}

% For example theory of lists
\newcommand{\function}{\rightarrow}

\newcommand{\Zero}{\text{Z}}
\newcommand{\Succ}{\text{S}}
\newcommand{\plus}{\text{plus}}
\newcommand{\mult}{\text{times}}

\newcommand{\List}{\text{List}}
\newcommand{\ListA}{\text{List} \  a}
\newcommand{\Nil}{\text{Nil}}
\newcommand{\Cons}{\text{Cons}}
\newcommand{\Head}{\text{head}}
\newcommand{\Tail}{\text{tail}}
\newcommand{\Append}{\text{append}}
\newcommand{\Reverse}{\text{reverse}}
\newcommand{\Length}{\text{length}}
\newcommand{\Map}{\text{map}}
\newcommand{\Foldl}{\text{foldl}}
\newcommand{\Foldr}{\text{foldr}}

% For interestingness table
\newcommand{\iE}{\textbf{E}}
\newcommand{\iN}{\textbf{N}}
\newcommand{\iS}{\textbf{S}}
\newcommand{\iA}{\textbf{A}}
\newcommand{\iC}{\textbf{C}}
\newcommand{\iU}{\textbf{U}}
\newcommand{\tIFF}{if-and-only-if}
\newcommand{\tNE}{non-exists}
\newcommand{\tIMP}{implies}
\newcommand{\tEQ}{equations}
\newcommand{\tINE}{inequalities}
\newcommand{\tCON}{conditional}
\newcommand{\tRow}[1]{#1 \\ \hline}

% Insert the name of "your journal" with
\journalname{Journal of Automated Reasoning}

\begin{document}

\title{Quantitative Benchmarking for Automatically Generated Conjectures%\thanks
% {Grants or other notes about the article that should go on the front page
% should be placed here. General acknowledgments should be placed at the end of
% the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton \and
        Alison Pease    \and
        Jianguo Zhang
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
           University of Dundee \\
           \email{c.m.warburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           A. Pease \at
           University of Dundee \\
           \email{a.pease@dundee.ac.uk}
           \and
           J. Zhang \at
           University of Dundee \\
           \email{j.n.zhang@dundee.ac.uk}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

% Look carefully at each tool's (paper's) eval section

% Make it clear that methodology is the point (it's not just a detail, like in a
% chemistry paper for example)

\begin{abstract}
  We propose a benchmarking methodology to evaluate the efficiency and quality
  of \emph{conjecture generation} by automated tools for \emph{mathematical
    theory exploration}. Our approach uses widely available theorem proving
  tasks as a \emph{ground-truth} corpus, and we demonstrate its use on the
  QuickSpec and IsaCoSy tools, finding that the former takes significantly less
  time to produce significantly more ``interesting'' output. By providing a
  standard, cross-tool evaluation technique we hope to encourage innovation and
  comparison between the disparate approaches currently being pursued, and spur
  improvements similar to those seen in the competitive field of automated
  theorem proving.
%\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% Lay out the field of TE: motivation, tools, evaluation, problems...

\emph{Conjecture generation/formation}, a sub-field of \emph{mathematical theory
  exploration} (MTE), is the open-ended problem of producing conjectures about a
given logical theory which are somehow ``interesting''.

This has applications wherever we find use for formal statements: in proof
assistants and their libraries, in mathematics education and research, and in
the specification, verification, optimisation and testing of software.

Existing attempts at tackling this problem are difficult to compare, due
partially to the variety of approaches taken, but also because of the inherent
ambiguity of the task and the different goals emphasised by their designers and
their choice of evaluation method.

We attempt to solve this discrepancy, at least for the foreseeable future, by
defining a standard, unambiguous benchmarking approach with which to compare
the conjecture generation of MTE tools. Our contributions include:

\begin{itemize}
\item A general methodology for benchmarking conjecture generation.
\item Resolving the issue of ``interestingness'' through the use of
  theorem-proving benchmarks as a ground-truth.
\item A specific instantiation of this methodology, using the Tons of Inductive
  Problems benchmark as a corpus.
\item Automated tooling to perform this benchmarking.
\item Application of our methodology to the QuickSpec and IsaCoSy MTE tools,
  and a comparison and discussion of the results.
\end{itemize}

We introduce the conjecture generation problem in more detail, along with the
difficulty of comparing existing solutions, in $\S$\ref{sec:background}. In
$\S$\ref{exploration-versus-formation} we give a history of the MTE field, and
its close cousin Automated Theory Formation (ATF), describing a variety of
existing approaches and clarifying the diverse terminology found in the
literature. We explain our proposal for a more general benchmark in
$\S$\ref{sec:proposal} and demonstrate its application to existing MTE tools
in $\S$\ref{sec:application}. Issues facing our approach, and the field in
general, are discussed in $\S$\ref{sec:discussion} and concluding remarks
are given in $\S$\ref{sec:conclusion}.

%- Existing approaches are difficult to compare
% - They want different things
% - They're evaluated differently
% - They're evaluated ``locally'' (as white boxen)

% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

\section{Background}
\label{sec:background}

\subsection{Motivation}
\label{sec:motivation}

% more detail about evaluation, interestingness, etc. Reference Simon/Alan
% interestingness, e.g. examples from textbooks, etc.

\begin{figure}
  \begin{equation*}
    \begin{split}
      \forall a. \Nil            &: \ListA                                  \\
      \forall a. \Cons           &: a \rightarrow \ListA \rightarrow \ListA \\
      \Head(\Cons(x, xs))        &= x                                       \\
      \Tail(\Cons(x, xs))        &= xs                                      \\
      \Append(\Nil,         ys)  &= ys                                      \\
      \Append(\Cons(x, xs), ys)  &= \Cons(x, \Append(xs, ys))               \\
      \Reverse(\Nil)             &= \Nil                                    \\
      \Reverse(\Cons(x, xs))     &= \Append(\Reverse(xs), \Cons(x, \Nil))   \\
      \Length(\Nil)              &= \Zero                                   \\
      \Length(\Cons(x, xs))      &= \Succ (\Length(xs))                     \\
      \Map(f, \Nil)              &= \Nil                                    \\
      \Map(f, \Cons(x, xs))      &= \Cons(f(x), \Map(f, xs))                \\
      \Foldl(f, x, \Nil)         &= x                                       \\
      \Foldl(f, x, \Cons(y, ys)) &= \Foldl(f, f(x, y), ys)                  \\
      \Foldr(f, \Nil,         y) &= y                                       \\
      \Foldr(f, \Cons(x, xs), y) &= f(x, \Foldr(f, xs, y))
    \end{split}
  \end{equation*}
  \caption{A simple theory defining a $\List$ type and some associated
    operations, taken from~\cite{Johansson.Dixon.Bundy:conjecture-generation}.
    $\Zero$ and $\Succ$ are from a Peano encoding of the natural numbers.}
  \label{figure:list_theory}
\end{figure}

Given a logical theory, such as the theory of lists shown in
Figure~\ref{figure:list_theory}, we may want to find theorems which describe its
behaviour. This could be for mathematical curiosity, or due to the theory's
importance in some domain. In particular, for theories which capture the
semantics of some software library, we may want to verify that certain
(un)desirable properties do (not) hold. We might also want to \emph{optimise}
programs using this library, rewriting expressions into a form which requires
less time, memory, network usage, etc. To avoid altering a program's result,
such rewrites should come with theorems proving their correctness, such as the
following theorem for our theory of lists:

\begin{equation} \label{eq:mapreduce}
  \forall f. \forall xs. \forall ys.
    \Map(f, \Append(xs, ys)) = \Append(\Map(f, xs), \Map(f, ys))
\end{equation}

Equation~\ref{eq:mapreduce} justifies a rewrite rule for splitting a single
$\Map$ call into multiple independent calls dealing with different sections of a
list. Such rules can be useful optimisations, for example in a distributed
context where each call can be evaluated in parallel (this is the basis of the
``map/reduce'' programming paradigm).

All of these example use cases require the ability to discover theorems about
some particular definitions. This is a hard problem in general, but presents
opportunities for automation due to the precise, symbolic nature of the domain.

\subsection{Automating Construction and Exploration of Theories}
\label{sec:te}

The task of discovering theorems in an arbitrary theory can be
described as the interplay of several processes: (i) formulating new
definitions and concepts; (ii) finding patterns suitable for posing as
conjectures; and (iii) proving that those conjectures are theorems.
The latter is studied extensively in the field of Automated Theorem
Proving (ATP). Far less attention has been paid to automating the
first two tasks, which both Automated Theory Formation (ATF) and
Mathematical Theory Exploration (MTE) address.  We discuss the relationship
between ATF and ATE in historical context in
$\S$\ref{exploration-versus-formation}.

A major challenge for both ATF and MTE is choosing how to narrow down
the set of generated conjectures to those deemed ``interesting'', since this is
an imprecise term with many different interpretations. For example, all existing
approaches agree that simple tautologies are ``uninteresting'', but differ when
it comes to more complex statements.  Colton \etal{} give a survey of tools for
theory formation and exploration, and their associated notions of
``interestingness'' for concepts and conjectures~\cite{colton2000notion}. Six
notions are identified, which are applied to conjectures as follows:

{\bf Empirical plausibility} checks whether a property holds across some
specific examples. This is especially useful for avoiding false conjectures,
without resorting to a full proof search.

{\bf Novelty} depends on whether a conjecture, or one isomorphic or more
general, has already been seen.

{\bf Surprisingness} of a conjecture is whether or not it is
``obvious'', for example if it is an instance of a tautology.

{\bf Applicability} depends on the number of models in which a
conjecture holds. The tool of \Bagai{} conjectures the non-existence of
objects, and hence favours statements with zero applicability. Other tools treat
applicability as a positive aspect: the more applicable the statement, the more
interesting it is.

{\bf Comprehensibility} depends on the complexity of a statement. Simpler
statements are considered more interesting, hence many of the tools explore
simpler/smaller statements before complex/larger ones, to more efficiently find
those which are interesting.

{\bf Utility} is the relevance or usefulness of a conjecture to the
user's particular task. For example, if we want to find optimising
rewrite rules such as equation~\ref{eq:mapreduce}, then utility would include
whether or not a conjecture justifies a rewrite rule, the difference in
resource usage of the expressions involved, and how common those
expressions are in real usage.


\begin{table}
  \centering
  \begin{tabular}{ |l|l|c|c|c|c|c|c| }
    \hline
    \multirow{2}{*}{\textbf{Program}}                      &
    \multirow{2}{*}{\textbf{Conjecture Types}}             &
    \multicolumn{6}{c|}{\textbf{Interestingness Measures}} \\ \hhline{~~------}
    \tRow{          &                     & \iE & \iN & \iS & \iA & \iC & \iU}
    \tRow{AM        & \tIFF, \tIMP, \tNE  &   X &   X &   X &   X &   X &   X}
    \tRow{GT        & \tIFF, \tIMP, \tNE  &   X &   X &   X &   X &   X &   X}
    \tRow{Graffiti  & \tINE               &   X &   X &   X &     &   X &   X}
    \tRow{\Bagai{}  & \tNE                &     &   X &     &   X &   X &    }
    \tRow{HR        & \tIFF, \tIMP, \tNE  &   X &   X &   X &   X &   X &   X}
    \tRow{QuickSpec & \tEQ                &   X &   X &     &   X &   X &    }
    \tRow{Speculate & \tCON\ \tEQ / \tINE &   X &   X &     &   X &   X &    }
    \tRow{IsaCoSy   & \tEQ                &   X &   X &     &   X &   X &   X}
    \tRow{IsaScheme & \tEQ                &   X &   X &     &   X &   X &   X}
  \end{tabular}
  \caption{Classification of MTE tools from~\cite{colton2000notion}, extended
    to include four more recent tools. The interestingness measures are
    \iE{}mpirical plausibility, \iN{}ovelty, \iS{}urprisingness,
    \iA{}pplicability, \iC{}omprehensibility (low complexity) and \iU{}tility.}
  \label{table:colton}
\end{table}

% JUSTIFICATIONS
%
% IsaCoSy uses counter-example checking; this ensures empirical plausibility
% IsaCoSy uses constraints to avoid special-cases; this ensures novelty
% IsaCoSy uses utility, since there are some hard-coded patterns which are
% looked for
%
% Speculate uses a form of unification to ensure novelty, making sure one
% equation is not a special-case of another
% Speculate uses LeanCheck to enumerate values, looking for counterexamples
% Speculate uses testing, to ensure Empirical Plausibility
%
% QuickSpec uses QuickCheck to ensure empirical plausibility
% QuickSpec uses a congruence closure algorithm to ensure novelty
%
% IsaScheme uses utility, since it determines the "quality" of a definition
% based on how many theorems it appears in?
% IsaScheme uses novelty, using an equational rewrite system (Knuth-Bendix
% completion) to remove redundancies
% IsaScheme uses utility, since it focuses on the terms and definitions of a
% user's theory. However, don't all of them?
% IsaScheme uses counterexample checking for empirical plausibility.

% ONLY APPLIES TO PRECONDITIONS
% APPLICABILITY: ensure a conjecture holds in many models (or exactly zero, in
% the case of Bagai et al). Does the testing-based approach of QuickSpec and
% Speculate ensure that there's a model? After all, these are concrete values
% rather than e.g. implications of some abstract specification.

We summarise the criteria in Table 1 and state which were used in key
ATF/MTE tools. The entries for AM, GT, Graffiti, \Bagai{} and HR are based
on this survey (the latter is Colton's own tool). We extend this analysis to
some more recent tools in the remaining rows (based on our understanding of
their function): QuickSpec~\cite{QuickSpec},
Speculate~\cite{braquehais2017speculate},
IsaCoSy~\cite{Johansson.Dixon.Bundy:conjecture-generation}
and IsaScheme~\cite{MontanoRivas2011}.

The latter tools are described in more detail in $\S$\ref{section:mte},
but all use testing to check for counterexamples, which ensures results are
empirically plausible and applicable (conditions are satisfiable, types are
inhabited, etc.). They also use a small-to-large search order, ensuring more
easily-comprehensibile conjectures are explored first.

IsaCoSy ensures novelty using a constraint system to prevent special-cases being
generated, whilst the others use post-hoc filters based on unification and term
rewriting to achieve the same result. IsaCoSy is able to look for commonly
desirable patterns, such as commutativity and associativity, which increases the
utility of its output, whilst IsaScheme is completely based around such
pattern-instantiation.

This diversity of approaches makes it difficult to compare the existing
evaluations of these tools directly. In particular, evaluation methods created
for one tool might not make sense for another, due to assumptions made about
the algorithms' operation. For example, the novelty filters used by QuickSpec
and Speculate are applied to the whole output set, guaranteeing that no
special-cases will appear; yet we cannot assume this in the case of IsaCoSy,
since its constraint solver allows special cases if they're found \emph{before}
the general case (it does not go back and discard previous results).

With such ambiguous and varied goals, approaches and assessment criteria it is
difficult to compare MTE tools in a quantitative way, and hence to form some
measure of ``progress'' for the field. Our proposed benchmarking methodology
attempts to remedy this by setting a clear target, hopefully in line with the
myriad goals of existing researchers, whilst being generally applicable to
tools of this type.

\section{Theory Exploration versus Theory
  Formation}\label{exploration-versus-formation}

In this section we put the Mathematical Theory Exploration approach
into context both historically, and with respect to related approaches
such as in Automated Theory Formation tools.

\subsection{A Short History of Automated Mathematics}

\begin{quote}
...``in his subsequent design for an Analytical Engine Mr. Babbage has
shown that material machinery is capable, in theory at least, of
rivalling the labours of the most practised mathematicians in all
branches of their science.''~\cite[p. 498]{jevons}
\end{quote}

The first automated mathematical tool, the mechanical calculator
(known as the Pascaline), was an adding machine that could perform
additions and subtractions directly and multiplication and divisions
by repetitions, and was conceived by Pascal in 1642 while reorganising
tax revenues~\cite{d'ocagne}. Subsequent early tools include
M\"uller's universal calculating machine in 1784, which he built for
the purpose of calculating and printing numerical tables: he invented
this when he had to check and recalculate some tables relating to the
volumes of trees~\cite[p. 65]{lindgren}. Thirty seven years later,
Babbage invented his famous difference engine: an automatic,
mechanical calculator designed to tabulate polynomial functions. This
was inspired by a flawed table of logarithms and the idea that
machines would be quicker and more reliable~\cite{bowden}. It is
interesting to note background and motivation: while Pascal and
Babbage were mathematicians, with an interest in engineering, M\"uller
was an engineer with an interest in mathematical knowledge. All three
tools were conceived as an aid to mathematicians, as well as
scientists, accountants and surveyors.

Differences in background and motivation continue to be relevant
today. While the majority of work in automating mathematics has been
in symbolic manipulation and theorem proving, we are concerned here
with other aspects of mathematics, including the construction of conjectures,
(counter\nobreakdash-)examples and theorems. These tasks, along with the
formation of new definitions and concepts, are varyingly known as
``Automated Theory Formation''~\cite{lenat:77,colton:book},
``Mathematical Theory Exploration''~\cite{buchberger:06} (also sometimes
prefaced with ``Computer-Aided'', ``Automated'' or ``Algorithm-Supported''),
``Automated Mathematical Discovery''~\cite{epstein:91,colton:interestingness,esarm2008},
``Concept Formation in Discovery Systems''~\cite{haase}, and
``Automated Theorem Discovery''~\cite{roy}. Such a plethora of terminology can
be unhelpful and can mask similarities between the different fields. In
particular, the twin strands of Automated Theory Formation and
Automated Mathematical Theory Exploration seem to be developing
somewhat independently without a clear differentiating
methodology. Below we discuss commonalities and differences between
the two schools of thought.

We limit our discussion to applications in mathematics, although there are
similar approaches being applied to the task of Computational \emph{Scientific}
Discovery~\cite{king2004functional,Williams20141289,schmidt2009distilling}.
Scientific knowledge relies on inductive reasoning and experimental testing,
which (in principle) are not so important to a mathematical theory. However,
these techniques \emph{do} play an important role in many mathematical theory
formation and exploration tools: in particular, inductive reasoning allows a
tool to conjecture statements which may be too difficult for it to deductively
prove; whilst experimental testing, e.g. by checking universally-quantified
statements against some specific values, is a simple way to quickly discard many
obvious falsehoods generated by powerful but imperfect algorithms (as opposed
to, say, limiting ourselves to verified theorem generators).

\subsection{Automated Theory Formation}
\label{section:atf}

% 1. history of terminology
Automated theory formation derives its terminology from psychology in which the
term ``concept formation'' is used (see, for example,~\cite{bruner:67}) to
describe the search for features which differentiate exemplars from
non-exemplars of various categories. Lenat used this term in his 1977 paper:
{\em Automated Theory Formation in Mathematics}~\cite{lenat:77}.

% 4. example tools - AM, HR, ...
When Lenat built the AM system~\cite{lenat:77}, there were systems
which could define new concepts for investigation, such as those
described in~\cite{winston}, and systems which could discover
relationships among known concepts, such as Meta-Dendral~\cite{buchanan:75}. No
system could perform both of these tasks: Lenat saw this as the next step. AM
was designed to both construct new concepts and conjecture relationships between
them; fully automating the cycle of discovery in mathematics. Lenat describes
this as follows:

\begin{quote}
``What we are describing is a computer program which
defines new concepts, investigates them, notices
regularities in the data about them, and conjectures
relationships between them. This new information is used
by the program to evaluate the newly-defined concepts,
concentrate upon the most interesting ones, and iterate the
entire process.''~\cite[p. 834]{lenat:77}
\end{quote}

AM was a rule-based system which used a frame-like scheme to represent
its knowledge, enlarged its knowledge base via a collection of
heuristic rules, and controlled the firing of these rules via an
agenda mechanism. Lenat chose elementary arithmetic as the development
domain because he could use personal introspection for the heuristics
for constructing and evaluating concepts. Given the age of this
discipline, Lenat thought it unlikely that AM would make significant
discoveries, although he did cite its ``ultimate achievements'' as the
concepts and conjectures it discovered (or could have discovered). He
suggested various criteria by which his system could be evaluated,
many of which focused on an exploration of the techniques. For
instance, he considered generality (running AM in new domains) and how
finely-tuned various aspects of the program are (the agenda, the
interaction of the heuristics, etc). %most of which were qualitative.
Lenat saw his system and future developments in this field as having
implications for mathematics itself (finding results of significance),
for automating mathematics research (developing AI techniques), and
for designing ``scientist assistant'' programs (aids for
mathematicians). This shows a broad spread of motivation. Despite the
seeming success of the AM system, it is one of the most criticised
pieces of AI research. In their case study in methodology, Ritchie and
Hanna analysed Lenat's written work on AM and found that there was a
large discrepancy between his theoretical claims and the implemented
program~\cite{partridge}. For instance, Lenat made claims about how AM
invented natural numbers from sets, whereas it used one heuristic
which was specifically written in order to make this connection (and
not used in any other context). Another problem was that the processes
were sometimes under-explained. For an argument of why many of the
claims made by Lenat about AM were false, see chapter 13 of~\cite{colton:book}.

\subsection{Mathematical Theory Exploration}
\label{section:mte}

The phrase Mathematical Theory Exploration (MTE) is a recent term
which seems to have originated with Buchberger and colleagues (see,
for example,~\cite{buchberger}). His motivation is to support
mathematicians during their exploration of mathematical theories. This
support is intended to be for the straightforward reasoning, which he
argues, covers most mathematical thought, rather than the ingenious
points, which he leaves for human mathematicians. Buchberger's long
term goal is to provide routine tools for the exploration activity of
working mathematicians, to support the invention and structured
build-up of mathematical knowledge. The Theorema project aims at
prototyping features of a system for such a purpose. These features
include Integration of the Functionality of Current Mathematical
Systems (retention of the full power of current numerics and computer
algebra systems, as well as enabling the user to add their own
algorithms to the system); Attractive Syntax (input and output is
readable and presented attractively, and can be personalised by the
user); and Structured Mathematical Knowledge Bases (tools are provided
for building and using large mathematical knowledge
libraries). Buchberger has evaluated the potential of this strategy by
illustrating the automated synthesis of his own Gr\"obner bases
algorithm~\cite{buchberger:04}.

Recent systems developed in this area include IsaScheme~\cite{MontanoRivas2011},
IsaCoSy~\cite{Johansson.Dixon.Bundy:conjecture-generation},
QuickSpec~\cite{QuickSpec} and Speculate~\cite{braquehais2017speculate}
(included in table~\ref{table:colton}) as well as MATHsAiD~\cite{roy}.

The goal of the MATHsAiD (Mechanically Ascertaining Theorems from Hypotheses,
Axioms and Definitions) project is to build a tool which takes in a set of
axioms, concept definitions and a logic and applies its inference rules to
reason from the axioms to theorems. The motivation is to produce a tool which
will help mathematicians to explore the consequences of a set of axioms or a
particular concept definition. One of the main challenges of the project has
been to automatically evaluate interestingness: to distinguish important
theorems from results which, although they follow from a set of axioms, are of
little mathematical interest.

%IsaScheme
Monta{\~n}o-Rivas \etal{} have implemented a scheme-based approach to MTE in their
IsaScheme system~\cite{MontanoRivas2011}. Schemes are higher-order formulae
which can be used to generate new concepts and conjectures; variables within the
scheme are instantiated automatically and this drives the invention process.
For instance, in the theory of natural numbers, given the concepts of zero
($\Zero$), successor ($\Succ$) and addition ($\plus$), IsaScheme can use the
following scheme to invent the concept of multiplication:

\[
\text{def-scheme}(g,h,i,j) \equiv\\
  \exists f. \forall x y. \left\{
  \begin{array}{l l}
    f(g,y) = h (y)  \: \text{and}  & \\
    f(i(x),y) = j(y,f(x,y)) & \\
  \end{array} \right.
\]

where the existentially quantified variable $f$ stands for the new
function to be defined in terms of the variables $g$, $h$, $i$ and $j$. Within
the theory of natural numbers, IsaScheme instantiates this scheme with
$\sigma_1 = \{g \mapsto \Zero, h \mapsto (\lambda x.\Zero), i \mapsto \Succ, j
\mapsto \plus\}$. In this example, $f \mapsto \mult$ (multiplication), since:

\begin{align*}
\mult(\Zero, y)  &= \Zero \\
\mult(\Succ(x), y) &= \plus(y, \mult(x, y))
\end{align*}

The new multiplication function $f$ can itself be used to instantiate
variables in the same scheme, resulting in the invention of the
exponentiation concept (these examples are taken from~\cite{MontanoRivas2011}).

%IsaCoSy
The IsaCoSy system (Isabelle Conjecture Synthesis) conjectures equations by
enumerating expressions involving a given set of (typed) constants and free
variables (a signature). A constraint solver forbids certain (sub)expressions
from being synthesised, and these constraints are extended whenever a new
conjecture is generated, to avoid generating any special-cases of this
conjecture in the future. Conjectures are tested by sending them to the
QuickCheck counterexample finder and, if no counterexamples are found, then
sent to IsaPlanner which attempts to prove them.

%QuickSpec
QuickSpec emerged from work on the QuickCheck software testing framework for the
Haskell programming language, and has been used to generate conjectures for the
subsequent HipSpec and Hipster MTE systems. Like IsaCoSy, a signature is
provided and terms are enumerated. Rather than a constraint system, QuickSpec
version 1 collects expressions of the same type into equivalence classes,
assuming them to be equal. QuickCheck is used to test this assumption by
instantiating each variable with a randomly chosen value, evaluating the
resulting expressions and comparing them. Any equivalence class whose elements
don't compare equal are split up to separate those observably-different
expressions, and the process is repeated with new random values.

After 500 rounds of testing, any expressions still sharing the same equivalence
class are conjectured to be equal for all values of their variables. Finally, a
congruence closure algorithm is applied to these equations, to remove
redundancies and special-cases.

QuickSpec version 2 replaces this two-step process with a single, iterative
algorithm similar to that of IsaCoSy. Generated conjectures are fed into a
Knuth-Bendix completion algorithm to form a corresponding set of rewrite rules.
As expressions are enumerated, they are simplified using these rules and
discarded if equal to a known expression. If not, QuickCheck tests whether the
new expression can be distinguished from the known expressions through random
testing: those which can are added to the set of known expressions. Those which
cannot be distinguished are conjectured to be equal, and the rewrite rules are
updated.

%Speculate
Speculate is a Haskell tool which operates in a similar way to QuickSpec
version 2. In addition to equations, Speculate uses the laws of total orders and
Boolean algebra, along with testing (via LeanCheck), to conjecture inequalities
and conditional relations between expressions, such as (for
$x, y \in \mathbb{Z}$):

\begin{equation*}
  \begin{aligned}
    x + y \leq \lvert x \rvert + \lvert y \rvert \\
    x \leq 0 \rightarrow x + \lvert x \rvert = 0
  \end{aligned}
\end{equation*}

\subsection{Commonalities and Differences between MTE and ATF}

We believe that ATF and MTE share many goals and would benefit from a
closer alignment: both fields are highly specialised and by joining
forces (while being explicit about differences) the techniques being
developed would have greater impact. This closer alignment is already
beginning to take place. The main commonalities between MTE and ATF
are that both are interested in the same aspects of mathematical
thinking, aiming to automatically construct and evaluate elements of a
mathematical theory, including concepts, conjectures, theorems, axioms
and examples. Both strands contrast themselves with Automated Theorem
Proving. The main historical differences between these two approaches seem to be
that MTE was initiated by mathematicians as an attempt to support existing
mathematical research, perhaps with user interaction; while ATF is a product of
AI researchers trying to extend AI techniques, with full automation and often
applications to non-mathematical domains.

The different backgrounds of the people who named the fields
``automated theory formation'' (Lenat) and ``mathematical theory
exploration'' (Buchberger) perhaps reflects different metaphysical
perspectives on invention (in which case new mathematical ideas are
being {\em formed}, created or produced) and discovery (in which case
abstract mathematical objects exist independently of us and are {\em
  explored} or investigated) in mathematics.\footnote{There is a vast
  literature on the Platonic and the constructivist view of
  mathematics: we shall not venture down this path here (interested
  readers are referred to~\cite{hersh:97,shapiro}). Instead, for now,
  we make the pragmatic assumption that while there may well be
  different cognitive processes involved in invention than those
  involved in discovery, the fields of ATF and MTE are not currently
  concerned with this level of detail and so the philosophical
  distinction is not relevant for our purposes.} As can be seen above,
some of these historical differences are disappearing, and recent
implementations are bridging the methodological gap between ATF and MTE. This is
particularly true in terms of fully-automatic/user-interactive and the
domain-specific/general aspects.

\subsection{Existing Evaluations}
\label{sec:existing}

There are three aspects to consider when evaluating a tool for conjecture
generation:

\begin{enumerate}
\item The quality of each individual conjecture, which we will refer to as their
  ``interestingness''.
\item The quality of the \emph{set} of generated conjectures, which is important
  for criteria like novelty, as well as to assess consistency.
\item Resources used by the tool, to find the balance struck between output
  quality and time taken.
\end{enumerate}

We make the assumption that each tool is intended to be \emph{general}, and
hence isn't hard-coded or fine-tuned to only perform well in a narrow domain.
Examples where this does \emph{not} hold include the natural number heuristics
of AM discussed in section~\ref{section:atf}, as well as machine learning models
trained on some specific dataset (such as this benchmark).

Evaluation of new tools and techniques for conjecture generation tends to be
\emph{ad hoc} and hence difficult to compare, whilst comparative surveys (such
as Colton's) are descriptive and qualitative rather than providing direct
numerical measurements.

One quantitative approach which has gained traction is \emph{precision/recall
  analysis}. This has been applied to IsaCoSy, IsaScheme and HipSpec, the latter
providing a direct comparison of these three
tools~\cite{claessen2013automating}.\footnote{Unfortunately this comparison only
  reports prior results from the other tools, rather than reproducing them.
  This makes the numbers less comparable, since each tool was tested with
  slightly different definitions (e.g. whether or not the exponential and
  maximum functions were included).} To measure precision and recall we first
choose a set of definitions (such as those in figure \ref{figure:list_theory})
and a \emph{ground truth} to treat as the ``ideal'' set of conjectures we would
like a tool to produce for those definitions (e.g. this might include
equation~\ref{eq:mapreduce}). Each tool is run and their outputs are compared to
the ground truth. To score 100\% on precision and recall, all of the conjectures
which appear in the ground truth must be generated, and nothing else:

\begin{itemize}
\item \emph{Precision} is the proportion of a tool's output which appears in
  the ground truth. This penalises overly-liberal tools which output large
  numbers of conjectures in the hope that some turn out to be ``good''.
\item \emph{Recall} is the proportion of the ground truth which appears in the
  tool's output. This penalises overly-conservative tools which generate very
  few conjectures as a way to avoid ``bad'' ones.
\end{itemize}

Precision and recall act as quality tests for sets of conjectures, fulfilling
one of our requirements. They also provide a coarse quality assessment for
individual conjectures: either a conjecture appears in the ground truth set or
it doesn't.

The analyses in~\cite{claessen2013automating} took their definitions and ground
truths from the standard library of the Isabelle theorem prover: since the
library authors have gone to the effort of stating, proving and including these
theorems in every copy of their software, this is a good indication that they
are interesting.

Two tests were performed: one on a theory of natural numbers and one on a theory
of lists (whose definitions are shown in figure~\ref{figure:list_theory}). These
tests are limited both in number (it is difficult to draw general conclusions
from two datapoints) and by the small size of these theories (e.g. the natural
number theory from~\cite{Johansson.Dixon.Bundy:conjecture-generation} contains
only 4 definitions and 12 theorems).

We believe that this is a viable approach to drawing meaningful, objective
conclusions about such tools, but larger data sets are needed to judge
performance in different, especially \emph{novel}, domains (which, after all, is
the purpose of such tools). In addition, we might be concerned that such
``popular'' theories may be unrepresentative of typical usage. For example, we
would not expect the library functions in a typical verification problem to be
as mathematically rich as number theory.

\section{Theory Exploration Benchmark}
\label{sec:proposal}

Our main contribution is a benchmarking methodology, shown in figure
\ref{figure:flow_chart}, which both generates a large definition/ground-truth
corpus, and provides a scalable, statistical approach to evaluating MTE tools
using this corpus. We follow a precision/recall approach similar to prior work,
with the main difference being the source of definitions and ground truths: we
take existing problem sets designed for automated theorem proving, and adapt
their content for use in the conjecture generation setting.

\subsection{Preparation}
\label{section:prep}

Automated theorem proving is an active area of research, with large problem sets
and regular competitions to prove as much as possible, as fast as
possible~\cite{pelletier2002development}. These problem sets are an opportunity
for MTE, as their definitions and theorems can be used as a corpus in the same
way that Isabelle libraries have been used in the past.

Some problem sets are more amenable for this purpose than others. The most
suitable are those which have the following properties:

\begin{itemize}
\item For each problem, there should be a clear distinction between the theorem
  to be proved and the definitions involved, such that the two can be easily and
  meaningfully separated. This rules out problem sets like those of
  SMT-COMP~\cite{barrett2005smt}, where many problems involve uninterpreted
  functions, whose behaviour is \emph{implicit} in the logical structure of the
  theorem statement but not separately \emph{defined}.
\item Definitions should be translatable into a form suitable for the MTE tools
  under study. Our application translates to Haskell and Isabelle, which also
  benefits from having definitions be strongly typed.
\item The problem set should be relevant to the desired domain. Our focus on
  functional programming requires higher-order functions and inductively defined
  datatypes, which rules out first-order languages/logics (such as
  TPTP~\cite{sutcliffe2009tptp}).
\item Since it will act as our ground truth, the problem set should ideally
  contain every ``interesting'' conjecture involving its included
  definitions. Realistically, we should aim for each definition to appear in
  \emph{many} theorem statements; rather than each problem having unique
  definitions.
\item The problem set should be as large as possible, for robustness of the
  resulting statistics.
\end{itemize}

Once such a problem set has been chosen, we must separate the definitions
referenced by the theorems from the theorem statements themselves. The former
will be used as input to the tools, whilst the latter form a ground truth corpus
against which to compare the output.

It is important to ensure that there are no duplicate definitions: we are only
concerned with the \emph{logical} content of the input, not the more arbitrary
aspects like the names of functions. For example, consider a problem set which
includes a commutativity theorem for a \texttt{plus} function, and an
associativity theorem for an \texttt{add} function, where the definitions of
\texttt{plus} and \texttt{add} are $\alpha$-equivalent. We would expect an MTE
tool to either conjecture commutativity and associativity for \emph{both}
functions, or for \emph{neither} function, since they're logically equivalent.
Yet a na\"ive precision/recall analysis would treat commutativity of
\texttt{add} and associativity of \texttt{plus} as \emph{uninteresting}, since
they don't appear in the ground truth.

For this reason, duplicates should be removed, and any references in the theorem
statements updated to use the remaining definition (e.g. chosen based on
lexicographic order). In the above example, the \texttt{plus} function would be
removed, and the commutativity theorem updated to reference \texttt{add}
instead.

\subsection{Sampling}

We could, in theory, send these de-duplicated definitions straight into an MTE
tool and use the updated theorem statements as the ground truth for analysis.
However, this would cause two problems:

\begin{itemize}
\item The result would be a single data point, which makes it difficult to
  infer performance \emph{in general}.
\item It is impractical to run existing MTE tools on ``large'' inputs,
  containing more than a few dozen definitions.
\end{itemize}

To solve both of these problems we instead \emph{sample} a subset of
definitions.\footnote{This could be done randomly, but for reproducibility we
  use a deterministic order (based on cryptographic hashes, to discourage
  ``cheating'').} Given a sample size, we choose a subset of that many definitions,
and provide only those as input to the tool. We generate a corresponding
ground truth by selecting those theorems from the corpus which ``depend on''
(contain references to) \emph{only} the definitions in that sample. Transitive
dependencies aren't required (e.g. a theorem which only references
\texttt{times} does not depend on \texttt{plus}, even if \texttt{plus} occurs in
the definition of \texttt{times}).

Unfortunately, uniform sampling of definitions gives rise to a lottery: for a
given sample size, increasing the size of the corpus (which provides better
statistics) makes it less likely that a chosen sample will contain all of
a theorem's dependencies. The majority of such samples would hence have an empty
ground truth, and thus $0$ precision and undefined recall \emph{independent} of
the tool's output! This is clearly undesirable as an evaluation method.

Instead, we only allow a sample if it contains all dependencies of at least one
theorem. We could do this using rejection sampling, but it is more efficient to
pick a theorem, weighted in proportion to their number of dependencies and
ignoring those with more dependencies than our sample size. That theorem's
dependencies become our sample, padded up to the required size with uniform
choices from the remaining definitions.

The ground truth for such samples is guaranteed to contain at least one theorem
(the one we picked), and hence the precision and recall will depend meaningfully
on the tool's output.

\subsection{Evaluation}

Given a sample of definitions and a corresponding ground truth, the actual
execution of the MTE tool proceeds as in prior work. We must translate the
chosen definitions into the required input format, then we time the execution
with a timeout (e.g. 5 minutes).

In our experiments we have found that memory usage is also an important part of
a tool's performance, but rather than complicating our analysis with an extra
dimension, we instead allow programs to use as much memory as they like, and
either get killed by the operating system or slow down so much from swapping
that they time out. This is in line with the expected usage of these tools:
either there is enough memory, or there isn't; implementations shouldn't be
penalised for making use of available resources.

To calculate precision and recall, the conjectures generated by each tool, as
well as the ground-truth theorems, need to be parsed into a common format and
compared syntactically after normalising away ``irrelevant'' details. We
consider variable naming to be irrelevant, which can be normalised by numbering
free variables from left to right and using de Bruijn indices for bound
variables. We also consider the left/right order of equations to be irrelevant,
which we normalise by choosing whichever order results in the
lexicographically-smallest expression.

Other than $\alpha$-equivalence and equation symmetry, we specifically
\emph{ignore} logical relationships between syntactically distinct statements,
such as one equation being implied by another. Whilst logically sound,
second-guessing the ground truth in this way would harm other aspects which
influence interestingness (for example, a more general statement is more widely
applicable, but it might also be harder to comprehend).

This procedure gives us a single runtime, precision and recall value for each
sample. We propose two methods for analysing this data: \emph{summarising}
the performance of a single MTE tool and \emph{comparing} the performance of two
tools.

\subsection{Summarising}

Each sample is only explored once by each tool, so that we cover as many
independent samples as possible to better estimate how a tool's performance
generalises to unseen inputs. How we combine these data into an aggregate
summary depends on what we are interested in measuring.

One general question we might ask is how a tool's performance scales with
respect to the input size (the number of given definitions). This is
straightforward to measure by varying the sample size, but we need some way to
combine the results from samples of the same size.

We can summarise runtimes by choosing their median, as this is more robust than
the mean against long-running outliers, and hence represents performance for a
``typical'' input of that size. Unlike previous one-off evaluations, we can also
compute the \emph{spread} of our data, for example with the inter-quartile
range.

The precision and recall for multiple samples can be combined in two ways. We
can calculate their mean value, known as the \emph{average of the ratios}, to
find the expected precision and recall for runs of this size. Alternatively, we
can sum their numerators and denominators to get the \emph{ratio of the
  averages}: this normalises the output and ground truth cardinalities, allowing
precision and recall to be modelled as simple Bernoulli processes.

\subsection{Comparison}

To measure progress and encourage competition in the field, it is important to
compare the relative performance of different tools on the same task. Since the
aggregate statistics in the above summaries have obscured the details of
specific runs, any comparison based on them (such as comparing average recall)
would have very low statistical power. More direct comparisons can be made using
\emph{paired} tests, since for each individual sample we have measurements for
\emph{both} tools.

We propose that running times are compared using a Wilcoxon signed-rank
test~\cite{wilcoxon1945individual}, since it does not require that the data be
normally distributed. In our application we follow a variant of the Speedup-Test
protocol of Touati, Worms and Briais~\cite{touati2013speedup}; the differences
are our use of the paired form of the Wilcoxon test, and the fact that each run
was performed on a different sample (to reduce sampling bias, at the expense of
accuracy).

Another benefit to such a paired difference test is its robustness to the
censoring caused by timeouts: upper-bounding the run time gives an upper-bound
to the observable difference, which biases our results \emph{towards} the null
hypothesis of indistinguishable performance.

To compare the recall we use the standard McNemar test for paired samples: each
ground-truth theorem is categorised as either being found only by the first
tool, only by the second, by both or by neither. Those found by both or neither
are ignored, and the tool-specific numbers are tested for marginal homogeneity
(i.e. whether their differences give rise to similar totals).

We cannot use McNemar's test to compare precision, since each tool may generate
different conjectures. Instead, we use Boschloo's form of Barnard's test to
compare the proportions of wanted and unwanted conjectures generated by each
tool.

\begin{figure}
  \tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm,
                           minimum height=1cm,text centered, draw=black]

  \tikzstyle{io} = [trapezium, trapezium left angle=70,
                    trapezium right angle=110, minimum width=1cm,
                    minimum height=1cm, text centered, draw=black]

  \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm,
                         text centered, draw=black]
  \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm,
                          text centered, draw=black]

  \tikzstyle{arrow} = [thick,->,>=stealth]

  % Avoids too much padding in io shapes
  \tikzset{trapezium stretches=true}

  \centering
  \begin{tikzpicture}[node distance=1cm]

    % Preparation section

    \node (in)  [io]{Theorem Proving Benchmark};
    \node (sep) [process,       below=1cm of in         ]{Separate definitions from theorems};
    \node (def) [process, below  left=1.5cm and -1cm of sep]{Remove duplicate definitions};
    \node (ref) [process, below right=1.5cm and -1cm of sep]{Update references};

    \node (thy)  [startstop, below=1cm of def]{Distinct Definitions};
    \node (thm)  [startstop, below=1cm of ref]{Theorem Corpus};

    \draw [arrow] (in)  -- (sep);
    \draw [arrow] (def) -- (ref);
    \draw [arrow] (def) -- (thy);
    \draw [arrow] (ref) -- (thm);

    % Arrows with labels
    \path[->]
        (sep) edge [arrow, sloped, above] node {definitions} (def)
        (sep) edge [arrow, sloped, above] node {theorems}    (ref);

    % Sampling section

    \node (choose) [process, below=of thm   ]{Choose a theorem};
    \node (deps)   [process, below=of choose]{List theorem dependencies};

    % Create dummy coordinate below deps, then use its y coordinate for pad
    \coordinate [below=of deps] (padDummy);
    \path let \p{dummy} = (padDummy),
              \p{in}    = (in)
              in coordinate (padPos) at (\x{in}, \y{dummy});
    \node (pad)  [process, at=(padPos)]{Pad list to form sample};

    \node (sthm) [startstop, below right=1.5cm and -1cm of pad]{Ground Truth};
    \node (sthy) [startstop, below  left=1.5cm and -1cm of pad]{Sampled Theory};


    % Calculate position of (size) using let, then define as normal
    \path let \p{pad}    = (pad),
              \p{choose} = (choose)
           in coordinate (sizePos) at (\x{pad},\y{choose});
    \node (size) [io, at=(sizePos)] {Sample size};

    % Evaluation section
    \path let \p{pad}  = (pad),
              \p{sthm} = (sthm)
           in coordinate (runPos) at (\x{pad}, \y{sthm});
    \node (run) [process, below=of runPos]{Run MTE tool};
    \node (pr)  [process, below=of run]{Analysis};

    \node (prec) [startstop, below=of pr  ]{Precision};
    \node (time) [startstop,  left=of prec]{Time taken};
    \node (rec)  [startstop, right=of prec]{Recall};

    \draw [arrow] (thy)    |- (pad);
    \draw [arrow] (thm)    -- (choose);
    \draw [arrow] (choose) -- (deps);
    \draw [arrow] (deps)   |- (pad);
    \draw [arrow] (size)   -- (choose);
    \draw [arrow] (size)   -- (pad);
    \draw [arrow] (pad)    -- (sthm);
    \draw [arrow] (pad)    -- (sthy);
    \draw [arrow] (sthy)   |- (run);
    \draw [arrow] (run)    -- (pr);
    \draw [arrow] (sthm)   |- (pr);
    \draw [arrow] (pr)     -- (prec);
    \draw [arrow] (pr)     -- (rec);
    \draw [arrow] (pr)     -- (time);


    % Awkward arrow
    \draw [arrow] (thm) -| ([shift={(7mm,-7mm)}]thm.east) |- (sthm);

    % Braces

    % Preparation
    \draw
      let \p{thm} = (thm.east),
          \p{in}  = (in.north),
          \p{rec} = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{in}) -- (\x{rec}, \y{thm})
         node [black, midway, right, xshift=0.3cm] {Preparation};

    % Sampling
    \draw
      let \p{thm} = (thm.east),
          \p{gt}  = (sthm.east),
          \p{rec} = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{thm}) -- (\x{rec}, \y{gt})
         node [black, midway, right, xshift=0.3cm] {Sampling};

    % Evaluation
    \draw
      let \p{sthm} = (sthm.east),
          \p{rec}  = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{sthm}) -- (\x{rec}, \y{rec})
         node [black, midway, right, xshift=0.3cm] {Evaluation};
  \end{tikzpicture}
  \caption[]{High-level view of our benchmarking methodology, showing
    \begin{tikzpicture}
      \node [rectangle, text centered, draw=black]{processes};
    \end{tikzpicture},
    \begin{tikzpicture}
      \node [rectangle, rounded corners, text centered, draw=black]{data};
    \end{tikzpicture} and
    \begin{tikzpicture}
      \node [trapezium, trapezium left angle=70, trapezium right angle=110,
             text centered, draw=black]{inputs};
  \end{tikzpicture}}
  \label{figure:flow_chart}
\end{figure}

\section{Application}
\label{sec:application}

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy MTE
tools, using version 0.2 of the TIP (Tons of Inductive Problems) theorem proving
benchmark as our ground truth corpus~\cite{claessen2015tip}.

To determine our benchmarking parameters we ran some initial tests on both tools
for a few samples sized between 1 and 100, for an hour each. Most QuickSpec runs
either finished within 200 seconds or not at all, and sizes above 20 mostly
timed out. IsaCoSy mostly finished within 300 seconds on sample sizes up to 4,
but by size 8 was mostly timing out; its few successes above this took thousands
of seconds each, which we deemed infeasibly long.

Based on this we decided to benchmark sample sizes up to 20, since neither tool
seemed to perform well beyond that. The Speedup-Test protocol follows the
statistical ``rule of thumb'' of treating sample sizes $\leq$ 30 as ``small'',
so we pick 31 samples of each size in order to cross this (arbitrary)
threshold. This gave a total of 1240 runs. To keep the benchmarking time down to
a few days we chose a timeout of 300 seconds, since that covered most of the
successful QuickSpec and IsaCoSy results we saw, and longer times gave rapidly
diminishing returns. During analysis, duplicate samples (caused by our sampling
restrictions) were found and discarded, so only 14 samples of size 1 were used
and 30 of size 2.

\subsection{TIP}

We chose TIP for our ground truth since it satisfies the desiderata of
section~\ref{section:prep}: each problem has standalone type and function
definitions, making their separation trivial; known examples from the software
verification and inductive theorem proving literature are included, ensuring
relevance to those fields; the format includes the higher-order functions and
inductive datatypes we are interested in; it is large enough to pose a challenge
to current MTE tools; plus it is accompanied by tooling to convert its custom
format (an extension of SMT-Lib~\cite{BarFT-SMTLIB}) into a variety of
languages, including Haskell and Isabelle.

We use TIP version 0.2 which contains 343 problems, each stating a single
theorem and together defining a total of 618 datatypes and 1498 functions. Most
of these are duplicates, since each problem (re\nobreakdash-)defines all of the
datatypes and functions it involves.

TIP datatypes can have several ``constructors'' (introduction forms) and
``destructors'' (elimination forms; field accessors). For example the type of
lists from figure~\ref{figure:list_theory} can be defined in the TIP format as
follows:

\begin{samepage}
\begin{verbatim}
(declare-datatypes
  (a)                       ;; Type parameter (element type)
  ((List                    ;; Type name
     (Nil)                  ;; Constructor (nullary)
     (Cons                  ;; Constructor (binary)
       (head a)             ;; Field name and type
       (tail (List a))))))  ;; Field name and type
\end{verbatim}
\end{samepage}

Our target languages (Haskell and Isabelle) differ in the way they handle
constructors and destructors, which complicates comparisons. To avoid this, we
generate a new function for each constructor (via $\eta$-expansion) and
destructor (via pattern-matching) of the following form:

\begin{samepage}
\begin{verbatim}
(define-fun
  (par (a)                   ;; Type parameter
    (constructor-Cons        ;; Function name
      ((x a) (xs (List a)))  ;; Argument names and types
      (List a)               ;; Return type
      (as                    ;; Type annotation
        (Cons x xs)          ;; Return value
        (List a)))))         ;; Return type

(define-fun
  (par (a)                        ;; Type parameter
    (destructor-head              ;; Function name
      ((xs (List a)))             ;; Argument name and type
      a                           ;; Return type
      (match xs                   ;; Pattern-match
        (case (Cons h t) h)))))   ;; Return relevant field
\end{verbatim}
\end{samepage}

\begin{sloppypar}
  We rewrite the TIP theorems (our ground truth) to reference these expanded
  forms instead of the raw constructors and destructors, and use these functions
  in our samples in lieu of the raw expressions. Note that these destructor
  wrappers are \emph{partial} functions (e.g. \texttt{destructor-head} and
  \texttt{destructor-tail} are undefined for the input \texttt{Nil}), which
  complicates their translation to proof assistants like Isabelle.
\end{sloppypar}

Another complication is TIP's ``native'' support for booleans and integers,
which allows numerals and symbols like \texttt{+} to appear without any
accompanying definition. To ensure consistency in the translations, we replace
all occurrences of such expressions with standard definitions written with the
``user-level'' \texttt{declare-datatypes} and \texttt{define-fun}
mechanisms.~\footnote{\texttt{Boolean} has \texttt{true} and \texttt{false}
  constructors; \texttt{Natural} has \texttt{zero} and \texttt{successor};
  \texttt{Integer} has unary \texttt{positive} and \texttt{negative}
  constructors taking \texttt{Natural}s, and a nullary \texttt{zero} for
  symmetry.}

When we add all of these generated types and functions to those in TIP, we get a
total of 3598 definitions. Removing $\alpha$-equivalent duplicates leaves 269,
and we choose to only sample from those 182 functions which occur in at least
one theorem statement (this removes ambiguity about which \emph{definitions}
count as interesting and which are just ``implementation details'' for other
definitions).

TIP comes with software to translate its definitions into Haskell and Isabelle
code, including comparison functions and random data generators suitable for
QuickCheck. We translate all 269 unique definitions into a single module/theory
which is imported on each run of the tools, although only those functions which
appear in the current sample are included in the signature and explored.

\subsection{QuickSpec}

We benchmarked QuickSpec version 1 (0.9.6), a tool written in Haskell for
conjecturing equations involving Haskell functions, described in more detail in
section~\ref{section:mte}. In order to thoroughly benchmark QuickSpec, we need
to automate some of the decisions which are normally left up to the user:

\begin{sloppypar}
  \begin{itemize}
  \item We must decide what variables to include. We choose to add three
    variables for each type that appears as a function argument, except for
    types which have no QuickCheck data generators.
  \item We must \emph{monomorphise} all types. For example, functions like
    \texttt{constructor-Cons} are \emph{polymorphic}: they build lists of any
    element type, but we need to pick a specific type in order to know which
    random value generator to use. We resolve this (arbitrarily) by picking
    \texttt{Integer}.~\footnote{We pick \texttt{Integer} for variables of kind
      \texttt{*} (types); for kind \texttt{* -> *} (type constructors) we pick
      \texttt{[]} (Haskell's list type constructor). If these violate some
      type class constraint, we pick a suitable type non-deterministically from
      those in scope during compilation; if no suitable type is found, we give
      up and don't include that function.}
  \item Haskell functions are ``black boxes'', which QuickSpec can't compare
    during its exploration process. They are also curried, always taking one
    argument but potentially returning another function. QuickSpec lets us
    assign an arity to each function in the signature, from 0 to 5, so we pick
    the highest that is type-correct, since this avoids a proliferation of
    incomparable, partially-applied functions.
  \end{itemize}
\end{sloppypar}

\begin{figure}
  \centering
  \input{quickspectime.pgf}
  \caption{Running times of QuickSpec on theories sampled from TIP. Each point
    is an individual run, with 31 per size except for 1 (14) and 2 (30). Colour
    indicates the total number of conjectures produced by each run, with failing
    processes shown in red. Points are spread out horizontally to prevent
    overlaps. Box plots show median and quartiles, with whiskers showing points
    at 1.5$\times$~inter-quartile range.}
  \label{figure:quickspec_runtimes}
\end{figure}

\begin{figure}
  \centering
  \input{quickspecprec.pgf}
  \caption{Precision and recall of successful QuickSpec runs (spread
    horizontally to avoid overlaps). Point colour shows how many conjectures
    were in the ground truth set for each run. Lines show the combined
    proportion for each sample size (ratio of averages) and the shaded region is
    the sample standard deviation.}
  \label{figure:quickspec_precRec}
\end{figure}

The time taken to explore samples of different sizes is shown in
figure~\ref{figure:quickspec_runtimes}. Runs which timed out are shown in red,
and these occurred for all sample sizes but are more common for larger samples
(over half the runs for sample size 20 timed out). Runs which succeeded mostly
finished within 30 seconds, generating few or no conjectures; those taking
longer generated more conjectures, with the most being 116 conjectures from a
sample of size 16.

Precision and recall are shown in figure~\ref{figure:quickspec_precRec}, which
we model as simple Bernoulli processes: the generated conjectures (wanted or
not) are like coin tosses whose bias is the precision; likewise, the ground
truth theorems (found or not) are like coin tosses biased by the recall. Note
that this simplification does not explain how many conjectures the tool decides
to output for each run.

QuickSpec's precision gets worse as the sample size grows. Since QuickSpec
generates monotonically more conjectures as definitions are added to a signature
(assuming sufficient testing), it can't be finding fewer correct conjectures at
larger sizes. This is supported by the relative flatness of the recall results.
Rather, the number of conjectures generated is increasing at a higher rate than
the size of the ground truth. These extra conjectures may involve those
``padding'' definitions in a sample which don't contribute to its ground truth,
or may be ``uninteresting'' relationships between the dependencies of different
ground truth theorems.

This indicates two potential improvements to the QuickSpec algorithm (as far as
this benchmark is concerned). The deluge of generated conjectures could be
filtered down to a more desirable sub-set by another post-processing step.
Alternatively, rather than exploring all of the given definitions together,
multiple smaller signatures could be selected from the input by predicting which
combinations are likely to lead to interesting conjectures; this would avoid
both the ``padding'' and the cross-dependency relationships. Both of these
methods could improve the precision, although care would be needed to avoid a
large reduction in recall. The latter option could also improve the running
time, since (based on figure~\ref{figure:quickspec_runtimes}) multiple smaller
signatures may be faster to explore than a single large one. Such improvements
would also need to be ``generic'', to avoid over-fitting to this particular
benchmark.

QuickSpec's recall is limited by two factors: the algorithm is unable to
synthesise some theorem statements, such as conditional equations, inequalities,
terms larger than the search depth and those containing anonymous functions.
The congruence closure algorithm used as a post-processor may also be removing
``interesting'' results, for example if we found an ``uninteresting'' result
which is more general.

\subsection{IsaCoSy}

IsaCoSy is a tool for the Isabelle proof assistant, mostly written in Standard
ML. We took IsaCoSy from version 2015.0.3 of the IsaPlanner project, and ran it
with the 2015 version of Isabelle. The following issues had to be overcome to
make our benchmark applicable to IsaCoSy:

\begin{itemize}
\item TIP includes a benchmark called \texttt{polyrec} whose types cannot be
  encoded in Isabelle. We strip out this type and the functions which depend on
  it before translating. It still appears in samples and contributes to the
  ground truth, which penalises IsaCoSy for being unable to explore such
  definitions.
\item When using a type in an IsaCoSy signature, that type's constructors will
  automatically be included in the exploration. Since those constructors will
  not appear in the ground truth (we use $\eta$-expanded wrappers instead, and
  even those may not be present in the current sample) this will unfairly reduce
  the calculated precision. To avoid this, we add a post-processing step which
  replaces all occurrences of a constructor with the corresponding wrapper, then
  discards any conjectures which involve functions other than those in the
  current sample. This presumably results in more work for IsaCoSy, exploring
  constructors unnecessarily, but it at least does not bring down the quality
  measures.
\item Since Isabelle is designed for theorem proving rather than programming, it
  requires every definition to be accompanied by proofs of exhaustiveness and
  termination. These are difficult to generate automatically, and don't exist in
  the case of partial functions like destructor wrappers. Hence we use the
  ``quick and dirty'' option in Isabelle, which lets us skip these proofs with
  the \texttt{sorry} keyword.
\item Partial functions cause problems during exploration, since they can throw
  an ``undefined'' exception which causes IsaCoSy to abort. We avoid this by
  pre-populating IsaCoSy's constraint set with these undefined expressions
  (for example \texttt{(destructor-head constructor-Nil)}), hence preventing
  IsaCoSy from ever generating them.
\end{itemize}

\begin{figure}
  \centering
  \input{isacosytime.pgf}
  \caption{Running times of IsaCoSy on theories sampled from TIP. Each point
    is an individual run, with 31 per size except for 1 (14) and 2 (30). Colour
    indicates the total number of conjectures produced by each run, with failing
    processes (timeouts and out of memory) shown in red. Points are spread out
    horizontally to prevent overlaps. Box plots show median and quartiles, with
    whiskers showing points at 1.5$\times$~inter-quartile range.}
  \label{figure:isacosy_runtimes}
\end{figure}

\begin{figure}
  \centering
  \input{isacosyprec.pgf}
  \caption{Precision and recall of all successful IsaCoSy runs (spread
    horizontally to avoid overlaps). Lines show the combined proportion for each
    sample size (ratio of averages) and the shaded region is the sample standard
    deviation.}
  \label{figure:isacosy_precRec}
\end{figure}

The most striking result is how rapidly IsaCoSy's running time, shown in
figure~\ref{figure:isacosy_runtimes}, increases with sample size: all runs above
size 6 failed, with most timing out and a few aborting early due to running out
of memory. Like in our preliminary testing, this increase appears exponential,
so even large increases to the timeout would not produce many more successful
observations. A significant amount of IsaCoSy's time (around 50 seconds) was
spent loading the generated theory (containing all distinct datatypes and
functions from TIP); this overhead is unfortunate, but since it is constant
across all sample sizes it does not affect our conclusions.

With so few successful datapoints it is difficult to make strong claims about
the precision/recall quality of IsaCoSy's output, shown in
figure~\ref{figure:isacosy_precRec}. It is clear from the quantity of non-zero
proportions that IsaCoSy is capable of discovering interesting conjectures, and
the graphs appear to follow the same shapes as those of QuickSpec (decreasing
precision, flat recall), although the error margins are too wide to be
definitive.

\subsection{Comparison}

We compare the running times of QuickSpec and IsaCoSy using our paired variant
of the Speedup-Test protocol, where each of our sample sizes is a separate
``benchmark''. We used version 2 of the Speedup-Test \texttt{R} implementation,
which we patched to use the \emph{paired} form of the (one-sided) Wilcoxon
signed-rank test~\cite{wilcoxon1945individual}. We use Pratt's
method~\cite{pratt1959remarks} to handle ties, which occur when both tools time
out.

For each sample size (``benchmark'') the Speedup-Test protocol compares the
distributions of each tool's times using a Kolmogorov-Smirnov test. If these are
significantly different (with $\alpha < 0.05$), then the signed-rank test is
only performed if more than 30 samples are available. This was the case for all
sample sizes except for 1 and 2 (due to the removed duplicates), and hence those
two speedups were not deemed statistically significant. For all other sample
sizes QuickSpec was found to be significantly faster with $\alpha = 0.05$,
putting the proportion of sizes with faster median times between 66.9\% and
98.2\% with 95\% confidence. The magnitude of each speedup is given in
table~\ref{table:speedups}. We would predict the speedup to grow for larger
sample sizes, but the increasing proportion of timeouts causes our estimates to
become more conservative: by size 20 most runs are timing out, and hence are
tied.

\begin{table}
  \centering
  \begin{tabular}{ |r|l| }
    \hline
    \bfseries Sample Size & \bfseries Speedup
    \csvreader[]{speedups.csv}{}
    {\\\hline\csvcoli&\csvcolii} \\
    \hline
  \end{tabular}
  \caption{Speed up from using QuickSpec compared to IsaCoSy, as reported by
    Speedup-Test for a confidence level of 95\%. Sizes 1 and 2 were not
    considered statistically significant due to having less than 30 samples.
    These are conservative estimates, since the 300 second time limit acts to
    reduce the measured time difference.}
  \label{table:speedups}
\end{table}

We compare the recall proportions by applying McNemar's test to only those
samples where both tools succeeded. We pooled these together from all sample
sizes to produce the following table, and found that QuickSpec is significantly
different from IsaCoSy with a p value of 0.0026.

\begin{table}[H]
  \centering
  \begin{tabular}{ |r|l|l| }
    \hline
    & \bfseries Found by IsaCoSy & \bfseries Not found by IsaCoSy \\
    \hline
    \bfseries Found by QuickSpec     & 28 & 19 \\
    \bfseries Not found by QuickSpec & 4  & 28 \\
    \hline
  \end{tabular}
\end{table}

Precision was also compared by pooling results from samples where both tools
finished successfully. We cannot perform McNemar's test in this case, since each
tool generated different sets of conjectures. Instead, we add up the number of
results which are ``interesting'' (appear in the ground truth) and those which
are ``uninteresting'' (do not appear in the ground truth):

\begin{table}[H]
  \centering
  \begin{tabular}{ |r|l|l| }
    \hline
              & \bfseries Interesting & \bfseries Uninteresting \\
    \hline
    \bfseries IsaCoSy   & 32          & 137      \\
    \bfseries QuickSpec & 47          & 301      \\
    \hline
  \end{tabular}
\end{table}

These totals were tested for independence using Boschloo's
test~\cite{lydersen2009recommended}, assuming a binomial distribution and
conditioning on the number of generated conjectures (as if this were fixed by
design). This produces a p-value of 0.111, which given our (arbitrary) choice of
$\alpha = 0.05$ is not significant enough to reject the null hypothesis that
their precision is equal.

Note that this is the only comparison where IsaCoSy performed better (although
not significantly so) with 19\% precision compared to QuickSpec's 14\%. However,
precision on its own is not the whole story: it can be increased at the expense
of recall by making the tool more conservative. IsaCoSy's significantly lower
recall indicates that it may be overly-conservative compared to QuickSpec, which
would also explain its precision.

More importantly, the poor time and memory usage of IsaCoSy meant that very few
samples finished successfully. If we include failed runs in our tables, treating
them as if they succeeded with no output, the resulting statistics lean heavily
in favour of QuickSpec simply because it generated so much more than IsaCoSy in
the available time.

% To do this we can take a set of ATP tasks, for example proving the equation
% \ref{eq:mapreduce} from the theory in figure \ref{figure:list_theory}, and we
% split apart the definitions (to form a theory) from the theorem statements (to
% form our ground truth).

\section{Discussion}
\label{sec:discussion}

Fundamentally, any mathematical reasoning system must decide on, and formalise,
what counts as ``the good'' in mathematics.  Obvious metrics such as ``true'' or
``provable'' include trivial tautologies, while at the same time failing to
capture the ``almost true'', which can be a valuable trigger for theory change,
as demonstrated by Lakatos in his case studies of mathematical
development~\cite{lakatos}. ``Beautiful'' is another -- albeit vague -- commonly
proposed metric. Neuro-scientists such as Zeki \etal{} have attempted to shed
light on this by testing whether mathematicians' experiences of abstract beauty
correlates with the same brain activity as experiences of sensory
beauty~\cite{10.3389/fnhum.2014.00068}. Metrics such as those in
table~\ref{table:colton}, from computer scientists like Colton, are based
largely on ``intuition'', plausibility arguments about why a metric would be
important, and previous use of such metrics (in the case of
``surprisingingness'', from a single quote from a mathematician).
Opinions from mathematicians themselves include Gowers' suggestion that we can
identify features which are commonly associated with good
proofs~\cite{gowers2000two}, Erdos's famous idea of ``The Book'' (a theoretical
book containing the most elegant proof of every theorem, brought to life
in~\cite{aigner2010proofs}) as well as McCasland's evaluation of the
interestingness of MATHsAiD's output~\cite{roy} (of which he was the main system
developer).

All of these approaches rest upon the assumption that it even makes sense to
speak of ``the good'' in mathematics. However, empirical psychological studies
call into question such assumptions: for example, work by Inglis and colleagues
has shown that there is not even a single standard of \emph{validity} among
contemporary mathematicians~\cite{inglis2013mathematicians}.

Whilst these difficulties are real and important, we cannot ignore the fact that
mathematics is nevertheless being practised around the world; and similarly that
researchers have forged ahead to develop a variety of tools for automated
exploratory mathematics. If we wish to see these tools head in a useful,
fruitful direction then \emph{some} method is needed to compare their
approximate ``quality'' in a concrete, measurable way.

The key to our benchmarking methodology is to side-step much of this
philosophical quagmire using the simplifying assumption that theorem proving
problem sets are a good proxy for desirable input/output behaviour of MTE tools.
As an extension of existing precision/recall analysis, this should hopefully not
prove too controversial, although we acknowledge that there are compelling
reasons to refute it.

We do not claim that our use of corpora as a ground truth exactly captures all
interesting conjectures of their definitions, or that those definitions exactly
represent all theories we may wish to explore. Rather, we consider our approach
to offer a pareto-optimal balance between theoretical rigour and experimental
practicality, at least in the short term. Futhermore, since research is already
on-going in these areas, we hope to at least improve on existing evaluation
practices and offer a common ground for future research.

One notable weakness is that our methodology only makes sense in domains where
existing human knowledge surpasses that discovered by the machine. Any
\emph{truly novel} insights discovered during testing will not, by definition,
appear in any existing corpus, and we would in fact \emph{penalise} tools for
such output. However, we do not believe this to be a realistic problem for the
time being (we would be delighted to be proved wrong!).

Another practical limitation of our benchmarking approach is that it only
applies to tools which act in ``batch mode'', i.e. those which choose to halt
after emitting some output. Whilst all of the systems we have encountered are of
this form, some (such as IsaCoSy, QuickSpec 2 and Speculate) could conceivably
be run without a depth limit, and form part of the ``scientist assistant'' role
envisaged by Lenat (or McCarthy's earlier ``advice
taker''~\cite{McCarthy_Programs59}). Analogous benchmarks could be designed for
such long-running, potentially interactive programs, but that is beyond the
scope of this project.

\section{Conclusion}
\label{sec:conclusion}

We propose a general benchmarking methodology for measuring, summarising and
comparing performance of diverse approaches to the conjecture generation
problem, whilst avoiding some of the philosophical complications and ambiguities
of the field. This methodology can be tailored for specific interests, such as
the choice of problem set and the focus of the resulting analysis.

We have also presented an example application for the domain of higher-order
inductive theories (which is immediately applicable to problems in functional
programming). Using the TIP problem set as a corpus, we have evaluated the
performance of the QuickSpec and IsaCoSy tools and demonstrated QuickSpec to be
both significantly faster and to also output more desirable conjectures than
IsaCoSy; although more \emph{undesirable} output may be generated as well. Based
on these results we proposed two possible directions to improve the QuickSpec
algorithm: a post-processing filter to remove more ``uninteresting'' conjectures
and a pre-processing filter to ``focus'' on promising subsets of the input.

Other promising directions for future work include the evaluation of other MTE
tools, the use of other corpora more suited to different problem domains, and
the extension of existing corpora with new definitions and theorems (which would
also benefit the ATP benchmarks they originally came from).

We believe that a standard approach to benchmarking and comparison such as ours
will ease the burden on researchers wanting to evaluate different potential
approaches to this task, and provide a common goal to pursue in the short term.

``Solving'' this benchmark suite would not solve the problem of theory
exploration in general, so more ambitious goals must be set in the future. For
now, we believe that our approach provides a compelling challenge and will
encourage healthy competition to improve the field.

%\begin{acknowledgements}
%  EPSRC
%\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

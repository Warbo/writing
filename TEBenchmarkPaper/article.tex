% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{multirow}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}

% For example theory of lists
\newcommand{\function}{\rightarrow}
\newcommand{\Zero}{\text{Z}}
\newcommand{\Succ}{\text{S}}
\newcommand{\List}{\text{List}}
\newcommand{\ListA}{\text{List} \  a}
\newcommand{\Nil}{\text{Nil}}
\newcommand{\Cons}{\text{Cons}}
\newcommand{\Head}{\text{head}}
\newcommand{\Tail}{\text{tail}}
\newcommand{\Append}{\text{append}}
\newcommand{\Reverse}{\text{reverse}}
\newcommand{\Length}{\text{length}}
\newcommand{\Map}{\text{map}}
\newcommand{\Foldl}{\text{foldl}}
\newcommand{\Foldr}{\text{foldr}}

% For interestingness table
\newcommand{\iE}{\textbf{E}}
\newcommand{\iN}{\textbf{N}}
\newcommand{\iS}{\textbf{S}}
\newcommand{\iA}{\textbf{A}}
\newcommand{\iC}{\textbf{C}}
\newcommand{\iU}{\textbf{U}}
\newcommand{\tIFF}{if-and-only-if}
\newcommand{\tNE}{non-exists}
\newcommand{\tIMP}{implies}
\newcommand{\tRow}[1]{#1 \\ \hline}

% Insert the name of "your journal" with
\journalname{Journal of Automated Reasoning}

\begin{document}

\title{Quantitative Benchmarking for Automatically Generated Conjectures%\thanks
% {Grants or other notes about the article that should go on the front page
% should be placed here. General acknowledgments should be placed at the end of
% the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton \and
        Alison Pease
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
           University of Dundee \\
           \email{c.m.warburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           A. Pease \at
           University of Dundee \\
           \email{a.pease@dundee.ac.uk}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

% Look carefully at each system's (paper's) eval section

% Make it clear that methodology is the point (it's not just a detail, like in a
% chemistry paper for example)

\begin{abstract}
  We propose a benchmark suite for evaluating the efficiency and effectiveness
  of automated tools for \emph{theory exploration} or
  \emph{conjecture formation} in higher-order, inductive theories; a domain
  especially suited for analysing software. By providing standard tools and
  metrics, we hope to encourage innovation and comparison between the disparate
  approaches currently being pursued, and spur improvements similar to those
  seen in the competitive field of automated theorem proving.
%\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% Lay out the field of TE: motivation, systems, evaluation, problems...

\emph{Automated theory exploration} (ATE), also known as
\emph{conjecture generation/formation}, is the open-ended problem of producing
conjectures about a given logical theory which are somehow ``interesting''.

This has applications wherever we find use for formal statements: in proof
assistants and their libraries, in mathematics education and research, and
(of special concern for the authors) in the specification, verification,
optimisation and testing of software.

Existing attempts at tackling this problem are difficult to compare, due
partially to the variety of approaches taken, but also because of the inherent
ambiguity of the task and the different goals emphasised by their designers and
their choice of evaluation method.

We attempt to solve this discrepancy, at least for the foreseeable future, by
defining a standard, unambiguous benchmarking approach with which to compare
ATE systems. Our contributions include:

\begin{itemize}
\item A general methodology for benchmarking ATE systems.
\item Resolving the issue of ``interestingness'' through the use of
  theorem-proving benchmarks as a ground-truth.
\item A specific instantiation of this methodology, using the Tons of Inductive
  Problems benchmark as a corpus.
\item Automated tooling to perform this benchmarking.
\item Application of our methodology to the QuickSpec and IsaCoSy ATE systems,
  and a discussion of the results.
\end{itemize}

Section \ref{sec:background} describes the ATE problem in more detail, along
with existing approaches and their evaluation methodologies. We explain our
proposal for a more general benchmark in section \ref{sec:proposal} and
section \ref{sec:application} shows the results when applied to existing ATE
tools. Analysis of these results is given in section \ref{sec:discussion} and
concluding remarks in \ref{sec:conclusion}.

%- Existing approaches are difficult to compare
% - They want different things
% - They're evaluated differently
% - They're evaluated ``locally'' (as white boxen)

% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

\section{Background}
\label{sec:background}

\subsection{Motivation}
\label{sec:motivation}

% more detail about evaluation, interestingness, etc. Reference Simon/Alan
% interestingness, e.g. examples from textbooks, etc.

\begin{figure}
  \begin{equation*}
    \begin{split}
      \forall a. \Nil            &: \ListA                                  \\
      \forall a. \Cons           &: a \rightarrow \ListA \rightarrow \ListA \\
      \Head(\Cons(x, xs))        &= x                                       \\
      \Tail(\Cons(x, xs))        &= xs                                      \\
      \Append(\Nil,         ys)  &= ys                                      \\
      \Append(\Cons(x, xs), ys)  &= \Cons(x, \Append(xs, ys))               \\
      \Reverse(\Nil)             &= \Nil                                    \\
      \Reverse(\Cons(x, xs))     &= \Append(\Reverse(xs), \Cons(x, \Nil))   \\
      \Length(\Nil)              &= \Zero                                   \\
      \Length(\Cons(x, xs))      &= \Succ (\Length(xs))                     \\
      \Map(f, \Nil)              &= \Nil                                    \\
      \Map(f, \Cons(x, xs))      &= \Cons(f(x), \Map(f, xs))                \\
      \Foldl(f, x, \Nil)         &= x                                       \\
      \Foldl(f, x, \Cons(y, ys)) &= \Foldl(f, f(x, y), xs)                  \\
      \Foldr(f, \Nil,         y) &= y                                       \\
      \Foldr(f, \Cons(x, xs), y) &= f(x, \Foldr(f, xs, y))
    \end{split}
  \end{equation*}
  \caption{A simple theory defining a $\List$ type and some associated
    operations. $\Zero$ and $\Succ$ are from a Peano encoding of the
    natural numbers. Taken from
    \cite{Johansson.Dixon.Bundy:conjecture-generation}}
  \label{figure:list_theory}
\end{figure}

Given a logical theory, like the theory of lists shown in figure
\ref{figure:list_theory}, we may want to find theorems which describe its
behaviour. This could be for mathematical curiosity, or due to the theory's
importance in some domain. In particular, for theories which capture the
semantics of some software library, we may want to verify that certain
(un)desirable properties do (not) hold; we might also want to \emph{optimise}
programs using this library, rewriting expressions into a form which requires
less time, memory, network usage, etc. To avoid altering a program's result,
such rewrites should come with theorems proving their correctness, such as the
following theorem for our theory of lists:

\begin{equation} \label{eq:mapreduce}
  \forall f. \forall xs. \forall ys.
    \Map(f, \Append(xs, ys)) = \Append(\Map(f, xs), \Map(f, ys))
\end{equation}

This justifies a rewrite rule for splitting a single $\Map$ call into
multiple independent calls dealing with different sections of a list. Such rules
are useful optimisations since each call can be evaluated in parallel, leading
to the ``map/reduce'' programming paradigm.

All of these example use cases require the ability to discover theorems about
some particular theory. This is a hard problem in general, but presents
opportunities for automation due to the precise, symbolic nature of the domain.

\subsection{Theory Exploration}
\label{sec:te}

The task of discovering theorems in an arbitrary theory can be described as the
interplay of several processes:

% Is it clear?

\begin{itemize}
\item \emph{Forming} or refining the theory, to add new definitions and
  concepts.
\item \emph{Exploring} the theory, to find patterns suitable for posing as
  conjectures.
\item \emph{Proving} that those conjectures are theorems.
\end{itemize}

The latter is studied extensively in the field of Automated Theorem Proving
(ATP). Less attention has been paid to automating the first two, those of
\emph{automated theory formation} and \emph{automated theory exploration} (ATE).

We focus on ATE, where a major challenge is choosing how to narrow down the set
of generated conjectures to those deemed ``interesting'', since this is an
imprecise term with many different interpretations. For example, all existing
approaches agree that simple tautologies are ``uninteresting'', but differ when
it comes to more complex statements.

A survey of systems for theory formation and exploration, and their notions of
``interestingness'' (for concepts and conjectures), is given by Colton et
al~\cite{colton2000notion}. Six notions of ``interestingness'' are identified in
these systems, their use by each is summarised in table \ref{table:colton},
along with (our interpretation of) their use in three more recent ATE systems.
These notions, as applied to ATE, are:

\textbf{Empirical plausibility}, which checks whether a property holds across
some specific examples. This is especially useful for avoiding  false
conjectures, without resorting to a full proof search.

\textbf{Novelty} is whether a conjecture, or one isomorphic or more general, has
already been seen.

\textbf{Surprisingness} of a conjecture is whether or not it is ``obvious'', for
example if it is an instance of a tautology.

\textbf{Applicability} depends on the number of models in which a conjecture
holds. The system of Bagai et al conjectures the \emph{non-existence} of
objects, and hence favours statements with \emph{zero} applicability. Other
systems treat applicability as a positive aspect: the more applicable the
statement, the more interesting it is.

\textbf{Comprehensibility} is the \emph{complexity} of a statement. Simpler
statements are considered more interesting, and many of the search algorithms
explore simpler/smaller statements before complex/larger ones, to more
efficiently find those which are interesting.

\textbf{Utility} is the relevance or usefulness of a conjecture to the user's
particular task. For example, if we want to find optimising rewrite rules such
as equation \ref{eq:mapreduce}, then utility would include whether or not a
conjecture justifies a rewrite rule, the difference in resource usage of the
expressions involved, and how common those expressions are in real usage.

\begin{table}
  \centering
  \begin{tabular}{ |l|l|c|c|c|c|c|c| }
    \hline
    \multirow{2}{*}{\textbf{Program}}                     &
    \multirow{2}{*}{\textbf{Conjecture Types}}            &
    \multicolumn{6}{c}{\textbf{Interestingness Measures}} \\ \hhline{~~------}
    \tRow{            &                    & \iE & \iN & \iS & \iA & \iC & \iU}
    \tRow{AM          & \tIFF, \tIMP, \tNE &   X &   X &   X &   X &   X &   X}
    \tRow{GT          & \tIFF, \tIMP, \tNE &   X &   X &   X &   X &   X &   X}
    \tRow{Graffiti    & inequalities       &   X &   X &   X &     &   X &   X}
    \tRow{Bagai et al & \tNE               &     &   X &     &   X &   X &    }
    \tRow{HR          & \tIFF, \tIMP, \tNE &   X &   X &   X &   X &   X &   X}
    \tRow{QuickSpec   & equalities         &   X &   X &     &     &   X &    }
    \tRow{IsaCoSy     & equalities         &     &   X &     &     &   X &    }
    \tRow{IsaScheme   & equalities         &     &     &     &     &   X &    }
  \end{tabular}
  \caption{Classification of ATE systems from \cite{colton2000notion}, extended
    to those compared in \cite{claessen2013automating} (QuickSpec is the
    conjecture generation component of HipSpec). The interestingness measures
    are \iE{}mpirical plausibility, \iN{}ovelty, \iS{}urprisingness,
    \iA{}pplicability, \iC{}omprehensibility (low complexity) and \iU{}tility.}
  \label{table:colton}
\end{table}

These criteria, especially empirical plausibility and comprehensibility, are not
only used for analysing results after the fact, but instead may form a core part
of an algorithm's design decisions. For example, the designers of IsaCoSy
consider simple substitutions of existing conjectures (i.e. those with low
novelty) to be uninteresting, and hence their system includes a constraint
solving component which avoids generating such statements entirely.

With such ambiguous and varied goals, approaches and assessment criteria it is
difficult to compare ATE systems in a quantitative way, and hence to form some
measure of ``progress'' for the field.

\subsection{Existing Evaluations}
\label{sec:existing}

There are three aspects to consider when we evaluate an ATE system:

\begin{itemize}
\item The quality of each conjecture, which is the ``interestingness'' discussed
  above.
\item The \emph{set} of conjectures produced, to assess things like novelty, as
  well as consistency of the results.
\item Performance of the system, to find the balance struck between output
  quality and time taken.
\end{itemize}

% TODO: Should "generality" be one? Or is that just an implicit "don't cheat"
% assumption?

If we limit ourselves to exploring higher-order theories with inductive types
(a domain closely matching our interest in analysing software), we do find a
direct comparison of three systems~\cite{claessen2013automating}: QuickSpec (the
conjecture-generating component of HipSpec), IsaCoSy and IsaScheme. This is a
\emph{precision/recall} analysis, providing the same theories to each system and
comparing the set of generated conjectures against a \emph{ground truth}.

% TODO: precision/recall is evaluating the set of conjectures. It assumes that
% interestingness == membership in the ground truth, and ignores efficiency
% completely

A common choice of ground truth is the standard library of a theorem prover,
such as Isabelle/HOL; the library authors have gone to the effort of stating,
proving and including these theorems in every copy of their software, which is a
good indication that they are useful or important.

One problem with this choice is the small size of these libraries. For example,
the benchmark based on Isabelle/HOL's theory of natural numbers given
in~\cite{Johansson.Dixon.Bundy:conjecture-generation} contains only 4
definitions and a ground truth of 12 theorems. Whilst such benchmarks allow
objective comparisons between different approaches, their narrow scope doesn't
provide much indication of performance in different, especially \emph{novel},
domains.

\section{Theory Exploration Benchmark}
\label{sec:proposal}

We propose a larger, more ambitious method for evaluating ATE systems, which we
call the Theory Exploration Benchmark (TEB). We take a similar approach to prior
evaluations, but our ground truth comes from the extensive test suites which are
already used to benchmark automated \emph{theorem provers}; which we repurpose
for use in the theory \emph{exploration} setting.

To do this we can take a set of ATP tasks, for example proving the equation
\ref{eq:mapreduce} from the theory in figure \ref{figure:list_theory}, and we
split apart the definitions (to form a theory) from the theorem statements (to
form our ground truth).

We focus on the Tons of Inductive Problems (TIP) theorem proving
benchmark~\cite{claessen2015tip}, since it has several desirable properties:

\begin{itemize}
\item Each problem provides separate definitions and a theorem statement; hence
  it's not difficult to tease them apart for our purposes.
\item Higher-order functions and inductively defined types are provided, which
  corresponds to our desired domain.
\item All together, TIP provides 219 distinct function definitions and 343
  theorem statements, which is enough to fully exercise current ATE systems.
\item Benchmark problems include examples from the theorem proving literature
  as well as common program verification tasks, ensuring the resulting corpus is
  relevant to researchers and practitioners.
\item TIP's maintainers provide a set of tools, including translators from TIP's
  native format (based on SMT-Lib~\cite{BarFT-SMTLIB}) to Haskell and Isabelle
  (which are used by ATE systems).
\end{itemize}

From TIP, we automatically derive our Theory Exploration Benchmark (TEB), and
provide tools to benchmark ATE systems and compare their results. Our
methodology is as follows:

% Better presentation, flowchart?
\begin{itemize}
\item Collect together all definitions found in TIP into one large theory,
  removing $\alpha$-equivalent duplicates.
\item For each type, provide extra functions which act as constructors and
  destructors. For example, the $\List$ type would have extra constructor
  functions $\text{constructor-Nil}$ and $\text{constructor-Cons}$, and
  destructor functions $\text{destructor-head}$ and $\text{destructor-tail}$.
  This allows us to ignore constructors and destructors, and concentrate solely
  on functions.
\item Collect together all theorem statements found in TIP. Update any
  references to removed definitions, and replace references to constructors with
  the associated constructor function from above.
\item \emph{Sample} a sub-set of functions from the large theory uniformly, such
  that we choose all of the names referenced by at least one of the theorems
  (for example, theorem \ref{eq:mapreduce} references the names
  $\Map$ and $\Append$).
\item Provide this sub-set as an input to the ATE system, timing how long it
  takes to complete.
\item Perform precision/recall analysis on the resulting conjectures, against
  the ground truth of TIP theorems which only reference names in this sample.
\end{itemize}

\section{Application}
\label{sec:application}

% evaluation

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy theory
exploration systems. To assess the scalability of their algorithms, we sampled
theories of various sizes: from theories containing only a single definition, up
to theories containing 20 definitions.

To provide a more robust comparison of these two systems, we also applied a
paired difference test: measuring, for each sampled theory, the difference in
time, precision and recall between the two systems.

\section{Discussion}
\label{sec:discussion}

% philosophical bits

\section{Conclusion}
\label{sec:conclusion}

% future work, etc.

Our benchmark suite provides a unifying goal for the diverse approaches of
theory exploration, whilst avoiding some of the philosophical complications of
the field. Whilst the current implementation is quite limited, we welcome
additions from other researchers, to more closely align the benchmark with their
goals, and the strenghts and weaknesses of their systems.

``Solving'' this benchmark suite would not solve the problem of theory
exploration in general, so more ambitious goals must be set in the future; but
we have found that there is still a long way to go until that problem arises.

\begin{acknowledgements}
  We are grateful to Jianguo Zhang for help with our statistical analysis.

  EPSRC
\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

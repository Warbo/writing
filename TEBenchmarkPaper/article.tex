% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Quantitative Benchmarking for Theory Exploration%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton% \and
        Alison Pease
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
              University of Dundee \\
              \email{cmwarburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           %\and
           %S. Author \at
           %   second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle
IGNORE ISABELLE THEORIES FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
IGNORE CLUSTERING FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

intro (motivation, high-level background, eval is a problem),
background,
our proposal,
application (evaluation),
discussion (e.g. philosophical bits),
future work, conclusions, etc.

COMPARE COLTON ET AL.

Evaluation of conjectures, sets of conjectures, systems
what constitutes a good/desired output (in each author's terms)? This combines
their evaluation with their (system-specific) assumptions (e.g. IsaCoSy doesn't
generate redundant terms: non-redundancy is clearly a goal, but it's not measured
in eval since it's assumed based on system's construction)

Look carefully at each system's (paper's) eval section
Make a table like tabl 1 of Colton et al

Make it clear that methodology is the point (it's not just a detail, like in a
chemistry paper for example)

\begin{abstract}
  % TODO: Write abstract sooner rather than layer
Insert your abstract here. Include keywords, PACS and mathematical
subject classification numbers as needed.
\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% TODO: Preaching to the choir;
% Jumping around a bit, rephrase
% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

% Maybe lay out argument in bullet points; flesh out sentences later

% Lay out the field of TE: motivation, systems, evaluation, problems...
% Compare to

The field of mathematics has been transformed by the introduction of computers:
calculations are now routinely automated, allowing humans to focus their efforts
on higher-level, more creative activities like proving theorems; \emph{automated
theorem proving} (ATP) seeks to automate this process too, and was one of the
first tasks studied in the field of artificial intelligence
\cite{newell1956logic,sutcliffe2001evaluating}. Despite this effort, ATP
methods are not widely used by traditional mathematicians, although they have
seen use in applied areas such as hardware and software verification
\cite{Moore:2003}.

% FIXME: Be more specific about our contributions

Even if ATP became widespread, the automation of theorem proving would still
leave an important aspect of mathematics inaccessible to computers: conjecturing
those theorems (and the definitions they involve) in the first place. Automated
methods for posing good conjectures have obvious applications for those who must
invent a constant stream of novel questions, e.g. for examinations or research.
A larger impact may be found in areas such as computer programming, security and
verification, where the benefits of formal methods are hindered by the
difficulty of their application; tools to aid this process, e.g. by proposing
test cases, can reduce this cost.

Many automated systems and algorithms have been developed to generate
conjectures, but it is difficult to judge or compare their performance in a
quantitative way. There are many aspects to consider when judging the quality of
a conjecture: how likely it is to be true, how difficult it is to prove, its
implications, and so on.

One way to make these subjective, ambiguous properties more concrete is to
choose a well-studied theory, with a set of existing theorems that are deemed to
be useful or important in some way, and treat these as a \emph{ground truth}
against which we can compare conjectures generated from the same theory.

A common choice of ground truth is the standard library of a theorem prover,
such as Isabelle/HOL; the library authors have gone to the effort of stating,
proving and including these theorems in every copy of their software, which is a
good indication that they are useful or important.

One problem with this approach is the small size of these libraries. For
example, a benchmark based on Isabelle/HOL's theory of natural numbers contains
only 4 definitions and 12 theorems. Whilst such benchmarks allow
comparison between different approaches, their narrow scope doesn't provide much
indication of performance in different, especially \emph{novel}, domains.

We propose that the extensive test suites which are already used to benchmark
automated theorem provers can be repurposed to create benchmarks for conjecture
generation systems. We provide a benchmark suite, automatically derived from the
Tons of Inductive Problems (TIP) benchmark suite, as well as software to convert
other benchmarks written in the same TIP format (an extension of the SMT-LIB
format \cite{BarFT-SMTLIB}).

\section{Existing Evaluations}
\label{sec:previous}

Theory exploration tools such as
IsaCoSy~\cite{Johansson.Dixon.Bundy:conjecture-generation}, QuickSpec and
IsaScheme have been evaluated using the standard library of the Isabelle theorem
prover as their ground truth.

ALGORITHM:

Concatenate all definitions from all files together.
For each definition, prefix its name with the filename it came from and update
all references.
Normalise all local variable names.
Remove any identical definitions, leaving the first one in place and updating
any references to the removed definitions to use this one.
Repeat, until no further removals are possible.

\section{Evaluation}
\label{sec:evaluation}

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy theory
exploration systems. To assess the scalability of their algorithms, we sampled
theories of various sizes: from theories containing only a single definition, up
to theories containing 20 definitions. These are shown in figure FIXME.

To provide a more robust comparison of these two systems, we also applied a
paired difference test: measuring, for each sampled theory, the difference in
time, precision and recall between the two systems.

\section{Conclusion}
\label{sec:conclusion}

Our benchmark suite provides a unifying goal for the diverse approaches of
theory exploration, whilst avoiding some of the philosophical complications of
the field. Whilst the current implementation is quite limited, we welcome
additions from other researchers, to more closely align the benchmark with their
goals, and the strenghts and weaknesses of their systems.

``Solving'' this benchmark suite would not solve the problem of theory exploration in general,
so more ambitious goals must be set in the future; but we have found that there is still a
long way to go until that problem arises.

\begin{acknowledgements}
  We are grateful to Jianguo Zhang for help with our statistical analysis.

  EPSRC
\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

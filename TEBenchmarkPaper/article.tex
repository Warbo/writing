% Derived from the template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.

\usepackage{attrib}
\usepackage{csquotes}

\usepackage{graphicx}
\graphicspath{ {support/} }

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Quantitative Benchmarking for Automatically Generated Conjectures%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

% \titlerunning{Short form of title}        % if too long for running head

\author{Chris Warburton \and
        Alison Pease
        % TODO: Invite Katya? Even if she only offers some help, can acknowledge
}

% \authorrunning{Short form of author list} % if too long for running head

\institute{C. Warburton \at
           University of Dundee \\
           \email{c.m.warburton@dundee.ac.uk}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           A. Pease \at
           University of Dundee \\
           \email{a.pease@dundee.ac.uk}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle
%IGNORE ISABELLE THEORIES FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%IGNORE CLUSTERING FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

%COMPARE COLTON ET AL.

% Evaluation of conjectures, sets of conjectures, systems
% what constitutes a good/desired output (in each author's terms)? This combines
% their evaluation with their (system-specific) assumptions (e.g. IsaCoSy doesn't
% generate redundant terms: non-redundancy is clearly a goal, but it's not measured
% in eval since it's assumed based on system's construction)

% Look carefully at each system's (paper's) eval section
% Make a table like tabl 1 of Colton et al

% Make it clear that methodology is the point (it's not just a detail, like in a
% chemistry paper for example)

\begin{abstract}
  We propose a benchmark suite for evaluating the efficiency and effectiveness
  of automated tools for \emph{theory exploration} or
  \emph{conjecture formation} in higher-order, inductive theories; a domain
  especially suitable for programming language libraries. By providing standard
  tools and metrics, we hope to encourage innovation and comparison between the
  disparate approaches currently being pursued, and spur improvements similar to
  those seen in the competitive field of automated theorem proving.
%\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

% Lay out the field of TE: motivation, systems, evaluation, problems...

Automated \emph{theory exploration} (ATE), also known as
\emph{conjecture generation/formation}, is the open-ended problem of producing
conjectures about a given logical theory which are somehow ``interesting''.

This has applications wherever we find use for formal statements: for example in
proof assistants and their libraries, in mathematics education and research, and
(especially for our purposes) in the specification, verification, optimisation
and testing of software.

Existing attempts at tackling this problem are difficult to compare and study,
due partially to the variety of approaches taken, but also because of the
inherent ambiguity of the task and the different goals emphasised by the authors
and their evaluation methods.

Our contributions include:

\begin{itemize}
\item A general methodology for benchmarking ATE systems.
\item Resolving the issue of ``interestingness'' through the use theorem-proving
  benchmarks as a ground-truth.
\item A specific instantiation of this methodology, using the Tons of Inductive
  Problems benchmark as a corpus.
\item Automated tooling to produce and analyse this benchmark, translating to
  Haskell and Isabelle code for the QuickSpec and IsaCoSy systems, respectively.
\end{itemize}

Section \ref{sec:background} describes the ATE problem in more detail, along
with existing approaches and their evaluation methodologies. We explain our
proposal for a more general benchmark in section \ref{section:proposal} and
section \ref{application} shows the results when applied to existing ATE tools.
Analysis of these results is given in section \ref{section:discussion} and
concluding remarks in \ref{section:conclusion}.

%- Existing approaches are difficult to compare
% - They want different things
% - They're evaluated differently
% - They're evaluated ``locally'' (as white boxen)

% Look at phd symposium 2017, moa's conjecture synthesis paper, etc.
% We can just jump straight in to automated conjecture generation

\section{Background}
\label{section:background}

\begin{figure}
  \label{figure:list_theory}
  \begin{equation*}
    Nil : \forall (a : Type). List a
    Cons : \forall (a : Type). \forall (x : a). \forall (xs : List a). List a

    Z : Nat
    S : \forall (p : Nat). Nat

    head(Cons(x, xs)) = x
    tail(Cons(x, xs)) = xs
    append(xs, Nil) = xs
    append(Cons(x, xs), ys) = Cons x (append xs ys)
    reverse(Nil) = Nil
    reverse(Cons(x, xs)) = append(reverse(xs), Cons(x, Nil))
    length(Nil) = Z
    length(Cons(x, xs)) = S (length(xs))
    map(f, Nil) = Nil
    map(f, Cons(x, xs)) = Cons(f(x), map(f, xs))
    foldl(f, x, Nil) = x
    foldl(f, x, Cons(y, ys)) = foldl(f, f(x, y), xs)
    foldr(f, Nil, y) = y
    foldr(f, Cons(x, xs), y) = f(x, foldr(f, xs, y))
  \end{equation*}
  \caption{A simple theory defining $Nat$ and $List$ types and some associated
    list operations, taken from \cite{Johansson.Dixon.Bundy:conjecture-generation}}
\end{figure}

If we have a logical theory, for example the theory of lists shown in figure
\ref{figure:list_theory} Many automated systems and algorithms have been developed to generate
conjectures, but it is difficult to judge or compare their performance in a
quantitative way. We consider three distinct aspects to evaluate:

\begin{itemize}
\item Each conjecture, to judge how ``interesting'' it is.
\item The \emph{set} of conjectures produced, to assess things like consistency
  and redundancy.
\item Performance of the system, to find the balance struck between output
  quality and time taken.
\end{itemize}

There are many aspects to consider when judging the quality of
a conjecture: how likely it is to be true, how difficult it is to prove, its
implications, and so on.

One way to make these subjective, ambiguous properties more concrete is to
choose a well-studied theory, with a set of existing theorems that are deemed to
be useful or important in some way, and treat these as a \emph{ground truth}
against which we can compare conjectures generated from the same theory.

A common choice of ground truth is the standard library of a theorem prover,
such as Isabelle/HOL; the library authors have gone to the effort of stating,
proving and including these theorems in every copy of their software, which is a
good indication that they are useful or important.

One problem with this approach is the small size of these libraries. For
example, the benchmark based on Isabelle/HOL's theory of natural numbers given
in~\cite{Johansson.Dixon.Bundy:conjecture-generation} contains
only 4 definitions and 12 theorems. Whilst such benchmarks allow
comparison between different approaches, their narrow scope doesn't provide much
indication of performance in different, especially \emph{novel}, domains.

\section{Our Proposal}
\label{section:proposal}

We propose that the extensive test suites which are already used to benchmark
automated theorem provers can be repurposed to create benchmarks for conjecture
generation systems. We provide a benchmark suite, automatically derived from the
Tons of Inductive Problems (TIP) benchmark suite, as well as software to convert
other benchmarks written in the same TIP format (an extension of the SMT-LIB
format \cite{BarFT-SMTLIB}).

%ALGORITHM:

%Concatenate all definitions from all files together.
%For each definition, prefix its name with the filename it came from and update
%all references.
%Normalise all local variable names.
%Remove any identical definitions, leaving the first one in place and updating
%any references to the removed definitions to use this one.
%Repeat, until no further removals are possible.

\section{Application}
\label{section:application}

% evaluation

We have applied our benchmarking methodology to the QuickSpec and IsaCoSy theory
exploration systems. To assess the scalability of their algorithms, we sampled
theories of various sizes: from theories containing only a single definition, up
to theories containing 20 definitions. These are shown in figure FIXME.

To provide a more robust comparison of these two systems, we also applied a
paired difference test: measuring, for each sampled theory, the difference in
time, precision and recall between the two systems.

\section{Discussion}
\label{section:application}

% (e.g. philosophical bits),

\section{Conclusion}
\label{section:conclusion}

% future work, etc.

Our benchmark suite provides a unifying goal for the diverse approaches of
theory exploration, whilst avoiding some of the philosophical complications of
the field. Whilst the current implementation is quite limited, we welcome
additions from other researchers, to more closely align the benchmark with their
goals, and the strenghts and weaknesses of their systems.

``Solving'' this benchmark suite would not solve the problem of theory exploration in general,
so more ambitious goals must be set in the future; but we have found that there is still a
long way to go until that problem arises.

\begin{acknowledgements}
  We are grateful to Jianguo Zhang for help with our statistical analysis.

  EPSRC
\end{acknowledgements}

\bibliographystyle{plain}
\bibliography{./Bibtex}

\end{document}

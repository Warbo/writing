Referee's review of a research article submitted to J. of Automated Reasoning:
"Quantitative Benchmarking for Automatically Generated Conjectures"
by Warburton, Pease and Zhang

As the title appropriately conveys, this paper describes and illustrates a
way of evaluating by means of benchmark tests the effectiveness of a
certain class of automated tools.  The tools of interest take as input the
definitions of datatypes and some functions over them, and give as output
conjectured algebraic properties relating these functions.

A central problem in any such evaluation is how to assess the quality of a
tool's outputs.  The authors propose to do so by measuring recall and
precision against the expectation of an ideal Ground Truth in the form of
an already known and accepted collection of properties.  They explain how
to address various practical issues that arise: in particular they address
the problem of how to set sufficiently representative ground-truth targets
without setting goals of unrealistic scale; their solution, in short, is to
use many small samples.

The authors discuss in some detail their choice of measurements and
statistical tests both for the evaluation of a single tool and for
comparison of tools.  They illustrate the application of their method in an
evaluation of two tools, IsaCoSy and QuickSpec.

The overall aim is a good one.  The paper is clearly set out and quite
readable.  It discusses many different aspects of the problem, explains the
rationale for the solutions adopted, and presents the illustrative results
of evaluation.

However, I think the paper as it stands also has some significant
weaknesses, and it therefore needs revision before it can be published.

1. The whole approach rests on the availability and choice of an
appropriate collection of known properties to be regarded as the Ground
Truth.  A key assumption is that an ideal tool should generate all and only
these properties.  If a tool generates other properties, regardless of
whether they are true, they are by assumption "uninteresting" and
"undesirable", so the more such properties a tool generates the worse its
"precision" score becomes.  Conversely, if a tool's output omits properties
found in the Ground Truth, each such omission counts as a failure, even if
similar but different properties are generated, and the more such omissions
there are, the worse the tool's "recall" score.

2. The authors' chosen source of a Ground Truth is the TIP (Tons of
Inductive Problems) collection, compiled by other researchers as a set of
benchmark test problems for inductive theorem provers.  It is far from
clear that this makes TIP suitable for use in this very different
evaluation exercise, and that the above-noted exclusive and inclusive
assumptions are warranted.  The authors themselves say on p21 "the
simplifying assumption that theorem proving problem sets are a good proxy
for desirable [output] should hopefully not prove too controversial,
although we acknowledge that there are compelling reasons to refute it".
This acknowledgement is welcome, but it does not resolve the problem. If
the authors reckon TIP is indeed fit for the purpose to which they have put
it, they need to explain why in some detail.  If not, the paper has a
central weakness.

3. There is a further twist in the assessment of TIP's suitability, as the
authors themselves note on p23.  The developers of the TIP collection of
benchmark tests for theorem provers were also the developers of IsaCoSy and
QuickSpec.  The authors claim "our independent re-purposing of this problem
set, in a way it was not designed for, reduces the risk that the benchmark
is tailor-made for these tools".  Maybe, a little.  But is it not highly
likely that many of the TIP problems were based on selections from IsaCoSy
and QuickSpec outputs?  And doesn't that make it all the more surprising
that the scores the authors obtain for precision and recall are quite low?
(See point 5 below.)

4. Even if a suitable Ground Truth can be established, there is another
critical issue.  As the authors note in Section 3.3, literal syntactic
comparison is insufficient for comparison of properties generated by tools
against those in the Ground Truth.  The solution offered is to normalise
variable names and the lexical ordering of left and right hand sides of
equations.  But what about, for example, associative-commutative
equivalence?  As an illustration, suppose the test problem concerns
arithmetic functions.  The Ground Truth may include the property (writing x
for multiplication)

a x (b + c) = (a x b) + (a x c).

What if a tool generates instead one of the many AC-equivalent properties,
such as

a x (c + b) = (b x a) + (c x a)?

One might argue that the tool lacks some natural rules of preference for
human readers.  But is it right to count its performance as
"uninteresting"?  Should one really count this discrepancy as both a
failure to "recall" exactly the Ground-Truth version and a failure of
"precision" by offering an AC-equivalent instead?

5. My concerns are heightened by some of the results presented, such as the
summary figures in Table 2 and Table 3 on p20.

Table 2 shows that most of the TIP properties in tested samples are not
found by either IsaCoSY or by QuickSpec --- even after results are thinned
to include only samples for which both tools succeed in producing output.
The authors' discussion majors on statistical tests, which of course have
their place, but such tests are of questionable relevance if "found by" is
unreasonably demanding.  What characterises the TIP properties that neither
tool could find?  What are the main classes into which they fall?  Do some
have a syntactic form categorically beyond the tools' reach?  Are some
simply too large?  Can the authors show the reader some typical examples of
ground-truth properties beyond "recall" for both tools?

Table 3 shows that in the outputs of both tools "uninteresting" conjectures
out-number "interesting" ones by around five to one.  (By the way, I would
much prefer the column headings "In TIP corpus" and "Not in TIP corpus"
here.) Similar remarks apply. Statistical tests of significance do not
sufficiently expose what is going on.  What characterises the properties
conjectured by the tools but not in the TIP corpus?  How might they be
classified?  Are some of them actually false?  Would any reasonable human
reader agree they are all "uninteresting", or do some of them perhaps fill
interesting gaps?  It would help to show the reader some typical examples
of properties produced by tools with a claimed "precision" below 20%.  The
authors do acknowledge a potential problem of penalising novelty (p21,
penultimate paragraph) but go on to say "We do not believe this to be a
realistic problem for the time being, as long as evaluation is limited to
well-studied domains and results."  If that is so, the paper can be
improved by incorporating the findings of a closer study of the TIP domain
and of the "uninteresting" results from tools.

6. There is also the issue of how to handle the results of comparative
tests for samples where at least one tool fails to terminate, or fails
because of memory exhaustion.  The authors explain on p21 that they ignore
the results for such samples in order to be "charitable".  Well, it is nice
to be generous, but it does seem strange in a comparative evaluation to
ignore results representing the most sharply defined differences in
performance.  If this sort of evaluation catches on, a tool developer
could, for example, artificially improve their tool's comparative
performance by rashly aggressive memoization or tabulation techniques: if
there is enough memory, they win; if there isn't, the result simply won't
count!



\documentclass[11pt]{article}
%\setlength{\textwidth}{6.5in}
%\addtolength{\voffset}{-5pt}
%\frenchspacing
\setlength\parindent{0pt}
\usepackage{parskip}
\usepackage{times}
\usepackage{amsfonts}
\usepackage[square]{natbib}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amssymb}



% \usepackage[square]{natbib}

%\newcommand{\icarus}{\textsc{ICARuS}\xspace}
%\newcommand{\oants}{\mbox{$\Omega$}\textsc{ants}\xspace}
%\newcommand{\gc}{\textsc{GC}\xspace}
%\newcommand{\gcatf}{\textsc{GC-ATF}\xspace}
%\newcommand{\homer}{\textsc{Homer}\xspace}
%\newcommand{\gcicarus}{\textsc{GC-ICARuS}\xspace}

%\title{Section 2}

\begin{document}
\section{Background}

\subsection{Motivation}
Given a logical theory, such as the theory of lists shown in Figure 1,
we may want to find theorems which describe its behaviour. This could
be for mathematical curiosity, or due to the theory's importance in
some domain. In particular, for theories which capture the semantics
of some software library, we may want to verify that certain
(un)desirable properties do (not) hold. We might also want to optimise
programs using this library, rewriting expressions into a form which
requires less time, memory, network usage, etc. To avoid altering a
program's result, such rewrites should come with theorems proving
their correctness, such as the following theorem for our theory of
lists: 

8f:8xs:8ys:map(f; append(xs; ys)) = append(map(f; xs); map(f; ys)) (1)
[**]

This justifies a rewrite rule for splitting a single map call into
multiple independent calls dealing with different sections of a
list. Such rules are useful optimisations since each call can be
evaluated in parallel, leading to the \map/reduce" programming
paradigm.  All of these example use cases require the ability to
discover theorems about some particular theory. This is a hard problem
in general, but presents opportunities for automation due to the
precise, symbolic nature of the domain.

\subsection{Automating Construction and Exploration of Theories}
The task of discovering theorems in an arbitrary theory can be
described as the interplay of several processes: (i) formulating new
definitions and concepts; (ii) finding patterns suitable for posing as
conjectures; and (iii) proving that those conjectures are theorems.
The latter is studied extensively in the field of Automated Theorem
Proving (ATP). Far less attention has been paid to automating the
first two tasks, which both Automated Theory Formation (ATF) and
Automated Theory Exploration (ATE) address.  We discuss the
relationship between ATF and ATE in historical context in
$\S$\ref{exploration-versus-formation}.

A major challenge for both ATF and ATE is choosing how to narrow down
the set of generated conjectures to those deemed ``interesting'',
since this is an imprecise term with many different
interpretations. For example, all existing approaches agree that
simple tautologies are ``uninteresting'', but differ when it comes to
more complex statements.  Colton {\em et al.}  give a survey of
systems for theory formation and exploration, and their notions of
``interestingness'' for concepts and conjectures, identifying six
notions in total. 


%methodology for getting these measures.

%interestingness paper


 are identified in these systems, their use by each
is summarised, along with (our interpretation of) their use
in three more recent ATE systems. These notions, as applied to ATE,
are:

{\bf Empirical plausibility}, which checks whether a property holds
across some specific examples. This is especially useful for avoiding
false conjectures, without resorting to a full proof search.

{\bf Novelty} is whether a conjecture, or one isomorphic or more
general, has already been seen.

{\bf Surprisingness} of a conjecture is whether or not it is
``obvious'', for example if it is an instance of a tautology.

{\bf Applicability} depends on the number of models in which a
conjecture holds. The system of Bagai et al conjectures the
non-existence of objects, and hence favours statements with zero
applicability. Other systems treat ap- plicability as a positive
aspect: the more applicable the statement, the more interesting it is.

{\bf Comprehensibility} is the complexity of a statement. Simpler
statements are considered more interesting, and many of the search
algorithms explore simpler/smaller statements before complex/larger
ones, to more efficiently find those which are interesting.

{\bf Utility} is the relevance or usefulness of a conjecture to the
user's particular task. For example, if we want to find optimising
rewrite rules such as equation 1, then utility would include whether
or not a conjecture justifies a rewrite rule, the difference in
resource usage of the expressions involved, and how common those
expressions are in real usage.

We summarise the criteria in Table 1 and state which were used in key
ATF/ATE systems, where Colton identified usage in AM, GT, Graffiti and
Bagai et al, and his own ATF system HR, and we identify usage in three
more recent ATE systems QuickSpec, IsaCoSy and IsaScheme. These
criteria, especially empirical plausibility and comprehensibility, are
not only used for analysing results after the fact, but instead may
form a core part of an algorithm's design decisions. For example, the
designers of IsaCoSy consider simple substitutions of existing
conjectures (i.e. those with low novelty) to be uninteresting, and
hence their system includes a constraint solving component which
avoids generating such statements entirely.

With such ambiguous and varied goals, approaches and assessment
criteria it is difficult to compare ATE systems in a quantitative way,
and hence to form some measure of ``progress'' for the field.

\section{Theory Exploration versus Theory
  Formation}\label{exploration-versus-formation}

In this section we put the Mathematical Theory Exploration approach
into context both historically, and with respect to related approaches
such as in Automated Theory Formation systems.

\subsection{A Short History of Automated Mathematics}

\begin{quote}
...``in his subsequent design for an Analytical Engine Mr. Babbage has
shown that material machinery is capable, in theory at least, of
rivalling the labours of the most practised mathematicians in all
branches of their science.'' \cite[p. 498]{jevons}
\end{quote}

The first automated mathematical system, the mechanical calculator
(known as the Pascaline), was an adding machine that could perform
additions and subtractions directly and multiplication and divisions
by repetitions, and was conceived by Pascal in 1642 while reorganising
tax revenues \cite{d'ocagne}. Subsequent early systems include
M\"uller's universal calculating machine in 1784, which he built for
the purpose of calculating and printing numerical tables: he invented
this when he had to check and recalculate some tables relating to the
volumes of trees \cite[p. 65]{lindgren}. Thirty seven years later,
Babbage invented his famous difference engine: an automatic,
mechanical calculator designed to tabulate polynomial functions. This
was inspired by a flawed table of logarithms and the idea that
machines would be quicker and more reliable \cite{bowden}. It is
interesting to note background and motivation: while Pascal and
Babbage were mathematicians, with an interest in engineering, M\"uller
was an engineer with an interest in mathematical knowledge. All three
systems were conceived as an aid to mathematicians, as well as
scientists, accountants and surveyors.

Differences in background and motivation continue to be relevant
today. While the majority of work in automating mathematics has been
in symbolic manipulation and theorem proving, we are concerned here
with other aspects of mathematics, including the construction of
concepts, axioms, conjectures, examples and theorems. This is
varyingly known as ``Automated Theory Formation''
\cite{lenat:77,colton:book}, ``Automated Mathematical Theory
Exploration'', ``Mathematical Theory Exploration''
\cite{buchberger:06} (also sometimes prefaced with ``Computer-Aided''
or ``Algorithm-Supported''), ``Automated Mathematical Discovery''
\cite{epstein:91,colton:interestingness,esarm2008}, ``Concept
Formation in Discovery Systems'' \cite{haase}, and ``Automated Theorem
Discovery'' \cite{roy}. Such a plethora of terminology can be
unhelpful and can mask similarities between the different fields. In
particular, the twin strands of Automated Theory Formation and
Automated Mathematical Theory Exploration seem to be developing
somewhat independently without a clear differentiating
methodology. Below we discuss commonalities and differences between
the two schools of thought.


%we limit our discussion to systems which work in maths: there are
%others which work in scientific discovery which are also relevant.

\subsection{Automated Theory Formation}

% 1. history of terminology
Automated theory formation, the focus of this paper, derives its
terminology from psychology in which the term ``concept formation'' is
used (see, for example, \cite{bruner:67}) to describe the search for
features which differentiate exemplars from non-exemplars of various
categories. Lenat used this term in his 1977 paper: {\em Automated
  Theory Formation in Mathematics} \cite{lenat:77} and we have
extended it to describe a family of techniques used to construct and
evaluate concepts, conjectures, examples and proofs (embodied in the
HR, HRL and GC systems and derivatives, as described above).

% 4. example systems - AM, HR, ...
When Lenat built the AM system \cite{lenat:77}, there were systems
which could define new concepts for investigation, such as those
described in \cite{winston}, and systems which could discover
relationships among known concepts, such as Meta-Dendral
\cite{buchanan:75}. No system could perform both of these tasks: Lenat
saw this as the next step. AM was designed to both construct new
concepts and conjecture relationships between them; fully automating
the cycle of discovery in mathematics. Lenat describes this as
follows:

\begin{quote} 
``What we are describing is a computer program which
defines new concepts, investigates them, notices
regularities in the data about them, and conjectures
relationships between them. This new information is used
by the program to evaluate the newly-defined concepts,
concentrate upon the most interesting ones, and iterate the
entire process.'' \cite[p. 834]{lenat:77}
\end{quote} 
 
AM was a rule-based system which used a frame-like scheme to represent
its knowledge, enlarged its knowledge base via a collection of
heuristic rules, and controlled the firing of these rules via an
agenda mechanism. Lenat chose elementary arithmetic as the development
domain because he could use personal introspection for the heuristics
for constructing and evaluating concepts. Given the age of this
discipline, Lenat thought it unlikely that AM would make significant
discoveries, although he did cite its ``ultimate achievements'' as the
concepts and conjectures it discovered (or could have discovered). He
suggested various criteria by which his system could be evaluated,
many of which focused on an exploration of the techniques. For
instance, he considered generality (running AM in new domains) and how
finely-tuned various aspects of the program are (the agenda, the
interaction of the heuristics, etc). %most of which were qualitative.
Lenat saw his system and future developments in this field as having
implications for mathematics itself (finding results of significance),
for automating mathematics research (developing AI techniques), and
for designing ``scientist assistant'' programs (aids for
mathematicians). This shows a broad spread of motivation. Despite the
seeming success of the AM system, it is one of the most criticised
pieces of AI research. In their case study in methodology, Ritchie and
Hanna analysed Lenat's written work on AM and found that there was a
large discrepancy between his theoretical claims and the implemented
program \cite{partridge}. For instance, Lenat made claims about how AM
invented natural numbers from sets, whereas it used one heuristic
which was specifically written in order to make this connection (and
not used in any other context). Another problem was that the processes
were sometimes under-explained. For an argument of why many of the
claims made by Lenat about AM were false, see chapter 13 of
\cite{colton:book}.

\subsection{Mathematical Theory Exploration}

The phrase Mathematical Theory Exploration (MTE) is a recent term
which seems to have originated with Buchberger and colleagues (see,
for example, \cite{buchberger}). His motivation is to support
mathematicians during their exploration of mathematical theories. This
support is intended to be for the straightforward reasoning, which he
argues, covers most mathematical thought, rather than the ingenious
points, which he leaves for human mathematicians. Buchberger's long
term goal is to provide routine tools for the exploration activity of
working mathematicians, to support the invention and structured
build-up of mathematical knowledge. The Theorema project aims at
prototyping features of a system for such a purpose. These features
include Integration of the Functionality of Current Mathematical
Systems (retention of the full power of current numerics and computer
algebra systems, as well as enabling the user to add their own
algorithms to the system); Attractive Syntax (input and output is
readable and presented attractively, and can be personalised by the
user); and Structured Mathematical Knowledge Bases (tools are provided
for building and using large mathematical knowledge
libraries). Buchberger has evaluated the potential of this strategy by
illustrating the automated synthesis of his own Gr\"obner bases
algorithm \cite{buchberger:04}.

Recent systems developed in this area include MATHsAiD \cite{roy},
IsaScheme \cite{MontanoRivas2011} and IsaCosy \cite{johansson}. The
goal of the MATHsAiD (Mechanically Ascertaining Theorems from
Hypotheses, Axioms and Definitions) project is to build a tool which
takes in a set of axioms, concept definitions and a logic and applies
its inference rules to reason from the axioms to theorems. The
motivation is to produce a tool which will help mathematicians to
explore the consequences of a set of axioms or a particular concept
definition. One of the main challenges of the project has been to
automatically evaluate interestingness: to distinguish important
theorems from results which, although they follow from a set of
axioms, are of little mathematical interest.

%IsaScheme
Monta{\~n}o-Rivas {\em et al.} have implemented a scheme-based
approach to MTE in their IsaScheme system
\cite{MontanoRivas2011}. Schemes are higher-order formulae which can
be used to generate new concepts and conjectures; variables within the
scheme are instantiated automatically and this drives the invention
process.  For instance, in the theory of natural numbers, given the
concepts of successor ($suc$), addition ($+$) and zero ($0$),
IsaScheme can use the following scheme to invent the concept of
multiplication:

\[
\text{def-scheme}(g,h,i,j) \equiv\\ 
  \exists f. \forall x y. \left\{
  \begin{array}{l l}
    f(g,y) = h (y)  \: \text{and}  & \\
    f(i(x),y) = j(y,f(x,y)) & \\
  \end{array} \right.
\]
where the existentially quantified variable $f$ stands for the new
function to be defined in terms of the variables g, h, i and j. Within
the theory of natural numbers, IsaScheme instantiates this scheme with
$\sigma_1 = \{g \mapsto 0, h \mapsto (\lambda x.0), i \mapsto suc, j
\mapsto +\}$. In this example, $f \mapsto *$ (multiplication), since:

\begin{align*} 
0*y  &= 0 \\
suc(x) * y &=y + (x*y).
\end{align*}
The new multiplication function $f$ can itself be used to instantiate
variables in the same scheme, resulting in the invention of the
exponentiation concept (these examples are taken from
\cite{MontanoRivas2011}).

%IsaCosy
The IsaCosy system (Isabelle Conjecture Synthesis) is a program for
inductive theory formation, which synthesises conjectures from the
available constants and free variables. Only terms which do not
contain subterms that can be rewritten by a set of rewrite rules can
be synthesised.  Conjectures are tested by sending them to a
counterexample checker and, if no counterexamples are found, then sent
to IsaPlanner which attempts to prove them.

% Discussion.

\subsection{Commonalities and Differences between MTE and ATF}

We believe that ATF and MTE share many goals and would benefit from a
closer alignment: both fields are highly specialised and by joining
forces (while being explicit about differences) the techniques being
developed would have greater impact. This closer alignment is already
beginning to take place. The main commonalities between MTE and ATF
are that both are interested in the same aspects of mathematical
thinking, aiming to automatically construct and evaluate elements of a
mathematical theory, including concepts, conjectures, theorems, axioms
and examples. Both strands contrast themselves with Automated Theorem
Proving. The main historical differences between these two approaches
seem to be that in MTE:
\begin{itemize}
\item the main proponents define themselves as mathematicians (they
  hold a PhD and have experience in research mathematics);
\item the primary motivation is to support mathematicians;
\item systems tend to be user-interactive;
\item systems tend to be specific to mathematics,
\end{itemize}

while in ATF: 

\begin{itemize}
\item the main proponents define themselves as AI researchers (they
  often have a mathematics background up to Masters level, but their
  PhD and research experience is in AI);
\item the primary motivation is to extend AI techniques; a secondary
  motivation is to produce interesting new mathematics;
\item systems tend to be fully automated;
\item systems can often be applied to non-mathematical domains.
\end{itemize}

The different backgrounds of the people who named the fields
``automated theory formation'' (Lenat) and ``mathematical theory
exploration'' (Buchberger) perhaps reflects different metaphysical
perspectives on invention (in which case new mathematical ideas are
being {\em formed}, created or produced) and discovery (in which case
abstract mathematical objects exist independently of us and are {\em
  explored} or investigated) in mathematics.\footnote{There is a vast
  literature on the Platonic and the constructivist view of
  mathematics: we shall not venture down this path here (interested
  readers are referred to \cite{hersh:97,shapiro}). Instead, for now,
  we make the pragmatic assumption that while there may well be
  different cognitive processes involved in invention than those
  involved in discovery, the fields of ATF and MTE are not currently
  concerned with this level of detail and so the philosophical
  distinction is not relevant for our purposes.} As can be seen above,
some of these historical differences are disappearing, and current
systems such as MATHsAiD \cite{roy}, IsaScheme \cite{MontanoRivas2011}
and IsaCosy \cite{johansson} are bridging the methodological gap
between ATF and MTE. This is particularly true in terms of
fully-automatic/user-interactive and the domain-specific/general
aspects.

\section{Discussion}

methodology behind Simon's paper

agreement in maths? matt inglis

other - zeki/two papers with ursula... 






\bibliographystyle{plain} 
%\bibliography{biblio}
\bibliography{biblio}
\end{document}



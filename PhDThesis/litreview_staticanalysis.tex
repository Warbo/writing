\section{Automated Software Improvement}

Software for analysing and improving code can be roughly divided into two
categories. Some contribute to the running of a program, such as (optimising)
compilers, linkers and interpreters. These must produce correct, predictable,
desirable results in a timely manner. For example, compiler optimisations should
not alter program semantics, should provide monotonic improvements in the
majority of cases (i.e. they should not be \emph{pessimisations}) and conditions
under which they apply should be predictable enough to prevent spurious
performance regressions when making seemingly innocuous
refactorings~\cite{robison2001impact}. Optimisations in ahead-of-time (AOT)
compilers must be fast enough to run during every build, and should have a large
enough impact on the generated code's performance that they
``pay for themselves''~\cite{Franz1994}; for just-in-time (JIT) compilers such
ideals become hard requirements, since any runtime transformation which doesn't
pay for itself is, by definition, a pessimisation.

The other category is that of ``standalone'' tools, which are run separately to
building or executing the target code, and whose results do not contribute
directly to the code's output. These include tools for testing, profiling,
verification, standards compliance, etc. The application of theory exploration
to software libraries fits into this latter category, whose constraints are not
as stringent as those applying to tools like compilers. The only real
restrictions on these tools are that their resource requirements are acceptable
to users, and that output is sufficiently high quality to be worth waiting for
and sifting through.

Some of the simplest code improvement tools are known as \emph{linters}, after
the \textsc{Lint} tool for the C programming language~\cite{Johnson78lint}.
Linters are characterised by parsing the given code and looking for occurrences
of certain syntax patterns, for example definitions which are never used, or
``no-op'' constructs (e.g. \texttt{(x~<~0)} when \texttt{x} is of
unsigned/non-negative type, making this comparison always false).

A popular linter for Haskell is \textsc{HLint}~\cite{mitchell2014hlint}, which
focuses mostly on simplifying code; for example by spotting use-cases for common
library functions like \texttt{null x} instead of \texttt{length x == 0}.
Linting can be seen as the inverse of theory exploration: rather than inferring
novel, general relationships from patterns found in the code, linting begins
with known relationships and looks through the code for specific instances.
Whilst linting is a useful programming aid, and fast enough to apply regularly
(e.g. as a background process in a code editor), it is not an open-ended
process: once the predetermined patterns are exhausted, no more improvements
will be found.

At the other end of the code improvement spectrum we find
\emph{superoptimisers}~\cite{massalin1987superoptimizer}, which can take an
arbitrarily long time to find improvements, but whose deductive power is only
limited by the undecidability of the source language. Superoptimisers, such as
\textsc{Souper}~\cite{sasnauskas2017souper} and \textsc{Aha!} (``A Hacker's
Assistant'', developed to accompany \emph{Hacker's
  Delight}~\cite{warren2013hacker}) treat the given code as a specification, and
perform an open-ended search through the space of all programs to find the
optimal implementation according to some quality measure or cost model (often a
branch- and effect-free subset of machine code is used, and measured by counting
instructions or cycles).  Superoptimisers need some method to check whether a
synthesised program implements the required specification, such as a test suite
and/or a theorem prover.

Proving correctness of machine code programs is helped by the uniform, finite
nature of their domain (e.g. a fixed set of instructions, operating on a fixed
set of fixed-size registers). Higher level languages like Haskell are more
difficult to reason about and measure, since their behaviour and efficiency may
depend on unknown runtime parameters. For example, a higher-order function $f$
accepting another function $g$ as an argument may choose to memoise calls to $g$
in order to avoid evaluating identical calls multiple times; however, storing
previous results requires more memory, and each input must be looked up in the
memo table to see if it's been used before. Whether this is an optimisation or
pessimisation depends on the runtime data at each call site, including the
function argument (whether it's cheaper than performing a comparison) and the
data it will be applied to (whether it actually contains any repeated values).
In addition to this uncertainty, each function's input domain may be infinite
(such as the natural numbers), and programs may use abstract interfaces and
datatypes with a variety of possible implementations (such as lists or key/value
mappings). All these factors makes effective testing more difficult, and proofs
potentially undecidable.

We can reduce the amount of unknown parameters in a concrete program by
propagating statically-known values, for example using
\emph{supercompilation}~\cite{Turchin:1986:CS:5956.5957}; however this can
dramatically increase code size, as each function may be duplicated into
multiple specialised instances. Related techniques, like \emph{symbolic
  execution} (a form of \emph{abstract interpretation}), allow approximate
execution of code, where unknown runtime data is represented as opaque
metavariables. Property- and model-checkers are similar, except they work
empirically by performing multiple runs, instantiating metavariables with
synthetic data (\quickcheck{} does this randomly, \smallcheck{} enumerates
values, etc.).

Efficiency can be calculated using techniques like cost
semantics~\cite{danner2015denotational}, with constant factors determined
through empirical testing. This cannot solve the problem of optimality depending
on the characteristics of runtime data, but it can expose these dependencies as
parameters allowing the user to select between pareto-optimal strategies.

Even if these problems are mitigated, the biggest difficulty with
superoptimisation remains: the length of time it takes to search such a
large space. Whilst pruning methods can help~\cite{phothilimthana2016scaling},
the power of superoptimisation comes from its ``bottom up'' nature, which makes
no assumptions about the structure of the generated program. This generality is
expensive, with current superoptimisers typically scaling to code fragments only
a few dozen machine instructions long. Contemporary research has focused on the
use of stochastic search algorithms, such as Markov Chain Monte-Carlo (MCMC), to
synthesise promising improvements more quickly, whilst still finding optimal
implementations in the limit~\cite{schkufza2013stochastic}; guiding such
algorithms with \emph{learned} policies is also a promising area of
study~\cite{mudigonda2017learning}.

The open-ended search process of such superoptimisers is similar to application
of mathematical theory exploration to software property discovery, especially
when exploration is limited to equality properties (which is the case for
\isacosy{} and \quickspec{}). Quality metrics like efficiency (e.g.
relative instruction count) can be considered a subset of the interestingness
criteria of theory exploration. The \emph{aims} of these analyses are in
opposition, since theory exploration \emph{builds up} structure and abstraction
latent in a set of definitions, whilst superoptimisation \emph{tears down} such
layers to find minimal implementations; yet the practical necessity of
superoptimising only small snippets at a time (e.g. to find ``peephole
optimisations''~\cite{Bansal.Aiken:2006}) results in remarkably similar
discovery processes.

% TODO: Evolutionary patches
% TODO: Hutter search
% TODO: Goedel machines

\section{Automated Software Improvement}

One obvious application of software properties discovered through Theory
Exploration is to improve that software, for example by \emph{refactoring} (e.g.
simplifying definitions) or \emph{optimising} (e.g. replacing a slow calculation
with an equivalent fast one). Existing tools for analysing and improving code
can be roughly divided into two categories. Some contribute to the running of a
program, such as (optimising) compilers, linkers and
interpreters\footnote{Optimisation has traditionally been the domain of
  compilers, but the recent popularity of \emph{link time optimisation} and
  \emph{just-in-time compilation} have brought the latter tools firmly under the
  code improvement umbrella.}. These must produce correct, predictable,
desirable results in a timely manner. For example, compiler optimisations should
not alter program semantics, should provide monotonic improvements in the
majority of cases (i.e. they should not be \emph{pessimisations}) and conditions
under which they apply should be predictable enough to prevent spurious
performance regressions when making seemingly innocuous
refactorings~\cite{robison2001impact}. Optimisations in ahead-of-time (AOT)
compilers/linkers must be fast enough to run during every build, and should have
a large enough impact on the generated code's performance that they
``pay for themselves''~\cite{Franz1994}; for just-in-time (JIT) compilers such
ideals become hard requirements, since any runtime transformation which doesn't
pay for itself is, by definition, a pessimisation.

The other category is that of ``standalone'' tools, which are run separately to
building or executing the target code, and whose results do not contribute
directly to the code's output. These include tools for testing, profiling,
verification, standards compliance, etc. The application of theory exploration
to software libraries fits into this latter category, whose constraints are not
as stringent as those applying to tools like compilers. The only real
restrictions on these tools are that their resource requirements are acceptable
to users, and that output is sufficiently high quality to be worth waiting for
and sifting through.

Some of the simplest code improvement tools are known as \emph{linters}, after
the \textsc{Lint} tool for the C programming language~\cite{Johnson78lint}.
Linters are standalone tools, characterised by parsing the given code and
looking for occurrences of certain syntax patterns; for example definitions
which are never used, ``no-op'' constructs (e.g. \texttt{if~(x~<~0)} when
\texttt{x} is of unsigned/non-negative type) and expressions which are valid but
usually used erroneously (e.g. \texttt{if (x = 1)} which performs assignment
rather than comparison).

A popular linter for Haskell is \textsc{HLint}~\cite{mitchell2014hlint}, which
focuses mostly on simplifying code; for example by spotting use-cases for common
library functions like \texttt{null x} instead of \texttt{length x == 0}.
Linting can be seen as the inverse of theory exploration: rather than inferring
novel, general relationships from patterns found in the code, linting begins
with known relationships and looks through the code for specific instances.
Whilst linting is a useful programming aid, and fast enough to apply regularly
(e.g. as a background process in a code editor), it is not an open-ended
process: once the predetermined patterns are exhausted, no more improvements
will be found.

To find project-specific improvements we need some description or specification
of how our program \emph{should} behave, to judge proposed improvements against.
Few projects have a complete formal specification, due to the prohibitive cost
of creating, validating, verifying and maintaining such artefacts. Partial
specifications, in the form of automated test suites, are more common. These
have been used to automatically patch errors in code (i.e. altering code until
its test suite passes), using methods from evolutionary
computation~\cite{Forrest.Nguyen.Weimer.ea:2009, weimer2009automatically,
  Schulte.Forrest.Weimer:2010}. Such methods are necessarily standalone, since
making such arbitrary changes to a program's behaviour without a full
specification or user supervision has the potential to break the program in
untested ways.

Optimisations treat \emph{the code itself} as a specification: formally
describing the desired behaviour, but not prescribing the exact steps that
should be taken to implement it. This allows different implementations to be
used as long as the result is equivalent\footnote{Optimisation assumes that
  extra efficiency is always desirable. In some areas that is not the case, such
  as security-sensitive code, where speed ups may expose sensitive information
  to timings attacks~\cite{kocher1996timing}.}. Taking this to the extreme gives
rise to \emph{superoptimisation}~\cite{massalin1987superoptimizer}, which
performs an open-ended search through the space of all programs to find those
equivalent to the given code, which are also optimal according to some quality
measure or cost model. Superoptimisers, such as
\textsc{Souper}~\cite{sasnauskas2017souper} and \textsc{Aha!} (``A Hacker's
Assistant'', developed to accompany \emph{Hacker's
  Delight}~\cite{warren2013hacker}), can take an arbitrarily long time to find
improvements, but their optimisation power is limited only by the undecidability
of the source language (optimisations which cannot be proven equivalent are
necessarily ignored, like the uncomputable speedups demonstrated by
Blum~\cite{blum1967machine}).

Superoptimisers are often restricted to a branch- and effect-free subset of
machine code, where cost can be reasonably estimated by counting instructions or
cycles. Higher-level languages require more elaborate estimation methods, such
as cost semantics~\cite{danner2015denotational}: such models will still contain
many parameters, some of which may be determined by empirical measurement,
whilst others may depend on characteristics of the input data. Despite this
drawback, pareto-optimal optimisations can still be identified, and either
presented to the user or tested against a realistic data set (known as
\emph{profile-guided optimisation}~\cite{hubicka2005profile}).

Superoptimisers also need some method to check whether a synthesised program
is equivalent to that given: either partially, like a test suite, or robustly
via a theorem prover. Proving correctness of machine code fragments is helped by
the uniform, finite nature of their domain (a fixed set of instructions,
operating on a fixed set of fixed-size registers). Higher level languages like
Haskell are more difficult to reason about and measure, since their behaviour
and efficiency may depend on unknown runtime parameters. In particular,
optimising higher-order functions is difficult in isolation, since many
performance tradeoffs cannot be decided without more knowledge of the functions
involved. For example, introducing a cache of previous return values (known as a
\emph{memo table}) can avoid re-computing repeated calls to a function; but
whether this actually saves time, or is worth the memory penalty, depends on how
long that function takes to calculate and how often it will be called with same
arguments.

In addition to this uncertainty, high level languages allow the input domain of
a function to be infinite (such as the natural numbers or lists), and programs
may use abstract interfaces and datatypes which either hide characteristics
required for establishing equivalence or cost, or may permit \emph{multiple}
implementations with varying characteristics. All these factors make effective
testing more difficult, and proofs potentially undecidable.

Even if these problems are mitigated, the biggest difficulty with
superoptimisation remains: the length of time it takes to search such a
large space. Whilst pruning methods can help~\cite{phothilimthana2016scaling},
the power of superoptimisation comes from its ``bottom up'' nature, which makes
no assumptions about the structure of the generated program. This generality is
expensive, with current superoptimisers typically scaling to code fragments only
a few dozen machine instructions long. Contemporary research has focused on the
use of stochastic search algorithms, such as Markov Chain Monte-Carlo (MCMC), to
synthesise promising improvements more quickly, whilst still finding optimal
implementations in the limit~\cite{schkufza2013stochastic}; guiding such
algorithms with \emph{learned} policies is also a promising area of
study~\cite{mudigonda2017learning}.

Rather than optimising individual units of code in isolation, we can instead
work ``top down'' and perform \emph{whole program
  optimisation}~\cite{weeks2006whole,siskind1999flow}. In particular, for every
function we may want to optimise, we know \emph{all} call sites of that function
within the program, and hence avoid the need for a general solution.
Statically-known values, including arguments to (higher-order) functions, can be
propagated through the code using techniques like
\emph{supercompilation}~\cite{Turchin:1986:CS:5956.5957}, and constant
expressions can be computed ahead of time; however this can dramatically
increase code size, as each function may be duplicated into multiple specialised
instances. Link-time optimisation permits a limited form of inter-procedural
optimisation (e.g. inlining function definitions which were abstract black-boxes
during compilation), but generally avoids the more invasive optimisations found
in compilers.

Other methods to determine function usage and behaviour include \emph{symbolic
  execution} (a form of \emph{abstract
  interpretation}), which allow information to be statically deduced by tracing
the behaviour of code using an alternative execution model, where unknown
runtime data are represented as opaque metavariables. Property- and
model-checkers are similar, except they work dynamically by performing multiple
runs, instantiating metavariables with exhaustive or synthetic data
(\quickcheck{} does this randomly, \smallcheck{} enumerates values, etc.).

The most common code-improvement technique performed \emph{during execution} is
just-in-time (JIT) compilation~\cite{aycock2003brief}, which allows an
interpreter to compile sections of code into a faster form (usually machine
code). Since this compilation takes time, it is usually reserved for sections
which are executed often enough to exceed some predefined threshold (e.g. 100).
By focusing on such ``hot paths'', the up-front costs of compiling everything
ahead of time are avoided; although compiled forms are kept only in memory,
getting recompiled as needed on each program run. Compiling at runtime also
allows specialisation with dynamically-known information; for example, inlining
specific implementations of a procedure rather than performing dynamic dispatch
(this is the problem higher-order functions discussed above). To avoid altering
semantics, such dynamic specialisations must be accompanied by a conservative
validity check (e.g. that the given arguments should use the procedure
implementation we have compiled-in), falling back to the interpreter upon
failure.

A more speculative use of optimisation during runtime has been proposed by
Hutter~\cite{HUTTER:2002}. As in a JIT-enabled interpreter, Hutter's
algorithm (which we refer to as \hsearch{}) begins by directly executing the
given (unoptimised) program $p$ on the relevant input $x$ (this may be provided
up front, or piecemeal throughout execution, e.g. as a stream of user
interaction events). The variables $p_{fast}$ and $t_{fast}$ are initialised to $p$
and $\infty$ respectively, to indicate that $p$ is the fastest implementation we
have so far, and we don't yet have an upper bound on its running
time. Concurrently with the execution of $p$, two other processes are also
started:

\begin{itemize}
\item Process A searches for proofs (in a pre-specified formal system) that some
  program $p'$ is both semantically equivalent to $p$ and has a running
  time bounded by some other program $t$, i.e. for all inputs $i$, the
  time taken to compute $p'(i)$ is bounded by $t(i)$. A list is built up of
  the $(p', t)$ pairs found so far.
\item Process B computes $t(x)$ for all of the bounds $t$ found so far by A.
  Computations are interleaved via a schedule similar to Levin's universal
  search algorithm~\cite{levin1984randomness,ming}, to prevent getting stuck on
  non-halting programs. Whenever a calculation of some $t(x)$ finishes, it is
  compared to $t_{fast}$: if smaller, $t_{fast}$ is set to $t(x)$, and $p_{fast}$ is
  set to $p'$ (the program paired with this $t$).
\end{itemize}

The execution of $p_{fast}(x)$ (initially $p(x)$) is restarted at
exponentially-spaced intervals: if the value of $p_{fast}$ gets updated,
execution will switch to the new, provably-faster program at the next restart.
Although wasteful, such restarting will only slow down execution by at most a
factor of 2; whilst switching to an updated $p_{fast}$ may bring arbitrarily large
speed-ups (depending on the problem). Since the proof search (process A) is
independent of the input $x$, it only contributes a constant factor to the
running time; although in the general case this factor will be exponential in
the length of the proof.

\hsearch{} is mostly of theoretical interest, being described as ``another $O()$
warning showing, how important [to running time] factors, and even subdominant
additive terms, are.''~\cite[p. 4]{HUTTER:2002}. Despite this caveat, the
general framework is still useful to compare with the code improvement
techniques described above. In particular, the use of open-ended search (process
A) is similar to that of superoptimisation; although the former enumerates
equivalence proofs and extracts programs from them, whilst the latter enumerates
programs and attempts to prove (or approximately determine)
equivalence. Likewise the speed estimation of process B is analogous to those
employed by superoptimisation. JIT compilation can also be achieved within this
framework, for example by including our desired optimisations in the axioms of
the formal system (making proof search trivial), biasing the search algorithm to
consider applications of these optimisations first, and including a simple cost
model for the language. Switching to faster programs can also be made
incremental, e.g. allowing (stateless) functions to be replaced individually
without having to restart execution, just like in a JIT-enabled interpreter.

The framework of the \goedelmachine{}~\cite{Schmidhuber:05icann} goes even
further than \hsearch{}: rather than limiting optimisations to just the given
program, it also searches for provable optimisations to \emph{itself}. In
principle, this allows the search algorithm, execution schedule, etc. to all be
optimised, replaced or even removed entirely, as long as a proof is found that
it would be beneficial to do so. The \goedelmachine{} concept is framed in the
language of reinforcement learning, with the goal of maximising``reward'' from
some ``utility function'', but it is easily to recast in terms of improving
efficiency: for example, choosing a utility function which provides a one-off
reward of $\frac{1}{t}$ if a result provably-equivalent to the original
program's is returned within $t$ steps; and 0 reward otherwise. The ability to
provide a utility function separate from the initial implementation is also
useful for specifying the desired task more declaratively: for example, if it is
acceptable to deviate from the behaviour of the initial implementation, as long
as some other criteria (given by the utility function) are satisfied. A relevant
example might be altering a search algorithm to find more results faster, even
if those aren't the same results that the original algorithm would have found.

The open-ended search through program and proof space performed by algorithms
like superoptimisation, \hsearch{} and the \goedelmachine{} are similar to that
of Theory Exploration (e.g. the discovery of equivalent expressions by
\isacosy{} and \quickspec{}). One major difference is in \emph{end result} of
these techniques: the latter tends to \emph{build up} structure and abstraction
latent in a set of definitions, whilst the former tend to \emph{tear down} such
layers to find direct, minimalistic implementations.

Another seeming contrast is that Theory Exploration has no specific ``target'',
unlike optimisers: it produces any properties found to match its
``interestingness'' criteria. However, the practical necessity of limiting
superoptimisers to only small snippets of code at a time has resulted in
applications like finding ``peephole optimisations''~\cite{Bansal.Aiken:2006},
whose scattershot nature is more akin to Theory Exploration than traditional
compiler optimisation. Likewise, the improvement in efficiency of an
optimisation (however that is measured or justified) contributes to its
interestingness, and hence suitability as an outputs for Theory Exploration
systems (especially in the use case of a programming assistant).

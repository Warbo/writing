\section{Statistics of Formal Systems}

\iffalse
TODO: Alison: I'd like to revisit this chapter once I see the whole
thesis. At the moment it feels in parts more like a discussion of your work,
than of related work. It might help to read other people's related work sections
to get a better idea of what to include here. I think it'll be more obvious too,
once we have the intro, etc. I suspect you'll need to do a fair bit more work on
this one.
\fi

The core problem of assigning ``interestingness'' to logical formulae is the
application of statistical reasoning to the discrete, semantically-rich domain
of formal systems. This problem has been tackled from various directions for a
variety of reasons; here we summarise those contributions which are of
particular importance for theory exploration.

\iffalse
\subsection{Clustering}
\label{sec:clustering}

% TODO: ML4PG
\cite{journals/corr/abs-1212-3618}
% TODO: ACL2(ml) (also examples section)
\cite{heras2013proof}
\fi

\subsection{Probability of Statements}

An important property of a logical formula (in general, and in particular with
regards to interestingness) is its truth value. Although we may sometimes be
able to determine truth values exactly, e.g. via deduction, for many formulae it
may only be possible (or practical) to \emph{approximate} their truth value. One
straightforward approximation is to adopt \emph{probabilities}, where we assign
probability $1$ to formulae which are known to be true (e.g. axioms), $0$ to
formulae known to be false, and intermediate values to those which are not
known~\cite{Hutter.Lloyd.Ng.ea:2013}. For example, the truth value of universal
statements (e.g. ``all ravens are black'') cannot be deduced from non-exhaustive
examples (e.g. real-world observations of raven colours), but various induction
methods may infer an (approximate) truth value~\cite{10.1093/mind/LIV.214.97}.

One difficulty with approximate truth values is maintaining consistency without
requiring \emph{logical omniscience} (knowledge of all the logical consequences
of the chosen axioms). Avoiding logical omniscience has given rise to the field
of \emph{bounded rationality}, where reasoning (deduction, induction, etc.) is
subject to some computational constraints. For example, the ``logical inductor''
of~\cite{garrabrant2016logical} avoids assigning particular probabilities to
formal statements, and instead produces a \emph{sequence} of successive
approximate probabilities, each computable with finite resources (albeit very
inefficiently), and guaranteed to be \emph{eventually consistent}.

The probability of a statement can affect its interestingness in a few ways.
Statements whose probability is very close to $0$ or $1$ may be considered
less interesting, since they are essentially known (e.g. easily derivable or
refutable via known facts). Those closer to $\frac{1}{2}$ may be considered more
interesting; although in a bounded rationality context this may simply indicate
the requirement for some routine (but lengthy) computation: for example stating
that the $n$th bit of $\pi$ is a zero, for some large $n$.

To avoid such trivially-easy and infeasibly-difficult statements (which we may
consider uninteresting) we can instead consider the \emph{variance}
(disagreement) between \emph{multiple} approximate truth values (similar to the
problem solvers of~\cite{fernando2013design1, fernando2013design2}). Those
statements which are assigned similar truth values by a variety of methods may
be considered less interesting, as they are either easy enough to infer by all
of our methods, or too hard for any of them (the stability of a single method's
prediction over time would serve a similar role). One benefit of this
application is that the approximation methods do not have to be particularly
capable; in fact choosing ``superhuman'' algorithms would be \emph{less} useful
in this context, since they would agree on the triviality of many statements we
might consider interesting.

\iffalse
\subsection{Interestingness in Concept Formation}
\label{sec:conceptformation}

% TODO:
\cite{Montano-Rivas.McCasland.Dixon.ea:2012}
\cite{Piantadosi.Tenenbaum.Goodman:2012}
\cite{Wille:2005}
\cite{colton1999automatic}
\cite{colton2000agent}
\cite{colton2012automated}
\cite{lenat1977automated}
\cite{mullerunderstanding}
\cite{Bundy.Cavallo.Dixon.ea:2015}
\cite{johansson2009isacosy}
\cite{spector2008genetic}
\cite{colton2012automated}
\cite{geng2006interestingness}
% TODO: How does https en.wikipedia.org/wiki/Discovery system relate?

% However, this search space grows exponentially in the length of the proofs,
% which is unfortunate since proof length has been proposed as an approximate
% measure of how interesting a theorem is \cite[\S~10.2.1]{colton2012automated}.

% Alan Bundy et al

% Eurisko, AM, etc.?
\fi

\subsection{Learning From Structured Data}

One major difficulty with formal mathematics as a domain in which to apply
statistical machine learning is the use of \emph{structure} to encode
information in objects. In particular, \emph{trees} appear in many places: from
inductive datatypes, to recursive function definitions; from theorem statements,
to proof objects. Such nested structures may extend to arbitrary depth, which
makes them difficult to represent with a fixed number of features, as is
expected by most machine learning algorithms. Here we review a selection of
solutions to this problem, and compare their distinguishing properties.

\subsubsection{Feature Extraction}\label{sec:featureextraction}

\emph{Feature extraction} is a common pre-processing step for machine learning
(ML). Rather than feeding ``raw'' data straight into our ML algorithm, we only
learn a sample of \emph{relevant} details, known as \emph{features}. This has
two benefits:

\begin{itemize}
\item \emph{Feature vectors} (ordered sets of features) are chosen to be more
  compact than the data they're extracted from: feature extraction is
  \emph{lossy compression}. This reduces the size of the ML problem, improving
  efficiency (e.g. running time).
\item We avoid learning irrelevant details such as the encoding system used,
  improving \emph{data} efficiency (the number of samples required to spot a
  pattern).
\item All of our feature vectors will be the same size, i.e. they will all have
  length (or \emph{dimension}) $d$. Many machine learning algorithms only work
  with inputs of a uniform size; feature extraction allows us to use these
  algorithms in domains where the size of each input is not known, may vary or
  may even be unbounded. For example, element-wise comparison of feature vectors
  is trivial (compare the $i$th elements for $1 \leq i \leq d$); for expressions
  this is not so straightforward, as their nesting may give rise to very
  different shapes.
\item Unlike our expressions, which are discrete, we can continuously transform
  one feature vector into another. This enables many powerful machine learning
  algorithms to be used, such as those based on \emph{gradient descent} or, in
  our case, arithmetic means.
\item Feature vectors can be chosen to represent the relevant information in a
  more compressed form than the raw data; for example, we might replace verbose,
  descriptive identifiers with sequential numbers. This reduces the input size
  of the machine learning problem, improving efficiency.
\end{itemize}

Another benefit of feature extraction is to \emph{normalise} the input data to a
fixed-size representation. Many ML algorithms only work with fixed-size inputs;
for example, the popular \emph{backpropagation} \cite{Russell:2003:AIM:773294}
algorithm works on models with \emph{fixed} topology (e.g. \emph{artificial
  neural networks} with fixed connections between nodes). This requires some
form of pre-processing in domains where the size of each input is not known, may
vary or may even be unbounded.

For example, in the case of \emph{online} learning we must make
predictions/decisions before seeing all of the inputs. Unbounded input appears
in domains such as programming and theorem proving, where individual term may be
trees of unbounded depth. In these situations we need a mapping from arbitrary
inputs to a fixed representation which is amenable to learning.

As an example, say we want to learn relationships between the following program
fragments:

\begin{haskell}
data Maybe a = Nothing | Just a

data Either a b = Left a | Right b
\end{haskell}

We might hope our algorithm discovers relationships like:

\begin{itemize}
  \item Both are valid Haskell code.
  \item Both describe datatypes.
  \item Both datatypes have two constructors.
  \item \hs{Either} is a generalisation of \hs{Maybe} (we can define \hs{Maybe a
      = Either () a} and \hs{Nothing = Left ()}).
  \item There is a symmetry in \hs{Either}: \hs{Either a b} is equivalent to
    \hs{Either b a} if we swap occurences of \hs{Left} and \hs{Right}.
  \item It is trivial to satisfy \hs{Maybe} (using \hs{Nothing}).
  \item It is not trivial to satisfy \hs{Either}; we require an \hs{a} or a
    \hs{b}.
\end{itemize}

However, this is too optimistic. Without our domain-knowledge of Haskell, an ML
algorithm cannot impose any structure on these fragments, and will treat them as
strings of bits. Our high-level hopes are obscured by low-level details: the
desirable patterns of Haskell types are mixed with undesirable patterns of ASCII
bytes, of letter frequency in English words, and so on.

In theory we could throw more computing resources and data at a problem, but
available hardware and corpora are always limited. Instead, feature extraction
lets us narrow the ML problem to what we, with our domain knowlege, consider
important.

There is no \emph{fundamental} difference between raw representations and
features: the identity function is a valid feature extractor. Likewise, there is
no crisp distinction between feature extraction and machine learning: a
sufficiently-powerful learner doesn't require feature extraction, and a
sufficiently-powerful feature extractor doesn't require any learning!
\footnote{Consider a classification problem, to assign a label $l \in L$ to each
  input. If we only extract a single feature $f \in L$, we have solved the
  classification problem without using a separate learning step.}

Rather, the terms are distinguished for purely \emph{practical} reasons: by
separating feature extraction from learning, we can distinguish straightforward,
fast data transformation (feature extraction) from complex, slow statistical
analysis (learning). This allows for modularity, separation of concerns, and in
particular allows ``off-the-shelf'' ML to be re-used across a variety of
different domains.

Even if we have no domain knowledge, we can still use a feature extraction phase
to improve efficiency: first we learn a compact representation for our data, for
example using \emph{autoencoding}; then we use that encoder as a feature
extractor for our main learning task. This stacking of one learning algorithm on
top of another, especially with greedy learning of each layer, has lead to the
recent trend of \emph{deep learning}.

\subsubsection{Truncation and Padding}

The simplest way to limit the size of our inputs is to truncate anything larger
than a particular size (and pad anything smaller). This is the approach taken by
ML4PG \cite{journals/corr/abs-1302-6421}, which limits itself to trees with at
most 10 levels and 10 elements per level; each tree is converted to a
$30 \times 10$ matrix (3 values per tree node) and learning takes place on these
normalised representations.

Truncation is unsatisfactory in the way it balances \emph{data} efficiency with
\emph{time} efficiency. Specifically, truncation works best when the input data
contains no redundancy and is arranged with the most significant data first (in
a sense, it is ``big-endian''). The less these assumptions hold, the less we can
truncate. Since many ML algorithms scale poorly with input size, we would prefer
to eliminate the redundancy using a more aggressive algorithm, to keep the
resulting feature size as low as possible.

\subsubsection{Dimension Reduction}

A more sophisticated approach to the problem of reducing input size is to view
it as a \emph{dimension reduction} technique: our inputs can be modelled as
points in high-dimensional spaces, which we want to project into a
lower-dimensional space.

Truncation is a trivial dimension reduction technique: take the first $N$
coordinates. More sophisticated projection functions consider the
\emph{distribution} of the points, and project with the hyperplane which
preserves as much of the variance as possible (or, equivalently, reduces the
\emph{mutual information} between the points).

There are many techniques to find these hyperplanes, such as \emph{principle
  component analysis} (PCA) and \emph{autoencoding}. If the data are labelled,
these classes can also be taken into account during dimension
reduction~\cite{Oveisi.Oveisi.Erfanian.ea:2012}. However, since these
techniques are effectively ML algorithms in their own right, they suffer some of
the same constraints we're trying to avoid:

\begin{itemize}
  \item They operate \emph{offline}, requiring all input points up-front
  \item All input points must have the same dimensionality
\end{itemize}

In particular, the second constraint is precisely what we're trying to
avoid. Sophisticated dimension reduction is still useful for \emph{compressing}
large, redundant features into smaller, information-dense representations, and
as such provides a good complement to truncation.

The requirement for offline ``batch'' processing is more difficult to overcome,
since any learning we perform for feature extraction will interfere with the
core learning algorithm that's consuming these features (this is why deep
learning is often done greedily).

\subsubsection{Sequences}

To handle input of \emph{variable} size, research attention has been given to
the handling of \emph{sequences}. This is a lossless approach, which splits the
input into fixed-size \emph{chunks} (e.g. splitting text into a sequence of
characters), which are fed into an appropriate ML algorithm one at a time. The
sequence is terminated by a sentinel; an ``end-of-sequence'' marker which, by
construction, is distinguishable from the data chunks. This technique allows us
to trade \emph{space} (the size of our input) for \emph{time} (the number of
chunks in a sequence).

Not all ML algorithms can be adapted to accept sequences. One notable approach
is to use \emph{recurrent ANNs} (RANNs), which allow arbitrary connections
between nodes, including cycles. Compared to \emph{feed-forward} ANNs (FFANNs),
which are acyclic, the \emph{future output} of a RANN may depend arbitrarily on
its \emph{past inputs} (in fact, RANNs are universal computers).

The main problem with RANNs, compared to the more widely-used FFANNs, is the
difficulty of training them. If we extend the standard backpropagation algorithm
to handle cycles, we get the \emph{backpropagation through time} algorithm
\cite{werbos1990backpropagation}. However, this suffers a problem known as the
\emph{vanishing gradient}: error values decay exponentially as they propagate
back through the cycles, which prevents effective learning of delayed
dependencies, undermining the main advantage of RANNs. The vanishing gradient
problem is the subject of current research, with countermeasures including
\emph{neuroevolution} (using evolutionary computation techniques to train an
ANN) and \emph{long short-term memory} (LSTM; introducing a few special,
untrainable nodes to persist values for long time periods
\cite{hochreiter1997long}).

An alternative to introducing recurrent connections, is the use of
\emph{attention}. % TODO: cite
In a (typically feed-forward) ANN with attention, the whole input is provided to
the network, which produces some output as well as some ``attention''
information; the network is run again, but with the input adjusted using this
attention information (increasing and decreasing some of the input values); more
output is produced, along with new attention information, and so on. Such
networks do not need to ``remember'' long-distance relationships, whether by
recurrent connections or gated nodes; they instead use the same input over and
over again, but use their ``attention'' to emphasise those parts which should be
relevant to future iterations.

\subsubsection{Recursive Structure}

Recursive structures, like trees and lists, have \emph{fractal} dimension:
adding layers to a recursive structure gives us more \emph{fine-grained}
features, rather than \emph{orthogonal} features. For data mining context-free
languages (e.g. those of programming and theorem-proving systems), we will
mainly be concerned with tree and graph structures of variable size.

Using sequences to represent recursive structures is also problematic: if we
want our learning algorithm to exploit structure (such as the depth of a token),
it will have to discover how to parse the sequences for itself, which seems
wasteful. The \emph{back-propagation through structure} approach
\cite{goller1996learning} is a more direct solution to this problem, using a
feed-forward NN to learn recursive distributed representations
\cite{pollack1990recursive} which correspond to the recursive structure of the
inputs. Such distributed representations can also be used for sequences, which
we can use to encode sub-trees when the branching factor of nodes is not uniform
\cite{kwasny1995tail}. More recent work has investigated storing recursive
structures inside LSTM cells \cite{zhu2015long}.

A simpler alternative for generating recursive distributed representations is to
use circular convolution \cite{conf/ijcai/Plate91}. Although promising results
are shown for its use in \emph{distributed tree kernels}
\cite{zanzotto2012distributed}, our preliminary experiments in applying
circular convolution to functional programming expressions found most of the
information to be lost in the process; presumably as the expressions are too
small.

\emph{Kernel methods} have also been applied to structured information, for
example in \cite{Gartner2003} the input data (including sequences, trees and
graphs) are represented using \emph{generative models}, such as hidden Markov
models, of a fixed size suitable for learning. Many more applications of kernel
methods to structured domains are given in \cite{bakir2007predicting}, which
could be used to learn more subtle relations between expressions than recurrent
clustering alone.

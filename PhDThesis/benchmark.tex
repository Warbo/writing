\chapter{Theory Exploration Benchmark}
\label{sec:benchmark}

If we wish to make measurable progress on the task of conjecture generation, we
must quantify what it is we are trying to achieve. Various approaches can then
be measured and compared to optimise toward our goal. To this end we define a
benchmarking methodology, shown in Figure~\ref{fig:flow_chart} on
page~\pageref{fig:flow_chart}, which both generates a large
definition/ground-truth corpus, and provides a scalable, statistical approach to
evaluating MTE tools using this corpus. We follow a precision/recall approach
similar to prior work, with the main difference being the source of definitions
and ground truths: we take existing problem sets designed for automated theorem
proving, and adapt their content for use in the conjecture generation setting.

\section{Preparation}
\label{section:prep}

Automated theorem proving is an active area of research, with large problem sets
and regular competitions to prove as much as possible, as fast as
possible~\cite{pelletier2002development}. These problem sets are an opportunity
for MTE, as their definitions and theorems can be used as a corpus in the same
way that Isabelle libraries have been used in the past.

Some problem sets are more amenable for this purpose than others. The most
suitable are those meeting the following criteria:

\begin{itemize}
\item For each problem, there should be a clear distinction between the
  statement to be proved and the definitions involved, such that the two can be
  easily and meaningfully separated. This rules out problem sets like those of
  SMT-COMP~\cite{barrett2005smt}, where many problems involve uninterpreted
  functions, whose behaviour is \emph{implicit} in the logical structure of the
  theorem statement but not separately \emph{defined}.
\item Definitions should be translatable into a form suitable for the MTE tools
  under study. Our application in $\S$\ref{sec:application} requires Haskell and
  Isabelle translations, and also benefits from having definitions be strongly
  typed.
\item The problem set should be relevant to the desired domain. Our focus on
  functional programming requires higher-order functions and inductively defined
  datatypes, which rules out first-order languages/logics (such as
  TPTP~\cite{sutcliffe2009tptp}).
\item The problem set should ideally contain \emph{every} ``interesting''
  property involving its included definitions, since non-membership in the
  ground truth will be treated as being ``uninteresting''. More realistically,
  we should aim for each definition to have \emph{multiple} properties; rather
  than a mixture of unrelated problems, cherry-picked from different domains.
\item Larger problem sets are preferable as they give more robust statistics, all
  else being equal (i.e. when this does not sacrifice quality).
\end{itemize}

Once such a problem set has been chosen, we must separate the definitions from
the theorem statements which reference those definitions. The definitions will
be used as input to the MTE tools, whilst the theorem statements form the ground
truth corpus of properties (which tool output will be compared against).

It is important to ensure that there are no duplicate definitions: we are only
concerned with the \emph{logical} content of the input, not the more arbitrary
aspects of their presentation like the names of functions. For example, consider
a problem set which includes a statement of commutativity for a \texttt{plus}
function, and of associativity for an \texttt{add} function, where the
definitions of \texttt{plus} and \texttt{add} are $\alpha$-equivalent. We would
expect an MTE tool to either conjecture commutativity and associativity for
\emph{both} functions, or for \emph{neither} function, since they are logically
equivalent. Yet a na\"ive precision/recall analysis would treat commutativity of
\texttt{add} and associativity of \texttt{plus} as \emph{uninteresting}, since
they don't appear in the ground truth.

For this reason, duplicates should be removed, and any references to them
updated to use the remaining definition (e.g. chosen based on lexicographic
order). In the above example, the \texttt{plus} function would be removed, and
the commutativity statement updated to reference \texttt{add} instead.

\section{Sampling}
\label{section:sampling}

We could, in theory, send these de-duplicated definitions straight into an MTE
tool and use the entire set of properties (taken from the theorem statements) as
the ground truth for analysis. However, this would cause two problems:

\begin{itemize}
\item The result would be a single data point, which makes it difficult to
  infer performance \emph{in general}.
\item It is impractical to run existing MTE tools on inputs containing more
  than a few dozen definitions.
\end{itemize}

To solve both of these problems we instead \emph{sample} a subset of
definitions.\footnote{This could be done randomly, but for reproducibility we
  use a deterministic order based on cryptographic hashes.} Given a sample size,
we choose a subset of that many definitions, and provide only those as input to
the tool. We generate a corresponding ground truth by selecting those properties
from the corpus which ``depend on'' (contain references to) \emph{only} the
definitions in that sample. Transitive dependencies aren't required (e.g. a
property involving only a \texttt{times} function would not depend on a
\texttt{plus} function, even if \texttt{plus} occurs in the definition of
\texttt{times}).

Unfortunately, uniform sampling of definitions gives rise to a lottery: for a
given sample size, increasing the size of the corpus (which provides better
statistics) makes it less likely that a chosen sample will contain all of
a property's dependencies. The majority of such samples would hence have an
empty ground truth, and thus $0$ precision and undefined recall
\emph{independent} of the tool's output! This is clearly undesirable as an
evaluation method.

Other measurements could be used for such cases, but our approach is to avoid
them by only allowing a sample if it contains all dependencies of at least one
property. We could do this using rejection sampling, but it is more efficient to
pick a property, weighted in proportion to their number of dependencies
(ignoring those with more dependencies than our sample size). That property's
dependencies become our sample, padded up to the required size with uniform
choices from the remaining definitions.

The ground truth for such samples is guaranteed to contain at least one property
(the one we picked), and hence the precision and recall will depend meaningfully
on the tool's output. One downside of this restriction is that we limit the
number of possible samples to measure; this is significant for sample sizes less
than 3.

\section{Evaluation}
\label{section:evaluation}

Given a sample of definitions and a corresponding ground truth, the actual
execution of the MTE tool proceeds as in prior work. We must translate the
chosen definitions into the required input format, then we time the execution
with a timeout (e.g. 5 minutes). We use wall-clock time since
\begin {enumerate*} [i) ]%
\item this is most relevant to a user's experience,
\item it has a straightforward interpretation and
\item measuring it does not require intrusive alterations to a tool's
  implementation.
\end {enumerate*}
The downside is that comparisons must take hardware performance into account,
which would not be the case if time were measured by some proxy like number of
expressions evaluated.

In our experiments we have found that memory usage is also an important part of
a tool's performance, but rather than complicating our analysis with an extra
dimension, we instead allow programs to use as much memory as they like, and
either get killed by the operating system or slow down so much from swapping
that they time out. This is in line with the expected usage of these tools:
either there is enough memory, or there isn't; implementations shouldn't be
penalised for making use of available resources.

To calculate precision and recall, the conjectures generated by each tool, as
well as the ground-truth properties, need to be parsed into a common format and
compared syntactically after normalising away ``irrelevant'' details. We
consider variable naming to be irrelevant, which can be normalised by numbering
free variables from left to right and using de Bruijn indices for bound
variables. We also consider the left/right order of equations to be irrelevant,
which we normalise by choosing whichever order results in the
lexicographically-smallest expression.

We specifically \emph{ignore} other logical relationships between syntactically
distinct statements, such as one equation being implied by another. Whilst
logically sound, second-guessing the ground truth in this way would harm other
aspects which influence interestingness (for example, a more general statement
is more widely applicable, but it might also be harder to comprehend).

This procedure gives us a single runtime, precision and recall value for each
sample. We propose two methods for analysing this data: \emph{summarising}
the performance of a single MTE tool and \emph{comparing} the performance of two
tools.

\subsection{Summarising}

Each sample is only explored once by each tool, so that we cover as many
independent samples as possible to better estimate how a tool's performance
generalises to unseen inputs. How we combine these data into an aggregate
summary depends on what we are interested in measuring. One general question we
might ask is how a tool's performance scales with respect to the input size (the
number of given definitions). This is straightforward to measure by varying the
sample size, but we need some way to combine the results from samples of the
same size.

We can summarise a set of runtimes by choosing the median, as this is more
robust than the mean against long-running outliers, and hence represents
performance for a ``typical'' input of that size. Since our methodology measures
performance over many samples, we can also compute the \emph{spread} of our
data, for example the inter-quartile range. This is not possible using the
one-off measurements common in prior evaluation methods.

Considering precision and recall separately, there are two ways to average these
for a set of samples. Since both of these measurements are ratios, their means
are known as the \emph{average of the ratios} (AOR). Using the definitions of
precision and recall given in $\S$\ref{sec:te}, we can define the average of the
ratios for $S$ samples indexed by $1 \leq i \leq S$ as follows:

\begin{align*}
       \overline{\text{precision}}_{AOR}
    &= \frac{1}{S} \sum_i{\text{precision}_i} \\
    &= \frac{1}{S} \sum_i{
         \frac{\#\text{interesting}_i}
              {\#\text{generated}_i}}         \\[10pt]
       \overline{\text{recall}}_{AOR}
    &= \frac{1}{S} \sum_i{\text{recall}_i} \\
    &= \frac{1}{S} \sum_i{
         \frac{\#\text{interesting}_i}
              {\#\text{groundtruth}_i}}
\end{align*}

We interpret these mean values as the \emph{expected} precision and recall for
samples of this size. In particular, these tell us the quality of the \emph{set}
of conjectures that will be generated if we were to run the tool on such a
sample, but they don't give us direct information about the individual
conjectures themselves. Since these are mean values, they are the reference
point for measures of spread such as the variance, standard deviation and mean
absolute deviation.

The alternative is to calculate \emph{pooled} averages, by combining the
(multi)sets from each sample before taking the ratios. The size of these pooled
sets is simply the sum of the individual set sizes, and ratios of these sums are
equal to ratios of the means (since the normalising factor $\frac{1}{S}$ appears
in both numerator and denominator). Hence these values are known as the
\emph{ratio of the averages} (ROA):

\begin{align*}
       \overline{\text{precision}}_{ROA}
    &= \frac{\sum_i{\#\text{interesting}_i}}
            {\sum_i{\#\text{generated}_i}}                              \\
    &= \frac{\frac{1}{S} \sum_i{\#\text{interesting}_i}}
            {\frac{1}{S} \sum_i{\#\text{generated}_i}}                  \\
    &= \frac{\hspace{5pt} \overline{\#\text{interesting}} \hspace{5pt}}
            {\hspace{5pt} \overline{\#\text{generated}}   \hspace{5pt}} \\[10pt]
       \overline{\text{recall}}_{ROA}
    &= \frac{\sum_i{\#\text{interesting}_i}}
            {\sum_i{\#\text{groundtruth}_i}}                            \\
    &= \frac{\frac{1}{S} \sum_i{\#\text{interesting}_i}}
            {\frac{1}{S} \sum_i{\#\text{groundtruth}_i}}                \\
    &= \frac{\hspace{5pt} \overline{\#\text{interesting}} \hspace{5pt}}
            {\hspace{5pt} \overline{\#\text{groundtruth}} \hspace{5pt}}
\end{align*}

Larger sets contribute more to a ratio of averages, unlike the mean which treats
each set equally. This weighting gives us expected qualities of
\emph{conjectures} rather than sets: the ROA for precision is the expected
chance that some particular generated conjecture will appear in the ground
truth; the ROA for recall is the expected chance that some particular ground
truth property will appear in the generated output.

These averages can differ when there is a large variation in set size. For
example, if we generate a single, interesting conjecture for one sample and 99
uninteresting conjectures for another sample, the AOR (mean) precision will be
$\frac{1}{2}$ but the ROA precision will be $\frac{1}{100}$. This highlights the
importance of knowing \emph{absolute} numbers of conjectures, such as the mean
output size $\overline{\#\text{generated}}$ and its spread, alongside the
proportions given by precision and recall.

It is also possible to combine precision values \emph{with} recall values
(either individually or aggregated) by calculating their \emph{F-score}, defined
as:

\begin{equation*}
  F = 2 \times \frac{\text{precision} \times \text{recall}}
                    {\text{precision} +      \text{recall}}
\end{equation*}

This is the harmonic mean of precision and recall, ranging between 0 and 1. The
F-score summarises performance into a single number, which is useful for
purposes like ranking, but it obscures insights we might gain from looking at
precision and recall separately (e.g. whether a tool is generating too much
output or too little).

\subsection{Comparison}

To measure progress and encourage competition in the field, it is important to
compare the relative performance of different tools on the same task. Since the
aggregate statistics in the above summaries have obscured the details of
specific runs, any comparison based on them (such as comparing mean recall)
would have very low statistical power. More direct comparisons can be made using
\emph{paired} tests, since for each individual sample we have measurements for
\emph{both} tools.

Running times do not follow a normal distribution, in particular due to the
lower bound at 0, which complicates any application of standard comparisons like
the paired z-test. An alternative which does not assume normality is the
Wilcoxon signed-rank test~\cite{wilcoxon1945individual}, which has been applied
to software benchmarking in the Speedup-Test protocol of Touati, Worms and
Briais~\cite{touati2013speedup}. Our methodology differs by measuring with a
different sample each time, rather than repeatedly measuring one sample of each
size; this also allows us to use the paired form of the Wilcoxon test.

Another complication with our time measurements is the censoring effect caused
by timeouts. Paired difference tests are robust to this, since the upper-bound
on total time imposes an upper-bound to the observable difference; this biases
our results in a conservative way, \emph{towards} the null hypothesis of
indistinguishable performance.

Quality can be compared using precision and recall, but we must choose how to
handle samples where one tool succeeded and the other failed (e.g. timing out).
One approach would be to consider failed runs as if they generated an empty set
of conjectures; this conflates quality with failure, which may be desirable, but
we prefer the more charitable approach of comparing only those samples where
both tools finished successfully. We pool the results from such samples together
(allowing duplicates) and compare the resulting proportions.

Recall relies on a fixed set of properties, the ground truth, so we can compare
tools using McNemar's test for paired samples~\cite{mcnemar1947note}. For each
tool, we count how many of the ground-truth properties it found which were
\emph{not} found by the other tool. McNemar's test determines whether we can
reject the null hypothesis that both tools have the same count (i.e. both tools
have a similar contribution to the symmetric difference of their interesting
outputs).

Comparing precision cannot be done in the same way as recall, since we do not
have a fixed set of properties to divide up (there are an unlimited number of
properties that may or may not appear in a tool's output). Instead, we can count
how many conjectures were interesting and uninteresting for each tool and use
Boschloo's form of Barnard's test~\cite{lydersen2009recommended} to determine if
these proportions are independent. This assumes a binomial distribution and is
conditioned on the number of generated conjectures (as if this were fixed by
design).

\begin{figure}
  \tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm,
                           minimum height=1cm,text centered, draw=black]

  \tikzstyle{io} = [trapezium, trapezium left angle=70,
                    trapezium right angle=110, minimum width=1cm,
                    minimum height=1cm, text centered, draw=black]

  \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm,
                         text centered, draw=black]
  \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm,
                          text centered, draw=black]

  \tikzstyle{arrow} = [thick,->,>=stealth]

  % Avoids too much padding in io shapes
  \tikzset{trapezium stretches=true}

  \centering
  \scalebox{0.9}{\begin{tikzpicture}[node distance=1cm]

    % Preparation section

    \node (in)  [io]{Theorem Proving Benchmark};
    \node (sep) [process,       below=1cm of in         ]{Separate definitions from property statements};
    \node (def) [process, below  left=1.5cm and -1cm of sep]{Remove duplicate definitions};
    \node (ref) [process, below right=1.5cm and -1cm of sep]{Update references};

    \node (thy)  [startstop, below=1cm of def]{Distinct Definitions};
    \node (thm)  [startstop, below=1cm of ref]{Corpus of Properties};

    \draw [arrow] (in)  -- (sep);
    \draw [arrow] (def) -- (ref);
    \draw [arrow] (def) -- (thy);
    \draw [arrow] (ref) -- (thm);

    % Arrows with labels
    \path[->]
        (sep) edge [arrow, sloped, above] node {definitions} (def)
        (sep) edge [arrow, sloped, above] node {properties}  (ref);

    % Sampling section

    \node (choose) [process, below=of thm   ]{Choose a property};
    \node (deps)   [process, below=of choose]{List property's dependencies};

    % Create dummy coordinate below deps, then use its y coordinate for pad
    \coordinate [below=of deps] (padDummy);
    \path let \p{dummy} = (padDummy),
              \p{in}    = (in)
              in coordinate (padPos) at (\x{in}, \y{dummy});
    \node (pad)  [process, at=(padPos)]{Pad list to form sample};

    \node (sthm) [startstop, below right=1.5cm and -1cm of pad]{Ground Truth};
    \node (sthy) [startstop, below  left=1.5cm and -1cm of pad]{Sampled Theory};


    % Calculate position of (size) using let, then define as normal
    \path let \p{pad}    = (pad),
              \p{choose} = (choose)
           in coordinate (sizePos) at (\x{pad},\y{choose});
    \node (size) [io, at=(sizePos)] {Sample size};

    % Evaluation section
    \path let \p{pad}  = (pad),
              \p{sthm} = (sthm)
           in coordinate (runPos) at (\x{pad}, \y{sthm});
    \node (run) [process, below=of runPos]{Run MTE tool};
    \node (pr)  [process, below=of run]{Analysis};

    \node (prec) [startstop, below=of pr  ]{Precision};
    \node (time) [startstop,  left=of prec]{Time taken};
    \node (rec)  [startstop, right=of prec]{Recall};

    \draw [arrow] (thy)    |- (pad);
    \draw [arrow] (thm)    -- (choose);
    \draw [arrow] (choose) -- (deps);
    \draw [arrow] (deps)   |- (pad);
    \draw [arrow] (size)   -- (choose);
    \draw [arrow] (size)   -- (pad);
    \draw [arrow] (pad)    -- (sthm);
    \draw [arrow] (pad)    -- (sthy);
    \draw [arrow] (sthy)   |- (run);
    \draw [arrow] (run)    -- (pr);
    \draw [arrow] (sthm)   |- (pr);
    \draw [arrow] (pr)     -- (prec);
    \draw [arrow] (pr)     -- (rec);
    \draw [arrow] (pr)     -- (time);

    % Awkward arrow
    \draw [arrow] (thm) -| ([shift={(7mm,-7mm)}]thm.east) |- (sthm);

    % Braces

    % Preparation
    \draw
      let \p{thm} = (thm.east),
          \p{in}  = (in.north),
          \p{rec} = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{in}) -- (\x{rec}, \y{thm})
         node [black, midway, right, xshift=0.3cm]
              {Preparation $\S$\ref{section:prep}};

    % Sampling
    \draw
      let \p{thm} = (thm.east),
          \p{gt}  = (sthm.east),
          \p{rec} = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{thm}) -- (\x{rec}, \y{gt})
         node [black, midway, right, xshift=0.3cm]
              {Sampling $\S$\ref{section:sampling}};

    % Evaluation
    \draw
      let \p{sthm} = (sthm.east),
          \p{rec}  = (rec.south east)
       in [decorate,decoration={brace, amplitude=10pt}, xshift=0.5cm]
       (\x{rec}, \y{sthm}) -- (\x{rec}, \y{rec})
         node [black, midway, right, xshift=0.3cm]
              {Evaluation $\S$\ref{section:evaluation}};

  \end{tikzpicture}}
  \caption[]{High-level view of the benchmarking methodology described in
    $\S$\ref{sec:benchmark}, showing
    \begin{tikzpicture}
      \node [rectangle, text centered, draw=black]{processes};
    \end{tikzpicture},
    \begin{tikzpicture}
      \node [rectangle, rounded corners, text centered, draw=black]{data};
    \end{tikzpicture} and
    \begin{tikzpicture}
      \node [trapezium, trapezium left angle=70, trapezium right angle=110,
             text centered, draw=black]{inputs};
  \end{tikzpicture}}
  \label{fig:flow_chart}
\end{figure}

\section{Application}
\label{sec:application}

We have applied our benchmarking methodology to the \quickspec{} and \isacosy{}
MTE tools, using version 0.2 of the TIP (Tons of Inductive Problems) theorem
proving benchmark as our ground truth corpus~\cite{claessen2015tip}.

To determine our benchmarking parameters we ran some initial tests on both tools
for a few samples sized between 1 and 100, for an hour each on our benchmarking
machine with a 3.2GHz dual-core Intel i5 processor with hyper-threading and 8GB
of RAM. Most \quickspec{} runs either finished within 200 seconds or not at all,
and sizes above 20 mostly timed out. \isacosy{} mostly finished within 300 seconds
on sample sizes up to 4, but by size 8 was mostly timing out; its few successes
above this took thousands of seconds each, which we deemed infeasibly long.

Based on this we decided to benchmark sample sizes up to 20, since neither tool
seemed to perform well beyond that. The Speedup-Test protocol follows the
statistical ``rule of thumb'' of treating sample sizes $\leq$ 30 as ``small'',
so we pick 31 samples of each size in order to cross this threshold. This gave a
total of 1240 runs. To keep the benchmarking time down to a few days we chose a
timeout of 300 seconds, since that covered most of the successful \quickspec{} and
\isacosy{} results we saw, and longer times gave rapidly diminishing
returns. During analysis, duplicate samples (caused by our requirement that
samples have a non-empty ground truth) were found and discarded, so only 14
samples of size 1 were used and 30 of size 2.

\subsection{Tons of Inductive Problems}
\label{sec:tip}

We chose the Tons of Inductive Problems (TIP) benchmark for our ground truth
since it satisfies the criteria specified in $\S$\ref{section:prep}: each
benchmark problem has standalone type and function definitions, making their
separation trivial; known examples from the software verification and inductive
theorem proving literature are included, ensuring relevance to those fields; the
format includes the higher-order functions and inductive datatypes we are
interested in; it is large enough to pose a challenge to current MTE tools; plus
it is accompanied by tooling to convert its custom format (an extension of
SMT-Lib~\cite{BarFT-SMTLIB}) into a variety of languages, including Haskell and
Isabelle.

We use TIP version 0.2 which contains 343 problems, each stating a single
property and together defining a total of 618 datatypes and 1498 functions. Most
of these are duplicates, since each problem (re\nobreakdash-)defines all of the
datatypes and functions it involves.

TIP datatypes can have several ``constructors'' (introduction forms) and
``destructors'' (elimination forms; field accessors). For example the type of
lists from Figure~\ref{fig:list_theory} can be defined in the TIP format as
follows:

\begin{samepage}
\begin{verbatim}
(declare-datatypes
  (a)                       ;; Type parameter (element type)
  ((List                    ;; Type name
     (Nil)                  ;; Constructor (nullary)
     (Cons                  ;; Constructor (binary)
       (head a)             ;; Field name and type
       (tail (List a))))))  ;; Field name and type
\end{verbatim}
\end{samepage}

Our target languages (Haskell and Isabelle) differ in the way they handle
constructors and destructors, which complicates comparisons. To avoid this, we
generate a new function for each constructor (via $\eta$-expansion) and
destructor (via pattern-matching) of the following form:

\begin{samepage}
\begin{verbatim}
(define-fun
  (par (a)                   ;; Type parameter
    (constructor-Cons        ;; Function name
      ((x a) (xs (List a)))  ;; Argument names and types
      (List a)               ;; Return type
      (as                    ;; Type annotation
        (Cons x xs)          ;; Return value
        (List a)))))         ;; Return type
\end{verbatim}
\end{samepage}

\begin{samepage}
\begin{verbatim}
(define-fun
  (par (a)                        ;; Type parameter
    (destructor-head              ;; Function name
      ((xs (List a)))             ;; Argument name and type
      a                           ;; Return type
      (match xs                   ;; Pattern-match
        (case (Cons h t) h)))))   ;; Return relevant field
\end{verbatim}
\end{samepage}

\begin{sloppypar}
  We rewrite the TIP properties (our ground truth) to reference these expanded
  forms instead of the raw constructors and destructors, and use these functions
  in our samples in lieu of the raw expressions. Note that these destructor
  wrappers are \emph{partial} functions (e.g. \texttt{destructor-head} and
  \texttt{destructor-tail} are undefined for the input \texttt{Nil}), which
  complicates their translation to proof assistants like Isabelle.
\end{sloppypar}

Another complication is TIP's ``native'' support for booleans and integers,
which allows numerals and symbols like \texttt{+} to appear without any
accompanying definition. To ensure consistency in the translations, we replace
all occurrences of such expressions with standard definitions written with the
``user-level'' \texttt{declare-datatypes} and \texttt{define-fun}
mechanisms.~\footnote{\texttt{Boolean} has \texttt{true} and \texttt{false}
  constructors; \texttt{Natural} has \texttt{zero} and \texttt{successor};
  \texttt{Integer} has unary \texttt{positive} and \texttt{negative}
  constructors taking \texttt{Natural}s, and a nullary \texttt{zero} for
  symmetry.}

When we add all of these generated types and functions to those in TIP, we get a
total of 3598 definitions. Removing $\alpha$-equivalent duplicates leaves 269,
and we choose to only sample from those 182 functions which are referenced by at
least one property (this removes ambiguity about which \emph{definitions} count
as interesting and which are just ``implementation details'' for other
definitions).

TIP comes with software to translate its definitions into Haskell and Isabelle
code, including comparison functions and random data generators suitable for
\quickcheck{}. We translate all 269 unique definitions into a single module/theory
which is imported on each run of the tools, although only those functions which
appear in the current sample are included in the signature and explored. We also
encode all names in hexadecimal to avoid problems with language-specific naming
rules, for example \texttt{add} becomes \texttt{global616464} (the prefix
distinguishes these from local variables and prevents names from beginning with
a digit). This ensures that the generated conjectures will be using the same
names as the ground truth, rather than some language-specific variant.

\subsection{\quickspec{}}

We benchmarked \quickspec{} version 0.9.6, a tool written in Haskell for
conjecturing equations involving Haskell functions, described in more detail in
$\S$\ref{sec:theoryexploration}. In order to thoroughly benchmark \quickspec{},
we need to automate some of the decisions which are normally left up to the
user:

\begin{sloppypar}
  \begin{itemize}
  \item We must decide what variables to include. We choose to add three
    variables for each type that appears as a function argument, except for
    types which have no \quickcheck{} data generators.
  \item We must \emph{monomorphise} all types. For example, functions like
    \texttt{constructor-Cons} are \emph{polymorphic}: they build lists of any
    element type, but we need to pick a specific type in order to know which
    random value generator to use. We resolve this (arbitrarily) by picking
    \texttt{Integer}.~\footnote{We pick \texttt{Integer} for variables of kind
      \texttt{*} (types); for kind \texttt{* -> *} (type constructors) we pick
      \texttt{[]} (Haskell's list type constructor). If these violate some
      type class constraint, we pick a suitable type non-deterministically from
      those in scope during compilation; if no suitable type is found, we give
      up and don't include that function.}
  \item Haskell functions are ``black boxes'', which \quickspec{} can't compare
    during its exploration process. They are also curried, always taking one
    argument but potentially returning another function. \quickspec{} lets us
    assign an arity to each function in the signature, from 0 to 5, so we pick
    the highest that is type-correct, since this avoids a proliferation of
    incomparable, partially-applied functions.
  \end{itemize}
\end{sloppypar}

\begin{figure}
  \centering
  \input{quickspectime.pgf}
  \caption{Running times of \quickspec{} on theories sampled from TIP. Each point
    is a successful run (spread out horizontally to reduce overlaps). Red
    circles show runs which timed out, which occurred for every size. Each size
    has 31 runs total, except for size 1 (14 runs) and size 2 (30 runs) due to
    sampling restrictions. Box plots show inter-quartile range, which reaches
    the timeout for sizes above 8. Medians are marked with a line and remain
    near zero until size 10, with those of size 13 and 20 timing out. Whiskers
    show 1.5$\times$~inter-quartile range.}
  \label{fig:quickspec_runtimes}
\end{figure}

\begin{figure}
  \centering
  \input{quickspecprec.pgf}
  \caption{Precision and recall of successful \quickspec{} runs (spread
    horizontally to reduce overlapping). Lines show the mean proportion for each
    sample size (average of the ratios) and errorbars show the sample standard
    deviation. Mean precision steadily reduces from around $0.5$ at size 1 to
    around $0.1$ for size 20, whilst mean recall remains roughly flat between
    $0.2$ and $0.5$}
  \label{fig:quickspec_precRec}
\end{figure}

The time taken to explore samples of different sizes is shown in
Figure~\ref{fig:quickspec_runtimes}. Failed runs are shown in red: all
failures were due to timing out, and these occurred for all sample sizes but are
more common for larger samples (over half the runs for sample sizes 13 and 20
timed out). Runs which succeeded mostly finished within 30 seconds, generating
few or no conjectures; those taking longer generated more conjectures, with the
most being 133 conjectures from a sample of size 19.

Precision and recall are shown in Figure~\ref{fig:quickspec_precRec}.
Since \quickspec{} generates monotonically more conjectures as definitions are
added to a signature (assuming sufficient testing), its decreasing precision
can't be due to finding fewer wanted conjectures at larger sizes. This is
supported by the relative flatness of the recall results. Rather, the number of
conjectures generated is increasing at a higher rate than the size of the ground
truth. These extra conjectures may involve those ``padding'' definitions in a
sample which don't contribute to its ground truth, or may be ``uninteresting''
relationships between the dependencies of different ground truth properties.

This indicates two potential improvements to the \quickspec{} algorithm (as far as
this benchmark is concerned). The deluge of generated conjectures could be
filtered down to a more desirable sub-set by another post-processing step.
Alternatively, rather than exploring all of the given definitions together,
multiple smaller signatures could be selected from the input by predicting which
combinations are likely to lead to interesting conjectures; this would avoid
both the ``padding'' and the cross-dependency relationships. Both of these
methods could improve the precision, although care would be needed to avoid a
large reduction in recall. The latter option could also improve the running
time, since (based on Figure~\ref{fig:quickspec_runtimes}) multiple smaller
signatures may be faster to explore than a single large one. Such improvements
would also need to be ``generic'', to avoid over-fitting to this particular
benchmark.

\quickspec{}'s recall is limited by two factors: the algorithm is unable to
synthesise some properties, such as conditional equations, inequalities, terms
larger than the search depth and those containing anonymous functions. The
congruence closure algorithm used as a post-processor may also be removing
``interesting'' results, for example if we found an ``uninteresting'' result
which is more general.

\subsection{\isacosy{}}

We took \isacosy{} from version 2015.0.3 of the IsaPlanner project, and ran it
with the 2015 version of Isabelle. The following issues had to be overcome to
make our benchmark applicable to \isacosy{}:

\begin{itemize}
\item TIP includes a benchmark called \texttt{polyrec} whose types cannot be
  encoded in Isabelle. We strip out this type and the functions which depend on
  it before translating. It still appears in samples and contributes to the
  ground truth, which penalises \isacosy{} for being unable to explore such
  definitions.
\item When using a type in an \isacosy{} signature, that type's constructors will
  automatically be included in the exploration. Since those constructors will
  not appear in the ground truth (we use $\eta$-expanded wrappers instead, and
  even those may not be present in the current sample) this will unfairly reduce
  the calculated precision. To avoid this, we add a post-processing step which
  replaces all occurrences of a constructor with the corresponding wrapper, then
  discards any conjectures which involve functions other than those in the
  current sample. This presumably results in more work for \isacosy{}, exploring
  constructors unnecessarily, but it at least does not bring down the quality
  measures.
\item Since Isabelle is designed for theorem proving rather than programming, it
  requires every definition to be accompanied by proofs of exhaustiveness and
  termination. These are difficult to generate automatically, and don't exist in
  the case of partial functions like destructor wrappers. Hence we use the
  ``quick and dirty'' option in Isabelle, which lets us skip these proofs with
  the \texttt{sorry} keyword.
\item Partial functions cause problems during exploration, since they can throw
  an ``undefined'' exception which causes \isacosy{} to abort. We avoid this by
  pre-populating \isacosy{}'s constraint set with these undefined expressions
  (for example \texttt{(destructor-head constructor-Nil)}), hence preventing
  \isacosy{} from ever generating them.
\end{itemize}

\begin{figure}
  \centering
  \input{isacosytime.pgf}
  \caption{Running times of \isacosy{} on theories sampled from TIP. Each point
    is a successful run (spread out horizontally to reduce overlaps). Red
    circles are failed runs, caused by timeouts or out-of-memory. All runs of
    size 1 succeeded, whilst nothing succeeded above size 6. Each size has 31
    runs total, except for size 1 (14 runs) and size 2 (30 runs) due to sampling
    restrictions. Box plots show inter-quartile range, which grows with size,
    and lines mark the median time, which increases rapidly from around 100
    seconds at size 1 to timing out at size 4 and above. Whiskers show
    1.5$\times$~inter-quartile range.}
  \label{fig:isacosy_runtimes}
\end{figure}

\begin{figure}
  \centering
  \input{isacosyprec.pgf}
  \caption{Precision and recall of all successful \isacosy{} runs (spread
    horizontally to reduce overlapping; all runs above size 6 failed). Lines
    show the mean proportion for each sample size (average of the ratios), which
    trends downwards for precision and appears flat for recall, although there
    are too few datapoints to be definitive. Errorbars show the sample standard
    deviation.}
  \label{fig:isacosy_precRec}
\end{figure}

The most striking result is how rapidly \isacosy{}'s running time, shown in
Figure~\ref{fig:isacosy_runtimes}, increases with sample size: all runs above
size 6 failed, with most timing out and a few aborting early due to running out
of memory. Like in our preliminary testing, this increase appears exponential,
so even large increases to the timeout would not produce many more successful
observations. A significant amount of \isacosy{}'s time (around 50 seconds) was
spent loading the generated theory (containing all distinct datatypes and
functions from TIP); this overhead is unfortunate, but since it is constant
across all sample sizes it does not affect our conclusions.

With so few successful datapoints it is difficult to make strong claims about
the precision/recall quality of \isacosy{}'s output, shown in
Figure~\ref{fig:isacosy_precRec}. It is clear from the quantity of non-zero
proportions that \isacosy{} is capable of discovering interesting conjectures, and
the graphs appear to follow the same shapes as those of \quickspec{} (decreasing
precision, flat recall), although the error margins are too wide to be
definitive.

\subsection{Comparison}

We compare the running times of \quickspec{} and \isacosy{} using our paired variant
of the Speedup-Test protocol, where each of our sample sizes is a separate
``benchmark''. We used version 2 of the Speedup-Test \texttt{R} implementation,
which we patched to use the \emph{paired} form of the (one-sided) Wilcoxon
signed-rank test~\cite{wilcoxon1945individual}. We use Pratt's
method~\cite{pratt1959remarks} to handle ties, which occur when both tools time
out.

For each sample size (``benchmark'') the Speedup-Test protocol compares the
distributions of each tool's times using a Kolmogorov-Smirnov test. If these are
significantly different (with $\alpha < 0.05$), then the signed-rank test is
only performed if more than 30 samples are available. This was the case for all
sample sizes except for 1 and 2 (due to the removed duplicates), and hence those
two speedups were not deemed statistically significant. For all other sample
sizes \quickspec{} was found to be significantly faster with $\alpha = 0.05$,
putting the proportion of sizes with faster median times between 66.9\% and
98.2\% with 95\% confidence. The magnitude of each speedup (median \isacosy{} time
divided by median \quickspec{} time) is given in table~\ref{table:speedups}. We
would predict the speedup to grow for larger sample sizes, but the increasing
proportion of timeouts causes our estimates to become more conservative: by size
20 most runs are timing out, and hence are tied.

\begin{table}
  \centering
  \begin{tabular}{ |r|l| }
    \hline
    \bfseries Sample Size & \bfseries Speedup
    \csvreader[]{speedups.csv}{}
    {\\\hline\csvcoli&\csvcolii} \\
    \hline
  \end{tabular}
  \caption{Speedup from using \quickspec{} compared to \isacosy{}, i.e. the median
    time taken by \isacosy{} divided by that of \quickspec{}. \quickspec{} was found to
    be significantly faster ($\alpha = 0.05$) for all sizes, except 1 and 2 due
    to too few samples. These are conservative estimates, since the 300 second
    time limit acts to reduce the measured time difference (e.g. sizes 13 and
    20, where the median for both tools timed out).}
  \label{table:speedups}
\end{table}

We compare the recall proportions by applying McNemar's test to only those
samples where both tools succeeded. We pooled these together from all sample
sizes to produce table~\ref{table:recall}, and found that the recall of
\quickspec{} ($37\%$) is significantly higher than \isacosy{} ($25\%$) with a p-value
of 0.0026.

\begin{table}
  \centering
  \begin{tabular}{ |r|l|l| }
    \hline
    & \bfseries Found by \isacosy{} & \bfseries Not found by \isacosy{} \\
    \hline
    \bfseries Found by \quickspec{}     & 28 & 19 \\
    \hline
    \bfseries Not found by \quickspec{} & 4  & 75 \\
    \hline
  \end{tabular}
  \caption{Contingency table for ground truth properties, pooled from those
    samples where both \quickspec{} and \isacosy{} finished successfully. The combined
    recall for \quickspec{} is $37\%$ and for \isacosy{} is $25\%$.}
  \label{table:recall}
\end{table}

McNemar's test is not applicable for comparing precision, since each tool
generated different sets of conjectures. Instead, we add up the number of
results which are ``interesting'' (appear in the ground truth) and those which
are ``uninteresting'' (do not appear in the ground truth), with the results
shown in table~\ref{table:precision}

\begin{table}
  \centering
  \begin{tabular}{ |r|l|l| }
    \hline
              & \bfseries Interesting & \bfseries Uninteresting \\
    \hline
    \bfseries \isacosy{}   & 32          & 137      \\
    \hline
    \bfseries \quickspec{} & 47          & 301      \\
    \hline
  \end{tabular}
  \caption{Interesting and uninteresting conjectures generated by \quickspec{} and
    \isacosy{}, pooled from those samples where both both finished successfully.
    The combined precision for \quickspec{} is $14\%$ and for \isacosy{} is $19\%$.}
  \label{table:precision}
\end{table}

We tested these totals for independence using Boschloo's test and find a p-value
of 0.111, which exceeds our (conventional) significance threshold of
$\alpha = 0.05$; hence we do not consider the difference between these
proportions (14\% for \quickspec{}, 19\% for \isacosy{}) to be significant.

Note that precision is the only comparison where \isacosy{} scored higher, and even
that was not found to be significant. Also, precision on its own is not the
whole story: it can be increased at the expense of recall by making a tool more
conservative. \isacosy{} may already be overly-conservative compared to \quickspec{},
given that its recall is significantly lower.

More importantly, the poor time and memory usage of \isacosy{} meant that very few
samples finished successfully. If we include failed runs in our tables, treating
them as if they succeeded with no output, the resulting statistics lean
overwhelmingly in favour of \quickspec{} simply because it generated so much more
than \isacosy{} in the available time.

\subsection{Discussion}
\label{sec:discussion}

Fundamentally, when designing any mathematical reasoning system we must decide
on, and formalise, what counts as ``the good'' in mathematics.  Obvious metrics
such as ``true'' or ``provable'' include trivial tautologies, while at the same
time failing to capture the ``almost true'', which can be a valuable trigger for
theory change, as demonstrated by Lakatos in his case studies of mathematical
development~\cite{lakatos}. ``Beautiful'' is another -- albeit vague -- commonly
proposed metric. Neuro-scientists such as Zeki \etal{} have attempted to shed
light on this by testing whether mathematicians' experiences of abstract beauty
correlates with the same brain activity as experiences of sensory
beauty~\cite{10.3389/fnhum.2014.00068}. Qualities from computer scientists like
Colton (such as those in~\cite{colton2000notion}) are based largely on
``intuition'', plausibility arguments about why a metric would be important, and
previous use of such metrics (in the case of ``surprisingingness'', from a
single quote from a mathematician). Opinions from mathematicians themselves
include Gowers' suggestion that we can identify features which are commonly
associated with good proofs~\cite{gowers2000two}, Erdos's famous idea of ``The
Book''~\cite{aigner2010proofs} as well as McCasland's personal evaluation of the
interestingness of MATHsAiD's output~\cite{roy} (of which he was the main system
developer).

All of these approaches rest upon the assumption that it makes sense to speak of
``the good'' in mathematics. However, empirical psychological studies
call into question such assumptions: for example, work by Inglis and colleagues
has shown that there is not even a single standard of \emph{validity} among
contemporary mathematicians~\cite{inglis2013mathematicians}.

Whilst these difficulties are real and important, we cannot ignore the fact that
mathematics is nevertheless being practised around the world; and similarly that
researchers have forged ahead to develop a variety of tools for automated
exploratory mathematics. If we wish to see these tools head in a useful,
fruitful direction then \emph{some} method is needed to compare their
approximate ``quality'' in a concrete, measurable way.

The key to our benchmarking methodology is to side-step much of this
philosophical quagmire using the simplifying assumption that theorem proving
problem sets are a good proxy for desirable input/output behaviour of MTE tools.
As an extension of existing precision/recall analysis, this should hopefully not
prove too controversial, although we acknowledge that there are compelling
reasons to refute it.

We do not claim that our use of corpora as a ground truth exactly captures all
interesting conjectures of their definitions, or that those definitions exactly
represent all theories we may wish to explore. Rather, we consider our approach
to offer a pareto-optimal balance between theoretical rigour and experimental
practicality, at least in the short term. Futhermore, since research is already
on-going in these areas, we hope to at least improve on existing evaluation
practices and offer a common ground for future endeavours.

One notable weakness is that our methodology does not allow negative examples,
such as particularly dull properties that tools should never produce. Everything
outside the ground truth set is treated equally, whether it's genuinely
uninteresting or was only left out due to oversight. In particular this limits
the use of our approach to domains where existing human knowledge surpasses that
discovered by the machine. Any \emph{truly novel} insights discovered during
testing will not, by definition, appear in any existing corpus, and we would in
fact \emph{penalise} tools for such output. We do not believe this to be a
realistic problem for the time being, as long as evaluation is limited to
well-studied domains and results can be generalised to real areas of
application. This does emphasise the continued importance of testing these tools
with real human users, rather than relying solely on artificial benchmarks.

Another practical limitation of our benchmarking approach is that it only
applies to tools which act in ``batch mode'', i.e. those which choose to halt
after emitting some output. Whilst all of the systems we have encountered are of
this form, some (such as \isacosy{}, \quickspec{} 2 and \speculate{}) could
conceivably be run without a depth limit, and form part of the ``scientist
assistant'' role which Lenat envisaged for AM, or McCarthy's earlier ``advice
taker''~\cite{McCarthy_Programs59}. Analogous benchmarks could be designed for
such long-running, potentially interactive programs, but that is beyond the
scope of this project.

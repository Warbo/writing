\chapter{Future Work}
\label{sec:future}

Our use of clustering to pre-process \quickspec{} signatures has required many
decisions and tradeoffs to be made. Hence our approach is just one possibility
out of many alternatives which could be investigated to push this work
further. In addition, there are other ways in which machine learning could aid
theory exploration besides our relevance filter technique. Below, we elaborate
on the details, background and motivation for these choices.

\section{Clustering Extensions}
\label{sec:preprocessing}

The most glaring omission in our algorithm is its disregard for types. By
ignoring types, not only are we losing valuable information about expressions,
but we also lose the ability to distinguish between constructors. This is
because a constructor, like \hs{True} or \hs{Just}, has no internal structure;
it is just a token. The distinguishing features of constructors are their types,
which not only tell us which data type they construct, but also their arity, the
types of their arguments, etc.

Our algorithm closely follows that of ML4PG, which \emph{does} support
types. This is handled by populating matrix cells with tokens \emph{and} their
types. Unfortunately this is more complicated in Haskell than it is in Coq,
since types form a separate part of the language from terms, and we do not have
an interactive Core environment to query for types (unlike ML4PG, which runs
inside the Proof General environment).

One partial solution would be leave most Core expressions without types, but to
include them for non-local identifiers (i.e. globals and constructors), which we
can look up in a database. In fact, our \mlforhs{} framework already includes such
type information in its database, alongside the Core syntax trees. Integrating
this information into our algorithm is the next logical step.

We can also compare the performance of our hand-selected features with
\emph{learned} representations, like those reviewed in
\cite{bengio2013representation}. This may provide an indication of how
important it is to understand the language when identifying salient aspects of
expressions, and how difficult various aspects of it might be to learn.

With more expressive features, it may also be useful to experiment with more
powerful learning algorithms. An interesting possibility is to add a feedback
loop between the theory exploration phase and the clustering phase, to more
directly base the similarity of expressions on whether they (are predicted to)
occur together in equations.

\section{Theory Exploration Extensions}

Our current approach is a rather conservative change to the existing theory
exploration approaches, as it is essentially a wrapper around \quickspec{}.
There is potential for more radical changes to be made, which alter the search
process itself.

\subsection{Variable Instantiation}

\quickcheck{} is certainly the most popular property checker for Haskell, which
motivates its use in \quickspec{} to instantiate variables to random
values. However, this task of finding type inhabitants has also been solved in
many other ways, which may be worth investigating in place of \quickcheck{} (or
perhaps even as part of an ensemble).

The \smallcheck{} system \cite{runciman2008smallcheck} \emph{enumerates}
values rather than sampling them randomly. Whilst this does not make
\smallcheck{} objectively ``better'' than \quickcheck{}, one major advantage
is that it may use much less memory, as the generated values are built up
incrementally. In contrast, \quickcheck{} may generate very large values; in
particular, generating tree structures na\"{\i}vely can cause them to grow
exponentially. For example, here is a potential generator for \hs{RoseTree}s:

\begin{haskell}
genRoseTree = do f        <- arbitrary
                 subtrees <- listOf genRoseTree
                 return (Node f subtrees)
\end{haskell}

The \hs{listOf genRoseTree} call will return a list of arbitrary length, where
each element is generated by \hs{genRoseTree}. This allows an arbitrary number
of recursive calls to be made for each invocation of \hs{genRoseTree}, which
will quickly exhaust the resources of any machine. Whilst such problems may be
anticipated, or quickly spotted, in a property checking setting, this can be
more difficult for our automated approach. For example, if a type does not have
a generator available, we cannot use a library like \hs{derive} to create one
automatically, as it suffers from this na\"{\i}vity problem.

A relative of \smallcheck{} is \lazysmallcheck{}~\cite{reich2013advances}, which
uses laziness to only produce parts of a datastructure as they are demanded.
This may narrow down our search procedures greatly, especially when predicates
are involved. \quickcheck{} allows predicates to restrict the values it tests
with, and hence allows \emph{conditional} equations to be discovered. However,
its implementation uses a simple rejection sampling technique: values are
generated just as if the predicate were not there, and afterwards are filtered
to reject any which do not satisfy the predicate. This makes it difficult to use
very specific predicates, as it is unlikely that many of our random samples will
exactly match our criteria. On the other hand, \lazysmallcheck{} will focus its
search on exactly those parts of the datastructure which are checked by the
predicate, as those are the parts being forced to evaluate. This makes it much
more likely that we will find values which satisfy the predicate, allowing us to
effectively explore more specific conditional properties.

Other approaches to generating inhabitants include
\djinn{}~\cite{augustsson2005djinn}, which uses a decision procedure for a sub-set of
Haskell types which in particular can generate and apply functions (unlike the
above tools, which generate values ``bottom-up'' from constructors, and only use
functions when they have been explicitly written in a
generator). \mucheck{}~\cite{le2014mucheck} is designed for
\emph{mutation testing}, and contains combinators for altering functions in
common ways (e.g. changing the order of pattern match clauses); whilst not as
exhaustive as the other approaches, mutating existing values in this way is
claimed to yield values which correspond more closely to what a programmer might
write. This is an interesting possibility for focusing theory exploration on to
more ``realistic'' areas of the search space, and hence avoiding some of the
more useless or bizarre expressions that random search and enumeration may
produce.

In fact, the database generated by our \astplugin{} may prove helpful in
generating values, since its type information can be fed to a tool like
\djinn{} to discover chains of function applications for building values,
which would be particularly useful in cases where constructors are private, like
in our email example. This is similar to the \hoogle{} tool, but also offers the
ability to use dependency information to avoid potentially infinite recursion.

The Core syntax trees in our database could also be used to generate theories
for automated theorem provers. \hipspec{} currently uses the GHC API to transform
Core within its own process, however that approach suffers from the problems
described in \S~\ref{sec:astplugin}.

\subsection{Interestingness}

If we do succeed in producing a fast theory exploration system, which chooses
productive combinations of terms and finds a large number of properties, we
encounter the problem of managing the output: finding the needles we are
interested in among the haystack of trivialities and coincidences.

This is governed by the ``interestingness'' criteria of the theory exploration
system: what to keep and what to discard, and even what areas of the search
space to prioritise. \quickspec{}'s approach, briefly mentioned in
\S~\ref{sec:theoryexploration}, is very simple: we discard equations which are
direct consequences of others, and keep all the rest. Different, and more
sophisticated notions of interestingness have been widely studied in other
fields, which may be applied in the context of theory exploration.

\subsubsection{Concept Formation} \label{sec:conceptformation} \leavevmode \newline

One directly applicable area to consider is \emph{concept formation}, which
considers the (automatic) generation of new definitions and axioms. Unlike
theory exploration, such systems are not constrained by the requirement that
their output be provable, and hence interestingness is an important way to judge
the quality of the results.

Approaches vary, from those which are directly related to theory exploration
(such as the scheme-instantiating approach of
\cite{Montano-Rivas.McCasland.Dixon.ea:2012}, which forms part of a theory
exploration system), to others which are more closely related to theories of
human learning and discovery \cite{Piantadosi.Tenenbaum.Goodman:2012,
  mullerunderstanding}. Those based on finding patterns in data, such as
\cite{Wille:2005}, may be useful in tandem with our expression database, and
the results of value generators like \quickcheck{}.

Since tools like \hipspec{} already call out to third-party automated theorem
provers, there may also be merit in using external concept formation or
conjecture generation tools (or reimplementations of their ideas), in order to
build up more structure on top of that provided by the code we are exploring.
For example, the approaches taken by
AM~\cite{lenat1977automated, lenat1979automated},
Graffiti~\cite{delavina2005some, delavina2005graffiti} and
HR~\cite{colton1999automatic, colton2000agent} could be used alongside those of
\quickspec{}.

%\subsubsection{Artificial Curiosity} \label{sec:curiosity} \leavevmode \newline

\paragraph{Evolutionary Computation} \leavevmode \newline

Coevolution is a form of \emph{evolutionary computation}; an umbrella term for
heuristic search algorithms which mimic the process of evolution by natural
selection among a population of candidate solutions
\cite{back1997evolutionary}. Whilst \emph{genetic algorithms} are perhaps the
most well-known instance of evolutionary computation, their use of
\emph{strings} to represent solutions causes complications when comparing to a
domain like theory exploration, where recursive structures of unbounded depth
arise. Thankfully these problems are not insurmountable, for example
\emph{genetic programming} can operate on tree-structures natively
\cite{banzhaf1998genetic}, which makes evolutionary computation a useful source
of ideas for reuse in our theory exploration setting (there are also precedents
for using evolutionary computation in a theorem proving domain
\cite{spector2008genetic}).

Traditionally, evolutionary approaches assign solutions a \emph{fitness} value,
using a user-supplied \emph{fitness function}. Fitness should correlate with how
well a solution solves the user's problem; for example, the fitness of a
solution to some engineering problem may depend on the estimated materials
cost. If we frame the task of theory exploration in evolutionary computation
terms, the fitness function would be our interestingness measure.

Pure exploration (i.e. for its own sake) has been studied in evolutionary
computation for two main reasons: \emph{artificial life} and \emph{deceptive
  problems}. The former attempts to gain insight into the nature of life and
biology through competition over limited resources. Whilst this may have utility
in resource allocation, e.g. efficient scheduling of a portfolio of ATP
programs, there is no direct connection to interestingness in theory
exploration, so we will not consider it further (note that similar
resource-usage ideas can also be found in the literature on \emph{artificial
  economies}, e.g. \cite{baum2000evolution}).

On the other hand, work on deceptive problems is highly relevant, as it has lead
to studying various notions of intrinsic fitness, which are analogous to the
interestingness measures we want. Deceptive problems are those where
\textquote{pursuing the objective may prevent the objective from being reached}
\cite{lehman2011abandoning}, which is caused by the fitness (objective)
function having many local optima which are easy to find (e.g. by hill
climbing), but few global optima which are hard to find. Many approaches try to
avoid deception by augmenting the given fitness function to promote
\emph{diversity} and \emph{novelty}, such as \emph{niching methods}
\cite{sareni1998fitness}.

One example is \emph{fitness sharing}, which divides up fitness values between
identical or similar solutions. Say we have a user-provided fitness function
$f$, and a population containing two identical solutions $s_1$ and $s_2$; hence
$f(s_1) = f(s_2)$. In a fitness sharing scheme, we interpret fitness as a fixed
resource, distributed according to $f$; when multiple individuals occupy the
same point in the solution space, they must \emph{share} the fitness available
there. We can describe the fitness \emph{allocated} to a solution by augmenting
$f$, e.g. if we allocate fitness uniformly between identical solutions we get:

$$f'(x) = \frac{f(x)}{\sum_{i=1}^n \delta_{s_i x}}$$

Where $n$ is the population size, $s_i$ is the $i$th solution in the population
and $\delta$ is the Kronecker delta function. In the example above, assuming
there are no other copies in the population, then
$f'(s_1) = \frac{f(s_1)}{2} = \frac{f(s_2)}{2} = f'(s_2)$. By sharing in this
way, the fitness of each solution is balanced against redundancy in the
population: there may still be many copies of a solution, but only when the
fitness is high enough to justify all of them.

There are many variations on this theme, such as sharing between ``close''
solutions rather than just identical ones and judging distance based on fitness
(AKA phenotypically) rather than based on the location in solution space (AKA
genetically). Yet the underlying principle is always the same: penalise
duplication in order to promote diversity. This lesson can be carried over to
our theory exploration context, where a theorem should be considered less
interesting if it is ``close'' to others which have been found.

In a similar way, we can bias our search procedure, rather than our fitness
function, towards diversity. The search procedure in population-based
evolutionary algorithms consists of \emph{selecting} one or more individuals
from the population, e.g. via truncation (select the best $n$ individuals,
discarding the rest); then \emph{transforming} the selected individuals,
e.g. via mutation and crossover, to obtain new solutions.

Traditional selection methods are biased towards high fitness individuals (this
is especially clear for truncation). Alternative schemes have been proposed
which favour diversity \emph{at the expense of} fitness. For example, the
fitness uniform selection scheme (FUSS) \cite{hutter2002fitness} selects a
target fitness $f_t$ uniformly from the interval
$\left[ f_{min}, f_{max} \right]$ between the highest and lowest of the
population. An individual $s$ is then selected with fitness closest to $f_t$,
i.e. $s = \argmin\limits_{x} \lvert f(x) - f_t \rvert$

In this way, the fitness function $f$ is used to assign comparable quantities to
solutions, but it is not treated as the objective; instead, the implicit
objective is to maintain a diverse population, with individuals spread out
uniformly in fitness space. This approach seems useful for informing our work in
theory exploration, as it supports search criteria which \emph{describe}
solutions, but which we may not want to \emph{optimise}. As a simple example, we
might distinguish different forms of theorem by measuring how balanced their
syntax trees are (-1 for left-leaning, +1 for right leaning, 0 for balanced);
but it would be senseless to \emph{maximise} how far they lean.

Once we begin this process of augmenting fitness functions, or abandoning their
use as objectives, an obvious question arises: what happens if our new function
contains nothing of the original? This kind of pure exploration scenario leads
to a variety of ideas for \emph{instrinsic} fitness, such as novelty
\cite{lehman2011abandoning}, which can lead to learning useful ``stepping
stones'' even in objective-driven domains. Such intrinsic notions of fitness are
direct analogues of the interestingness measures we seek for theory exploration.

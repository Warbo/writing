\chapter{Similarity and Clustering}
\label{sec:recurrentclustering}

To improve the performance of theory exploration tools, we need to avoid
brute-force search in favour of a more informed strategy. This, in turn,
requires more information about the terms in the given signature. One
generally available source of information is a term's definition.

Unfortunately these definitions are over-complicated due to the relatively large
syntax of Haskell, which includes many convenience features (``syntactic
sugar'') and alternative ways to write essentially the same thing (e.g.
\hs{let} vs \hs{where}, \hs{if} vs \hs{case}, \hs{f x = ...} vs
\hs{f = \x -> ...}, etc.). This obscures the simplicity of the lambda calculus
underlying the language, and more importantly it makes syntactic analysis less
useful for our purposes by making distinctions between semantically identical
terms (which will behave identically under theory exploration). For this reason
we perform our analyses on the \emph{GHC Core} language instead.

\section{GHC Core}\label{sec:core}
\label{sec:ghccore}

\begin{figure}
  \begin{equation*}
    \begin{split}
      expr\    \rightarrow\ & \CVar\ id                          \\
                         |\ & \CLit\ literal                     \\
                         |\ & \CApp\ expr\ expr                  \\
                         |\ & \CLam\ \mathcal{L}\ expr           \\
                         |\ & \CLet\ bind\ expr                  \\
                         |\ & \CCase\ expr\ \mathcal{L}\ [alt]   \\
                         |\ & \CType                             \\
      id\      \rightarrow\ & \CLocal\       \mathcal{L}         \\
                         |\ & \CGlobal\      \mathcal{G}         \\
                         |\ & \CConstructor\ \mathcal{D}         \\
      literal\ \rightarrow\ & \CLitNum\ \mathcal{N}              \\
                         |\ & \CLitStr\ \mathcal{S}              \\
      alt\     \rightarrow\ & \CAlt\ altcon\ expr\ [\mathcal{L}] \\
      altcon\  \rightarrow\ & \CDataAlt\ \mathcal{D}             \\
                         |\ & \CLitAlt\ literal                  \\
                         |\ & \CDefault                          \\
      bind\    \rightarrow\ & \CNonRec\ binder                   \\
                         |\ & \CRec\ [binder]                    \\
      binder   \rightarrow\ & \CBind\ \mathcal{L}\ expr
    \end{split}
  \end{equation*}
  Where:
  \begin{tabular}[t]{l @{ $=$ } l}
    $\mathcal{S}$ & string literals    \\
    $\mathcal{N}$ & numeric literals   \\
    $\mathcal{L}$ & local identifiers  \\
    $\mathcal{G}$ & global identifiers \\
    $\mathcal{D}$ & constructor identifiers
  \end{tabular}

  \caption{Simplified syntax of GHC Core in BNF style. $[]$ and $(,)$ denote
    repetition and grouping, respectively. Other forms of literal, like machine
    words, are treated in a similar way and have been omitted for brevity.}
  \label{fig:coresyntax}
\end{figure}

GHC Core is an intermediate language of the \textsc{GHC} compiler based on
\fc{}, and described in detail in~\cite[Appendix C]{sulzmann2007system}. Core
contains explicit type annotations and coercions, which we omit as they have no
effect on runtime behaviour (they have no ``computational content''). The
resulting sub-set of Core\footnote{As of GHC version 7.10.2.} is shown in
Figure~\ref{fig:coresyntax}.

\begin{figure}
  \begin{subfigure}[haskell]{\textwidth}
    \underline{\texttt{Haskell definitions}}
    \begin{haskell}
plus    Z  y = y
plus (S x) y = S      (plus x y)

mult    Z  y = Z
mult (S x) y = plus y (mult x y)

odd     Z  = False
odd  (S n) = even n

even    Z  = True
even (S n) = odd n
    \end{haskell}
    \smallskip
  \end{subfigure}
  \begin{subfigure}[plus]{\textwidth}
    \begin{small}
      \underline{\texttt{plus}}
      \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Local "y")))
                       (Alt (DataAlt "S") (App (Var (Constructor "S"))
                                               (App (App (Var (Global "plus"))
                                                         (Var (Local  "x")))
                                                    (Var (Local "y"))))
                                          "x")))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[mult]{\textwidth}
    \begin{small}
      \underline{\texttt{mult}}
      \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Constructor "Z")))
                       (Alt (DataAlt "S") (App (App (Var (Global "plus"))
                                                    (Var (Local  "y")))
                                               (App (App (Var (Global "mult"))
                                                         (Var (Local  "x")))
                                                    (Var (Local  "y"))))
                                          "x")))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[odd]{\textwidth}
    \begin{small}
      \underline{\texttt{odd}}
      \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "False")))
              (Alt (DataAlt "S") (App (Var (Global "even"))
                                      (Var (Local  "n")))
                                 "n"))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[even]{\textwidth}
    \begin{small}
      \underline{\texttt{even}}
      \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "True")))
              (Alt (DataAlt "S") (App (Var (Global "odd"))
                                      (Var (Local  "n")))
                                 "n"))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \caption{Example Haskell functions and their translations into the Core syntax
    of Figure \ref{fig:coresyntax}. Notice the introduction of explicit
    $\lambda$ abstractions (\texttt{Lam}) and the use of \texttt{Case} to
    represent piecewise definitions. Fresh variables are chosen arbitrarily
    as \texttt{"a"}, \texttt{"b"}, etc.}
  \label{fig:coreexample}
\end{figure}

GHC's translation from Haskell to Core is routine: Figure~\ref{fig:coreexample}
shows the Core representation of some simple functions. Although the Core is
more verbose, we can see that similar structure in the Haskell definitions gives
rise to similar structure in the Core; for example, the definitions of \hs{odd}
and \hs{even} are identical in both languages, except for the particular
identifiers used. It is this close correspondence which allows us to analyse
Core expressions in place of their more complicated Haskell source.

For simplicity we also avoid using constructors directly, choosing instead to
wrap them with normal functions (e.g. \hs{s x = S x}, known as
$\eta$-expansion), since this eliminates another distinction (constructors vs
functions) and also reduces cross-language differences (e.g. Isabelle treats
constructors in a different way to Haskell).

\section{Feature Extraction}

Raw syntax trees, even in Core form, are unsuitable for many forms of analysis
due to their recursive structure, use of discrete labels and heavy reliance on
references. These are unsuitable for many machine learning algorithms, which
instead require a fixed-size vector of continuous numbers known as a
\emph{feature vector}.

The process of turning raw data into feature vectors is called \emph{feature
  extraction}, and we adapt the methodology of \emph{recurrent clustering}
proposed in~\cite{DBLP:journals/corr/HerasK14,heras2013proof}, since this
handles recursive tree structures, resolves references and produces numeric
vectors. We suggest a new recurrent clustering and feature extraction algorithm
for Haskell (shown in Algorithm \ref{alg:recurrent}) and compare its similarity
and differences to the existing approaches of ML4PG (Coq) and ACL2(ml) (ACL2).

We describe our algorithm in two stages: the first transforms the nested
structure of expressions into a flat vector representation; the second converts
the discrete symbols of Core syntax into features (real numbers), which we will
denote using the function $\phi$. Before introducing our algorithm, we briefly
describe \emph{clustering}, and its use in \emph{recurrent clustering}.

\subsection{Clustering}

Clustering is an unsupervised machine learning task for grouping $n$ data points
using a similarity metric. There are many variations on this theme, but in our
case we make the following choices:

\begin{itemize}
\item For simplicity, we use the ``rule of thumb'' given in
  \cite[pp. 365]{mardia1979multivariate} to fix the number of clusters at
  $k = \lceil \sqrt{\frac{n}{2}} \rceil$.
\item Data points will be $d$-dimensional feature vectors, as defined above.
\item We will use euclidean distance (denoted $e$) as our similarity metric.
\item We will use \emph{k-means} clustering, implemented by Lloyd's algorithm
  \cite{lloyd1982least}.
\end{itemize}

This is a standard setup, supported by off-the-shelf tools; in particular we use
the \hPackage{k-means} Haskell package.

Since k-means is iterative, we will use function notation to denote time steps,
so $x(t)$ denotes the value of $x$ at time $t$. We denote the clusters as $C^1$
to $C^k$. As the name suggests, k-means uses the mean value of each cluster,
which we denote as $\vect{m}^1$ to $\vect{m}^k$, hence:

\begin{equation*}
  m^i_j(t) = \mean{C^i_j}(t) = \frac{\sum_{\vect{x} \in C^i(t)} x_j}{|C^i(t)|} \quad \text{for $t > 0$}
\end{equation*}

Before k-means starts, we must choose \emph{seed} values for
$\vect{m}^i(0)$. Many methods have been proposed for choosing these values
\cite{arthur2007k}. For simplicity, we will choose values randomly from our
data set $S$; this is known as the Forgy method.

The elements of each cluster $C^i(t)$ are those data points closest to the mean
value at the previous time step:

\begin{equation*}
  C^i(t) = \{ \vect{x} \in S \mid i = \argmin\limits_j e(\vect{x}, \vect{m}^j(t-1)) \} \quad \text{for $t > 0$}
\end{equation*}

As $t$ increases, the clusters $C^i$ move from their initial location around the
``seeds'', to converge on a local minimum of the ``within-cluster sum of squared
error'' objective:

\begin{equation} \label{eq:kmeansobjective}
  \argmin\limits_C \sum_{i=1}^k \sum_{\vect{x} \in C^i} e(\vect{x}, \vect{m}^i)^2
\end{equation}

We chose k-means because it is a standard approach and we are more concerned
with the application of feature extraction to Haskell and its use in theory
exploration, rather than precise tuning of learning algorithms. There are many
other clustering algorithms we could use, such as \emph{expectation
  maximisation}~\footnote{In fact, k-means is very similar to
  expectation-maximisation, as it alternates between an \emph{expectation step}
  (finding the mean value of each cluster) and a \emph{maximisation step}
  (assigning points to the cluster they're most similar to; or alternatively,
  \emph{minimising} the distance of each point to the centre of its cluster, as
  per equation \ref{eq:kmeansobjective}).}, but experiments with ML4PG have
shown little difference in their results; in effect, the quality of the features
is the bottleneck to learning, so there is no reason to avoid a fast algorithm
like k-means.

In any case there are many conservative improvements to the standard k-means
algorithm, which could be applied to our setup. For example, a more efficient
approach like \emph{yinyang k-means}~\cite{conf/icml/DingZSMM15} could make
larger input sizes more practical to cluster, especially since recurrent
clustering invokes k-means many times. The \emph{k-means++}
approach~\cite{arthur2007k, bahmani2012scalable} can be used to more carefully
select the ``seed'' values for the first timestep, and the \emph{x-means}
algorithm~\cite{pelleg2000x} is able to estimate how many clusters to use.

\subsection{Recurrent Clustering}

\emph{Recurrent clustering} has been implemented in the
ML4PG tool for analysing Coq proofs~\cite{journals/corr/abs-1212-3618} and
ACL2(ml) for ACL2~\cite{heras2013proof}. Whilst both transform syntax trees into
matrices, the algorithm of ML4PG most closely resembles ours as it assigns tokens directly to matrix elements. In contrast,
the matrices produced by ACL2(ml) \emph{summarise} information about the tree;
for example, one column counts the number of variables appearing at each tree
level, others count the number of function symbols which are nullary, unary,
binary, etc. Whilst it may be interesting to contrast our current algorithm with
an alternative based on that of ACL2(ml), it is unclear how such summaries could
be extended to include types, which seems the next logical step for our
approach. The ML4PG algorithm extends trivially, by using (term, type) pairs
instead of just terms.

The way we \emph{use} our clusters to inform theory exploration is actually more
similar to that of ACL2(ml) than ML4PG. ML4PG can either present clusters to the
user for inspection, or produce automata for recreating proofs. In ACL2(ml), the
clusters are used to restrict the search space of a proof search, much like we
restrict the scope of theory exploration.

ACL2(ml) reasons by analogy: finding theorem statements which are similar to the
current goal, and attempting to prove the goal in a similar way. In particular,
the lemmas used to prove a theorem are mutated by substituting symbols for those
which appear in the same cluster. For example, if \texttt{plus} and
\texttt{multiply} are clustered together, and we are trying to prove a goal
involving \texttt{multiply}, then ACL2(ml) might consider an existing theorem
involving \texttt{plus}. The lemmas used to prove that theorem will be mutated,
for example replacing occurrences of \texttt{plus} with \texttt{mult}, in an
attempt to prove the goal.

\iffalse
% TODO: Move this to the end of the chapter?
Whilst we do not currently reason by analogy, this is an interesting area for
future work in theory exploration: given a set of theorems relating particular
terms, we might form conjectures regarding similar terms found through
clustering.
\fi

\subsection{Rose Trees}\label{sec:rosetree}

The $toTree$ function shown below transforms Core expressions, described in
\S~\ref{sec:core}, to rose trees. We follow the presentation
in~\cite{blundell2012bayesian} and define rose trees recursively as follows: $T$
is a rose tree if $T = (f, T_1, \dots, T_{n_T})$, where $f \in \mathbb{R}$ and
$T_i$ are rose trees.

$T_i$ are the \emph{sub-trees} of $T$ and $f$ is the \emph{feature at} $T$.

$n_T$ may differ for each (sub-) tree; trees where $n_T = 0$ are \emph{leaves}.

The recursive definition is mostly routine; each repeated element (shown as
$\dots$) has an example to indicate their handling, e.g. for $\CRec$ we apply
$toTree$ to each $e_i$. We ignore values of $\mathcal{D}$, since constructors
have no internal structure for us to compare. We also ignore values from
$\mathcal{S}$ and $\mathcal{N}$ as it simplifies our later definition of $\phi$,
and we conjecture that the effect of such ``magic values'' on clustering real
code is low.

\begin{figure}
  \begin{align*}\label{fig:totree}
    toTree(e) &=
    \begin{cases}
      (\feature{\CVar},     toTree(e_1))                                 & \text{if $e = \CVar\ e_1$} \\
      (\feature{\CLit},     toTree(e_1))                                 & \text{if $e = \CLit\ e_1$} \\
      (\feature{\CApp},     toTree(e_1), toTree(e_2))                    & \text{if $e = \CApp\ e_1\ e_2$} \\
      (\feature{\CLam},     toTree(e_1))                                 & \text{if $e = \CLam\ l_1\ e_1$} \\
      (\feature{\CLet},     toTree(e_1), toTree(e_2))                    & \text{if $e = \CLet\ e_1\ e_2$} \\
      (\feature{\CCase},    toTree(e_1), toTree(a_1), \dots)             & \text{if $e = \CCase\ e_1\ l_1\ a_1\ \dots$} \\
      (\feature{\CType})                                                & \text{if $e = \CType$} \\
      (\feature{\CLocal},   (\feature{l_1}))                            & \text{if $e = \CLocal\ l_1$} \\
      (\feature{\CGlobal},  (\feature{g_1}))                            & \text{if $e = \CGlobal\ g_1$} \\
      (\feature{\CConstructor})                                         & \text{if $e = \CConstructor\ d_1$} \\
      (\feature{\CLitNum})                                              & \text{if $e = \CLitNum\ n_1$} \\
      (\feature{\CLitStr})                                              & \text{if $e = \CLitStr\ s_1$} \\
      (\feature{\CAlt},     toTree(e_1), toTree(e_2))                   & \text{if $e = \CAlt\ e_1\ e_2\ l_1\ \dots$}  \\
      (\feature{\CDataAlt})                                             & \text{if $e = \CDataAlt\ g_1$}  \\
      (\feature{\CLitAlt},  toTree(e_1))                                & \text{if $e = \CLitAlt\ e_1$}  \\
      (\feature{\CDefault})                                             & \text{if $e = \CDefault$}  \\
      (\feature{\CNonRec},  toTree(e_1))                                & \text{if $e = \CNonRec\ e_1$}  \\
      (\feature{\CRec},     toTree(e_1), \dots)                         & \text{if $e = \CRec\ e_1\ \dots$} \\
      (\feature{\CBind},    toTree(e_1))                                & \text{if $e = \CBind\ l_1\ e_1$}
    \end{cases}
  \end{align*}
\end{figure}

\subsubsection{Expressions to Vectors}
\label{sec:expressionstovectors}

Our recurrent clustering algorithm is based on k-means clustering, which
considers the elements of a feature vector to be \emph{orthogonal}. Hence we
must ensure that similar expressions not only give rise to similar numerical
values, but crucially that these values appear \emph{at the same position} in
the feature vectors. Since different patterns of nesting can alter the ``shape''
of expressions, simple traversals (breadth-first, depth-first, post-order, etc.)
may cause features from equivalent sub-expressions to be mis-aligned. For
example, consider the following expressions, which represent pattern-match
clauses with different patterns but the same body (\hs{\vlocal{y}}):

\begin{equation}\label{eq:xy}
  \begin{array}{r@{}l@{}l@{}}
    X\ &=\ \CAlt\ (\CDataAlt\ \id{C})\ & (\vlocal{y}) \\
    Y\ &=\ \CAlt\ \CDefault\           & (\vlocal{y})
  \end{array}
\end{equation}

If we traverse these expressions in breadth-first order, converting each token
to a feature using $\phi$ and padding to the same length with $0$, we would get
the following feature vectors:

\begin{small}
  \begin{equation}
    \begin{array}{r@{}l@{}l@{}l@{}l@{}l@{}l@{}l}
      breadthFirst(X)\ &=\ (\feature{\CAlt},\ &\feature{\CDataAlt},\ &\feature{\CVar},\ &\feature{\id{C}},\ &\feature{\CLocal},\ &\feature{\id{y}} &) \\
      breadthFirst(Y)\ &=\ (\feature{\CAlt},\ &\feature{\CDefault},\ &\feature{\CVar},\ &\feature{\CLocal},\ &\feature{\id{y}},\ &0 &)
    \end{array}
  \end{equation}
\end{small}

Here the features corresponding to the common sub-expression $\CLocal\ \id{y}$
are misaligned, such that only $\frac{1}{3}$ of features are guaranteed to match
(others may match by coincidence, depending on $\phi$). These feature vectors
might be deemed very dissimilar during clustering, despite the intuitive
similarity of the expressions $X$ and $Y$ from which they derive.

If we were to align these features optimally, by padding the fourth column
rather than the sixth, then $\frac{2}{3}$ of features would be guaranteed to
match, making the similarity of the vectors more closely match our intuition and
depend less on coincidence.

The method we use to ``flatten'' expressions, described below, is a variation of
breadth-first traversal which pads each level of nesting to a fixed size $c$
(for \emph{columns}). This doesn't guarantee alignment, but it does prevent
mis-alignment from accumulating across different levels of nesting. Our method
would align these features into the following vectors, if $c = 2$: \footnote{In
  fact, the constructor identifier $\id{C}$ would not appear in the vector, but
  this example is still accurate in terms of laying out the features as given.}

\begin{small}
  \begin{equation}\label{eq:flattened}
    \begin{array}{r@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l@{}l}
      featureVec(X)\ &=\ (\feature{\CAlt},\ &0,\ &\feature{\CDataAlt},\ &\feature{\CVar},\ &\feature{\id{C}},\  &\feature{\CLocal},\ &\feature{\id{y}},\ &0 &) \\
      featureVec(Y)\ &=\ (\feature{\CAlt},\ &0,\ &\feature{\CDefault},\ &\feature{\CVar},\ &\feature{\CLocal},\ &0,\                 &\feature{\id{y}},\ &0 &)
    \end{array}
  \end{equation}
\end{small}

Here $\frac{1}{2}$ of the original 6 features align, which is more than
$breadthFirst$ but not optimal. Both vectors have also been padded by an extra 2
zeros compared to $breadthFirst$; raising their alignment to $\frac{5}{8}$.

To perform this flattening we first transform the nested tokens of an expression
into a \emph{rose tree} of features. Intuitively, these are simply an
s-expression representation of the Core syntax, with the feature extraction
function $\phi$ mapped over the labels. The full transformation is given in
Appendix \ref{sec:rosetree}, and Figure \ref{fig:rosetreeexample} shows the
result for the \hs{odd} function.

\begin{figure}
  \centering
  \begin{scriptsize}
      \Tree[ .$\feature{\CLam}$
                $\feature{\id{a}}$
                [ .$\feature{\CCase}$
                     [ .$\feature{\CVar}$
                          [ .$\feature{\CLocal}$
                               $\feature{\id{a}}$ ]]
                     $\feature{\id{b}}$
                     [ .$\feature{\CAlt}$
                          $\feature{\CDataAlt}$
                          [ .$\feature{\CVar}$
                               $\feature{\CConstructor}$ ]]
                     [ .$\feature{\CAlt}$
                          $\feature{\CDataAlt}$
                          [ .$\feature{\CApp}$
                               [ .$\feature{\CVar}$
                                    [ .$\feature{\CGlobal}$
                                         $\feature{\id{even}}$ ]]
                               [ .$\feature{\CVar}$
                                    [ .$\feature{\CLocal}$
                                         $\feature{\id{n}}$ ]]]
                          $\feature{\id{n}}$ ]]]
  \end{scriptsize}
  \caption[Rose tree for odd]{\label{fig:rosetreeexample} Rose tree for the
    expression \hs{odd} from Figure \ref{fig:coreexample}. Each (sub-) rose tree
    is rendered with its feature at the node and sub-trees beneath.}
\end{figure}

\begin{figure}
    \begin{equation*}
      \begin{bmatrix}
        \feature{\CLam}      & 0                       & 0                 & 0                   & 0               & 0                \\
        \feature{\id{a}}     & \feature{\CCase}        & 0                 & 0                   & 0               & 0                \\
        \feature{\CVar}      & \feature{\id{b}}        & \feature{\CAlt}   & \feature{\CAlt}     & 0               & 0                \\
        \feature{\CLocal}    & \feature{\CDataAlt}     & \feature{\CVar}   & \feature{\CDataAlt} & \feature{\CApp} & \feature{\id{n}} \\
        \feature{\id{a}}     & \feature{\CConstructor} & \feature{\CVar}   & \feature{\CVar}     & 0               & 0                \\
        \feature{\CGlobal}   & \feature{\CLocal}       & 0                 & 0                   & 0               & 0                \\
        \feature{\id{even}}  & \feature{\id{n}}        & 0                 & 0                   & 0               & 0
      \end{bmatrix}
    \end{equation*}
    \caption{Matrix generated from Figure \ref{fig:rosetreeexample}, padded to 6 columns. Each level of nesting in the tree corresponds to a row in the matrix.}
    \label{fig:matrixexample}
\end{figure}

These rose trees are then turned into matrices, as shown in Figure
\ref{fig:matrixexample}. Each row $i$ of the matrix contains the features at
depth $i$ in the rose tree, read left-to-right, followed by any required
padding. These matrices are then either truncated, or padded with rows (on the
bottom) or columns (on the right) of zeros, to fit a fixed size $r \times c$.

\begin{sloppypar}
  Finally, matrices are turned into vectors by concatenating the rows from top
  to bottom, hence Figure \ref{fig:matrixexample} will produce a vector
  beginning
  $(\feature{\CLam} ,
    0               ,
    0               ,
    0               ,
    0               ,
    0               ,
    \feature{\id{a}},
    \feature{\CCase},
    0               ,
    0               ,
    0               ,
    0               ,
    \feature{\CVar} ,
    \feature{\id{b}},
    \feature{\CAlt} ,
    \feature{\CAlt} ,
    \dots$.
\end{sloppypar}

\subsubsection{Symbols to Features}
\label{sec:symbolstofeatures}

We now define the function $\phi$, which turns terminal symbols of Core syntax
into features (real numbers). For known language features, such as
$\feature{\CLam}$ and $\feature{\CCase}$, we can enumerate the possibilities and
assign a value to each, in a similar way to \cite{DBLP:journals/corr/HerasK14}
in Coq. We use a constant $\alpha$ to separate these values from those of other
tokens (e.g. identifiers), but the order is essentially arbitrary: \footnote{In
  \cite{DBLP:journals/corr/HerasK14}, ``similar'' Gallina tokens like \coq{fix}
  and \coq{cofix} are grouped together to reduce redundancy; we do not group
  tokens, but we do put ``similar'' tokens close together, such as \CLocal\ and
  \CGlobal.}

\begin{equation} \label{eq:feature}
  \begin{aligned}
    \feature{\CAlt}          &= \alpha      &
    \feature{\CDataAlt}      &= \alpha + 1  &
    \feature{\CLitAlt}       &= \alpha + 2  \\
    \feature{\CDefault}      &= \alpha + 3  &
    \feature{\CNonRec}       &= \alpha + 4  &
    \feature{\CRec}          &= \alpha + 5  \\
    \feature{\CBind}         &= \alpha + 6  &
    \feature{\CLet}          &= \alpha + 7  &
    \feature{\CCase}         &= \alpha + 8  \\
    \feature{\CLocal}        &= \alpha + 9  &
    \feature{\CGlobal}       &= \alpha + 10 &
    \feature{\CConstructor}  &= \alpha + 11 \\
    \feature{\CVar}          &= \alpha + 12 &
    \feature{\CLam}          &= \alpha + 13 &
    \feature{\CApp}          &= \alpha + 14 \\
    \feature{\CType}         &= \alpha + 15 &
    \feature{\CLit}          &= \alpha + 16 &
    \feature{\CLitNum}       &= \alpha + 17 \\
    \feature{\CLitStr}       &= \alpha + 18
  \end{aligned}
\end{equation}

To encode \emph{local} identifiers $\mathcal{L}$ we would like a quantity which
gives equal values for $\alpha$-equivalent expressions (i.e. renaming an
identifier shouldn't affect the feature). To do this, we use the \emph{de Bruijn
  index} of the identifier \cite{de1972lambda}, denoted $i_l$:

\begin{equation} \label{eq:localfeature}
  \feature{l} = i_l + 2 \alpha \quad \text{if $l \in \mathcal{L}$}
\end{equation}

We again use $\alpha$ to separate these features from those of other constructs.

We discard the contents of literals and constructor identifiers when converting
to rose trees, so the only remaining case is global identifiers
$\mathcal{G}$. Since these are declared \emph{outside} the body of an
expression, we cannot perform the same indexing trick as for local
identifiers. We also cannot directly encode the form of the identifiers,
e.g. using a scheme like G{\"o}del numbering, since this is essentially
arbitrary and has no effect on their semantic meaning (referencing other
expressions).

Instead, we use the approach taken in the latest versions of ML4PG and encode
global identifiers \emph{indirectly}, by looking up the expressions which they
\emph{reference}:

\begin{equation} \label{eq:globalfeature}
  \feature{g \in \mathcal{G}} =
    \begin{cases}
      i + 3 \alpha \quad & \text{if $g \in C_i$} \\
      f_{recursion}         & \text{otherwise}
    \end{cases}
\end{equation}

Where $C_i$ are our clusters (in some arbitrary order). This is where the
recurrent nature of the algorithm appears: to determine the contents of $C_i$,
we must perform k-means clustering \emph{during} feature extraction; yet that
clustering step, in turn, requires that we perform feature extraction.

For this recursive process to be well-founded, we perform a topological sort on
declarations based on their dependencies (the expressions they reference). In
this way, we can avoid looking up expressions which haven't been clustered
yet. To handle mutual recursion we use Tarjan's algorithm \cite{tarjan1972depth}
to produce a sorted list of \emph{strongly connected components} (SCCs), where
each SCC is a mutually-recursive sub-set of the declarations. If an identifier
cannot be found amongst those clustered so far, it must appear in the same SCC
as the expression we are processing; hence we give that identifier the constant
feature value $f_{recursion}$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require List $d$ contains SCCs of (identifier, expression) pairs, in
    dependency order.
    \Procedure{RecurrentCluster}{}
      \State $\vect{C} \gets []$
      \State $DB \gets \varnothing$
      \ForAll{$scc$ \textbf{in} $d$}
        \State $DB \gets DB \cup \{(i, featureVec(e)) \mid (i, e) \in scc\}$
        \State $\vect{C} \gets kMeans(DB)$
      \EndFor
      \Return $\vect{C}$
    \EndProcedure
  \end{algorithmic}
  \caption{Recurrent clustering of Core expressions.}
  \label{alg:recurrent}
\end{algorithm}

By working through the sorted list of SCCs, storing the features of each
top-level expression as they are calculated, our algorithm can be computed
\emph{iteratively} rather than recursively, as shown in Algorithm
\ref{alg:recurrent}.

As an example of this recurrent process, we can consider the Peano arithmetic
functions from Figure \ref{fig:coreexample}. A valid topological ordering is
given in Figure \ref{fig:sccexample}, which can be our value for $d$ (eliding
Core expressions to save space):

\begin{equation}
  d = [\{(\hs{plus}, \dots)\},
       \{(\hs{odd}, \dots), (\hs{even}, \dots)\},
       \{(\hs{mult}, \dots)\}]
\end{equation}

We can then trace the execution of Algorithm \ref{alg:recurrent} as follows:

\begin{itemize}
\item The first iteration through \textsc{RecurrentCluster}'s loop will set
  $scc \gets \{(\hs{plus}, \dots)\}$.
\item With $i = \hs{plus}$ and $e$ as its Core expression, calculating
  $featureVec(e)$ is straightforward; the recursive call $\feature{\hs{plus}}$
  will become $f_{recursion}$ (since \hs{plus} doesn't appear in $\vect{C}$).
\item The call to $kMeans$ will produce $\vect{C} \gets [\{\hs{plus}\}]$, i.e. a
  single cluster containing \hs{plus}.
\item The next iteration will set
  $scc \gets \{(\hs{odd}, \dots), (\hs{even}, \dots)\}$.
\item With $i = \hs{odd}$ and $e$ as its Core expression, the call to \hs{even}
  will result in $f_{recursion}$.
\item Likewise for the call to $\hs{even}$ when $i = \hs{odd}$.
\item Since the feature vectors for \hs{odd} and \hs{even} will be identical,
  $kMeans$ will put them in the same cluster. To avoid the degenerate case of a
  single cluster, for this example we will assume that $k = 2$; in which case
  the other cluster must contain \hs{plus}. Their order is arbitrary, so one
  possibility is $\vect{C} = [\{\hs{odd}, \hs{even}\}, \{\hs{plus}\}]$.
\item Finally \hs{mult} will be clustered. The recursive call will become
  $f_{recursion}$ whilst the call to \hs{plus} will become $2 + 3 \alpha$, since
  $\hs{plus} \in C_2$.
\item \begin{sloppypar}Again assuming that $k = 2$, the resulting clusters will
    be
    \mbox{$\vect{C} \gets [\{\hs{odd}, \hs{even}\}, \{\hs{plus},
      \hs{mult}\}]$}.\end{sloppypar}
\end{itemize}

Even in this very simple example we can see a few features of our algorithm
emerge. For example, \hs{odd} and \hs{even} will always appear in the same
cluster, since they only differ in their choice of constructor names (which are
discarded by $toTree$) and recursive calls (which are replaced by
$f_{recursion}$). A more extensive investigation of these features requires a
concrete implementation, in particular to pin down values for the parameters
such as $r$, $c$, $f_{recursion}$, $\alpha$ and so on.

\section{Implementation}
\label{sec:implementation}

We provide an implementation of our recurrent clustering algorithm in a tool
called \mlforhs{}. We obtain Core ASTs from Haskell packages using a plugin
for the GHC compiler, which emits a serialised version of each Core definition
as it is being compiled. This approach is more robust than, for example, parsing
source files, since it avoids the complications of preprocessors and language
extensions altering the syntax.

A post-processing stage determines which Core definitions can be used by
\quickspec{}, based on their type, visibility (whether they are encapsulated inside
their module or visible to importers), etc. For each definition, we simply use
the GHCi interpreter to check if calling \quickspec{} with that argument is
well-typed. This information, along with type signatures, arity, etc. are stored
alongside the Core definitions in JSON format.

The definitions are then sorted topologically, based on the non-local
identifiers appearing in their ASTs, and feature vectors are constructed using a
Haskell implementation of the approach described in \S
\ref{sec:recurrentclustering}. Since the features associated with each
identifier may vary between iterations (as the clusters change), we leave the
raw identifiers in the vector so their features can be extracted in the correct
context.

We implement Algorithm \ref{alg:recurrent} as a simple shell script, which
invokes Weka for clustering and associates each Core definition with a cluster
number. These numbers are used to finish the deferred feature extraction of
identifiers, the resulting feature vectors are clustered, and the process is
repeated until all SCCs have been processed.

Once the recurrent clustering is complete, we generate a string of Haskell code
for each of the final clusters, which will invoke \quickspec{} with a suitable
signature. This involves:

\begin{itemize}
\item{Monomorphising}: If a value has polymorphic type, e.g. \hs{equal :: forall
    t. t -> t -> Bool}, we must choose a concrete representation to use (in this
  case for \hs{t}), in order to know which random generator to use. We take the
  approach used in \quickcheck{} by attempting to instantiate all type variables to
  \hs{Integer}. Any values where this is invalid, such as those with
  incompatible class constraints (e.g. \hs{forall t. IsString t => t}, where
  \hs{Integer} does not implement \hs{IsString}) will not be included in the
  signature (this is checked at the AST post-processing stage).

\item{Qualifying}: All names are \emph{qualified} (prefixed by their module's
  name), to avoid most ambiguity. There is still the possibility that multiple
  packages will declare modules of the same name, although this is rare as it
  causes problems for any Haskell programmer trying to use those modules. In
  such cases the exploration process simply aborts.

\item{Appending variables}: Once a \quickspec{} theory has been defined containing
  all of the given terms, we inspect the types it references and append three
  variables for each to the theory (enough to discover laws such as
  associativity, but not too many to overflow the limit of \quickspec{}'s exhaustive
  search).

\item{Sandboxing}: One difficulty with Haskell's packaging infrastructure is
  that all required packages and modules must be provided up-front, usually by
  specification in a Cabal file. Since \mlspec{} builds signatures
  \emph{dynamically}, depending on the cluster information it is given, we do
  not know what packages it may need. To work around this problem, we evaluate
  these strings of Haskell using a custom library called \texttt{nix-eval}. This
  uses the Nix package manager to obtain all of the required packages and make
  them available to GHC, and is described further in
  Appendix~\ref{sec:nix-eval}.

\end{itemize}

The equations resulting from evaluating these strings are collected and
outputted as JSON values, to ease further processing (e.g. displaying in some
form, sending to a theorem prover, etc.).

\section{Evaluation}
\label{sec:evaluation}

% TODO: Move recurrent clustering to its own chapter, with this as its
% evaluation?
We have applied our recurrent clustering algorithm to several scenarios, with
mixed results. A major difficulty in evaluating these clusters is that we have
no ``ground truth'', i.e. there is no objectively correct way to compare
expressions. Instead, we provide a qualitative overview of the more interesting
characteristics.

As a simple example, we clustered (Haskell equivalents of) the running examples
used to present ACL2(ml) \cite{heras2013proof}, shown in Figure
\ref{fig:lisp}. These include tail-recursive and non-tail-recursive
implementations of several functions. We expect those with similar
\emph{structure} to be clustered together, rather than those which implement the
same function. The results are shown in Figure \ref{fig:haskellcluster}, where
we can see the ``\hs{Tail}'' functions clearly distinguished, with little
distinction between the tail recursive and na\"{\i}ve implementations.

Next we tested whether these same functions would be clustered together when
mixed with seemingly-unrelated functions, in this case 207 functions from
Haskell's \hs{text} library. In fact, the \hs{helperFib} and \hs{fibTail}
functions appeared together in a separate cluster from the rest. This was
unexpected, with no obvious semantic connection between these two functions and
the others in their cluster (although most are recursive, due to the nature of
the \hs{text} library).

\begin{figure}
  \begin{common-lisp}
(defun fact (n)
  (if (zp n) 1 (* n (fact (- n 1)))))

(defun helper-fact (n a)
  (if (zp n) a (helper-fact (- n 1) (* a n))))

(defun fact-tail (n)
  (helper-fact n 1))

(defun power (n)
  (if (zp n) 1 (* 2 (power (- n 1)))))

(defun helper-power (n a)
  (if (zp n) a (helper-power (- n 1) (+ a a))))

(defun power-tail (n)
  (helper-power n 1))

(defun fib (n)
  (if (zp n)
      0
      (if (equal n 1)
          1
          (+ (fib (- n 1)) (fib (- n 2))))))

(defun helper-fib (n j k)
  (if (zp n)
      j
      (if (equal n 1)
          k
          (helper-fib (- n 1) k (+ j k)))))

(defun fib-tail (n)
  (helper-fib n 0 1))
  \end{common-lisp}
  \caption{Common Lisp functions, both tail-recursive and non-tail-recursive.}
  \label{fig:lisp}
\end{figure}

\begin{figure}
  \begin{haskell}
fact n = if n == 0
            then 1
            else n * fact (n - 1)

helperFact n a = if n == 0
                    then a
                    else helperFact (n - 1) (a * n)

factTail n = helperFact n 1

power n = if n == 0
             then 1
             else 2 * power (n - 1)

helperPower n a = if n == 0
                     then a
                     else helperPower (n - 1) (a + a)

powerTail n = helperPower n 1

fib n = if n == 0
           then 0
           else if n == 1
                   then 1
                   else fib (n - 1) + fib (n - 2)

helperFib n j k = if n == 0
                     then j
                     else if n == 1
                          then k
                          else helperFib (n - 1) k (j + k)

fibTail n = helperFib n 0 1
  \end{haskell}
  \caption{Haskell equivalents of the Common Lisp functions in Figure \ref{fig:lisp}.}
  \label{fig:haskelllisp}
\end{figure}

\begin{figure}
  \centering
    \tikzstyle{block} = [rectangle, draw]
    \tikzstyle{line}  = [draw, -latex']
    \begin{tikzpicture}[node distance=3cm]
      \node [block] (c1) {
        \begin{tikzpicture}[node distance = 1cm]
          \node [block]                    (factTail)  {\hs{factTail}};
          \node [block, below of=factTail] (fibTail)   {\hs{fibTail}};
          \node [block, below of=fibTail]  (powerTail) {\hs{powerTail}};
        \end{tikzpicture}
      };
      \node [block, right of=c1] (c2) {
        \begin{tikzpicture}[node distance = 1cm]
          \node [block]                (fact)       {\hs{fact}};
          \node [block, below of=fact] (helperFact) {\hs{helperFact}};
        \end{tikzpicture}
      };
      \node [block, right of=c2] (c3) {
        \begin{tikzpicture}[node distance = 1cm]
          \node [block]                       (fib)         {\hs{fib}};
          \node [block, below of=fib]         (helperFib)   {\hs{helperFib}};
          \node [block, below of=helperFib]   (helperPower) {\hs{helperPower}};
          \node [block, below of=helperPower] (power)       {\hs{power}};
        \end{tikzpicture}
      };
  \end{tikzpicture}
  \caption{Typical clusters for the functions in Figure \ref{fig:haskelllisp}.}
  \label{fig:haskellcluster}
\end{figure}

We have also applied our recurrent clustering algorithm to a variety of the
most-downloaded packages from \hackage{} (as of 2015-10-30), including
\texttt{text} (as above), \texttt{pandoc}, \texttt{attoparsec},
\texttt{scientific}, \texttt{yesod-core} and \texttt{blaze-html}. Whilst we
expected functions with a similar purpose to appear together, such as the
various reader and writer functions of \hs{pandoc}, there were always a few
exceptions which became separate for reasons which are still unclear.

When clustering the \hs{yesod} Web framework, the clustering did seem to match
our intuitions, in particular since all 15 of Yesod's MIME type identifiers
appeared in the same cluster.

Whilst recurrent clustering has produced results which merit further
investigation, the application to theory exploration has yet to be tested
empirically. This is due to \quickspec{}'s use of \quickcheck{}'s \hs{Arbitrary} type
class to generate random values for instantiating variables. Whilst we can
automatically define \quickspec{} theories and invoke them with \hs{nix-eval}, not
all types have \hs{Arbitrary} instances; those without cannot be given any
variables in our signature, which severely limits the possible combinations
which can be explored. In many cases, no variables can be included at all,
leaving just equations involving constants. This has so far prevented us from
measuring the direct impact on \quickspec{} performance, either directly by
exploring the sub-sets identified through recurrent clustering, or indirectly by
comparing the equations generated by a full brute-force search to our recurrent
clusters: those equations relating terms from different clusters would not be
discovered by our method. It is this ratio of equations found through brute
force to those found after narrowing-down by clusters which is one of our key
objectives to maximise at this stage; until we begin to pursue the
\emph{interestingness} of the properties.

The following less-serious problems were also encountered while applying
\mlforhs{} to \hackage{} packages:

\begin{itemize}
\item Some packages, such as \texttt{warp} and \texttt{conduits}, get no
  declarations to cluster. This is because they make all of their declarations
  privately, e.g. in ``internal'' modules, then use separate modules to export
  the public declarations. GHC's renaming phase makes all references to such
  exports canonical, by pointing them to the private declarations. This forces
  us to ignore such declarations, as \quickspec{} will not be able to access them.

\item Since we do not support type-level entities, we ignore type
  classes. Unfortunately, this also means ignoring any value-level bindings (AKA
  ``methods'') which occur in a type class instance. Instead of being clustered,
  these result in references getting $f_{recursion}$ features. This is
  especially noticable in libraries like \hs{scientific}, where only the
  functions for constructing and destructing numbers in scientific notation are
  clustered; all of the arithmetic is defined in type classes. One difficulty
  with supporting methods is that their namespace in Core is disjoint from that
  of regular Haskell identifiers: a transformation layer would be required,
  along with explicit type annotations to avoid ambiguity.
\end{itemize}

It seems like this recurrent clustering method has promise, although it will
require a more thorough exploration of the parameters to obtain more intuitively
reliable results. These clusters can then be used in several ways to perform
theory exploration; the most na\"{\i}ve way being to explore each cluster as
\quickspec{} signature in its own right. \mlforhs{} already provides this
functionality, although the lack of test generators severely limits what can be
discovered.

As alluded to previously, we also have the opportunity to reason by
\emph{analogy}. Similar to the work on ACL2(ml) \cite{heras2013proof}, we could
produce a general ``scheme'' from each equation we find (either through \quickspec{}
or by data mining test suites); like Isabelle approaches have shown
\cite{Montano-Rivas.McCasland.Dixon.ea:2012}, these schemes could then be
instantiated to a variety of similar values, in an attempt to find new theorems
which are analogues of existing results from a different context. ``Mutating''
existing theorem statements in such a way would also increase the chance of any
result being considered interesting; since it's likely that the unmodified
statement was deemed interesting, and the new result would not in general follow
as a simple logical consequence.

\subsection{Recurrent Clustering}
\label{sec:clusteringexpressions}

Our recurrent clustering algorithm is similar to that of ML4PG, as our
transformation maps the elements of a syntax tree to distinct cells in a
matrix. In contrast, the matrices produced by ACL2(ml) \emph{summarise} the tree
elements: providing, for each level of the tree, the number of variables,
nullary symbols, unary symbols, etc.

Unlike ML4PG, our algorithm can handle mutually-recursive definitions, but it
also ignores types. This is because obtaining the type of each component of a
Core definition is more difficult than in ML4PG (which queries the current
\textsc{Proof General} state), and is left for future work.

The way we \emph{use} our clusters to inform theory exploration is actually more
similar to that of ACL2(ml) than ML4PG. ML4PG can either present clusters to the
user for inspection, or produce automata for recreating proofs. In ACL2(ml), the
clusters are used to restrict the search space of a proof search, much like we
restrict the scope of theory exploration.

\section{Conclusion and Future Work}
\label{sec:conclusion}

Our use of clustering to pre-process \quickspec{} signatures has required many
decisions and tradeoffs to be made. Hence our approach is just one possibility
out of many alternatives which could be investigated to push this work further.

The most obvious next step is to incorporate types. Types contain valuable
information about an expression, and would allow us to distinguish between
constructors. Since our algorithm closely follows that of ML4PG, which
\emph{does} support types, the only barrier is the practical issue of
propagating annotations through all of the Core definitions.

More speculative directions include the use of \emph{learned} representations,
rather than our hand-crafted features \cite{bengio2013representation}. This
would provide an interesting comparison, as well as being more robust in the
face of language evolution. Another intriguing possibility is to extend the
recurrent nature of our algorithm to make use of the discovered properties
during clustering; for example, by treating the discovered equations as rewrite
rules to reduce the ASTs prior to feature extraction.

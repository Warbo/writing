\section{Haskell}
\label{sec:haskell}

\begin{figure}
  \centering
  \begin{haskell}
-- A datatype with two constructors
data Bool = True | False

-- A recursive datatype (S requires a Nat as argument)
data Nat = Z | S Nat

-- A polymorphic (AKA "generic") datatype
data List t = Nil | Cons t (List t)

-- Arithmetic functions

plus :: Nat -> Nat -> Nat
plus    Z  y = y
plus (S x) y = S (plus x y)

mult :: Nat -> Nat -> Nat
mult    Z  y = Z
mult (S x) y = plus y (mult x y)

-- Mutually-recursive functions

odd :: Nat -> Bool
odd    Z  = False
odd (S n) = even n

even :: Nat -> Bool
even    Z  = True
even (S n) = odd n
  \end{haskell}
  \caption{Haskell code, defining datatypes and functions involving them (note
    that \hs{--} introduces a comment). \hs{Bool} is the Booleans, \hs{Nat} is a
    Peano encoding of the natural numbers and \hs{List t} are polymorphic lists
    with elements of type \hs{t}. Juxtaposition denotes a function call, e.g.
    \hs{f x}, and functions may be defined by pattern-matching (case analysis)
    on their arguments. \hs{A :: B} is an optional type annotation, stating that
    value \hs{A} has type \hs{B}. \hs{A -> B} is the type of functions from
    \hs{A} to \hs{B}.}
  \label{fig:haskellexample}
\end{figure}

We mostly focus our attention on the Haskell programming language, both as an
implementation vehicle and as our representation of functions, properties, etc.
to explore. We choose Haskell since it combines formal, logical underpinnings
which aid reasoning (compared to more popular languages like Java or C), yet it
is still popular enough to sustain a rich ecosystem of tooling and a large
corpus of existing code (compared to more formal languages like Coq or
Isabelle). Haskell is well-suited to programming language research; indeed, this
was a goal of the language's creators \cite{marlow2010haskell}. An example of
Haskell code is given in Figure~\ref{fig:haskellexample}.

Like most \emph{functional} programming languages, Haskell builds upon
$\lambda$-calculus, with extra features such as a strong type system and
``syntactic sugar'' to improve readability. The following features make Haskell
especially useful for our purposes, although many are also present in other
languages such as StandardML and Coq (which we also use, but only when needed
for compatibility with prior work):

\begin{description}

\item{Functional}: Most control flow in Haskell (all except
  \emph{pattern-matching}, described below) is performed by function abstraction
  and application , which we can reason about using standard rules of inference
  such as \emph{modus ponens}. For example, reading from external sources (like
  environment variables) is \emph{impure}, so the only way to parameterise a
  value of type \hs{B} with a value of type \hs{A} is using a function of type
  \hs{A -> B}. Conversely, applying a function of type \hs{A -> B} to a value of
  type \hs{A} can only ever produce a value of type \hs{B} (it may instead crash
  the program or loop forever, but neither of those are \emph{observable}, i.e.
  we can't branch on them).

\item{Pure}: Execution of actions (e.g. reading or writing files) is separate to
  evaluation of expressions; hence our reasoning can safely ignore complicated
  external and non-local interactions. Purity ensures \emph{referential
    transparency}: references can be substituted for their referent with
  no change in semantics (as in standard mathematical practice). This implies
  that calling a function twice with the same argument must produce the same
  result (modulo crashes or infinite loops). For example, consider applying the
  function \hs{pair x = (x, x)} to some arbitrary function call \hs{foo y}. The
  references \hs{x} in the resulting pair \hs{(x, x)} both refer to \hs{foo y}
  so, by referential transparency, can be substituted to get
  \hs{(foo y, foo y)}. Both members of \hs{(x, x)} are identical, by definition;
  hence, to preserve the semantics, both (and indeed \emph{all}) calls to
  \hs{foo y} must also be identical. This holds regardless of what we choose for
  \hs{foo} and \hs{y}, and implies that function behaviour cannot depend on
  external interactions or non-deterministic processes (which may change between
  calls).

\item{Statically Typed}: Expressions are constrained by \emph{types}, which can
  be used to eliminate unwanted combinations of values (for example
  \hs{plus True} is not a valid expression), and hence reduce search spaces;
  \emph{static} types can be deduced syntactically, without having to execute
  the code. The type of Haskell expressions can also be \emph{inferred}
  automatically.\footnote{Except for certain cases, such as those caused by the
    \emph{monomorphism restriction}~\cite{marlow2010haskell}[$\S$4.5.5]}

\item{Curried}: All functions in Haskell take a single argument (as in
  $\lambda$-calculus), which makes them easier to manipulate programatically.
  Currying allows multi-argument functions to be simulated, by accepting one
  argument and returning a function to accept the rest. The \hs{mult} function
  in Figure~\ref{fig:haskellexample} has type \hs{Nat -> Nat -> Nat} meaning it
  takes a \hs{Nat} as argument and returns a function of type
  \hs{Nat -> Nat}. Function calls are written with whitespace, so \hs{mult x y}
  calls the \hs{mult} function with the argument \hs{x}, then calls the result
  of that with the argument \hs{y}. This allows \emph{partial application} such
  as \hs{double = mult (S (S Z))}, but for our purposes it is important for
  unifying calling conventions. For example, in Javascript \hs{mult x y} would
  be either \js{mult(x, y)} or \js{mult(x)(y)}, and depends on the definition of
  \js{mult} (the problem compounds with more arguments). In Haskell there is no
  distinction between these forms.

\item{Non-strict}: ``Strict'' evaluation strategies evaluate the arguments of a
  function call before the function body; non-strict does the opposite. The
  Haskell standards do not specify a particular evaluation strategy, but they
  do require that it be non-strict (for efficiency, most implementations use
  \emph{lazy} evaluation to avoid duplicating work). Strictness can result in
  infinite loops and other errors which may be avoided by non-strictness,
  making the latter more useful for reasoning in the face of such errors.
  For example, given a pair of values \hs{(x, y)} and a projection function
  \hs{fst}, we might make the ``obvious''  conjecture that
  \hs{fst (x, y) = x}.\footnote{Incidentally, this is also a valid definition of
    the \hs{fst} function} This statement is true for non-strict languages, but
  \emph{not} for strict languages. Crucially, a strict language will attempt to
  calculate the value of \hs{y}, which may cause an infinite loop or other
  error; a non-strict language like Haskell will ignore \hs{y} since it isn't
  used in the body of the \hs{fst} function.

\item{Algebraic Data Types}: These provide a rich grammar for building up
  user-defined data representations, and an inverse mechanism to inspect these
  data by \emph{pattern-matching} (Haskell's other form of control flow). The
  \hs{Bool}, \hs{Nat} and \hs{List t} definitions in
  Figure~\ref{fig:haskellexample} are ADTs; whilst the functions use
  pattern-matching to branch on their first argument. For our purposes, the
  useful consequences of ADTs and pattern-matching include their amenability for
  inductive proofs and the fact they are \emph{closed}; i.e. an ADT's
  declaration specifies all of the normal forms for that type. This makes
  exhaustive case analysis trivial, which would be impossible for \emph{open}
  types (for example, consider classes in an object oriented language, where new
  subclasses can be introduced at any time).

\item{Parametricity}: This allows Haskell \emph{values} to be parameterised over
  \emph{type-level} objects; provided those objects are never inspected. This
  enables \emph{polymorphism}. The \hs{List t} type in
  Figure~\ref{fig:haskellexample} is an example: there are many useful functions
  involving lists which work in the same way regardless of the element type
  (e.g. getting the length, appending, reversing, etc.). An even simpler example
  is the polymorphic identity function \hs{id x = x}. The type of \hs{id} is
  \hs{forall t. t -> t}\footnote{The \hs{forall t.} is optional; type-level
    identifiers beginning with a lowercase letter are assumed to be universally
    quantified variables.}, which we can view as taking \emph{two} parameters:
  a type \hs{t} and a value of type \hs{t}. Only the latter argument appears in
  the definition (as \hs{x}), meaning that we can't use the type \hs{t} to
  determine the function's behaviour. Indeed, in the case of \hs{id} we can't
  branch on the value of \hs{x} either, since we don't know what type it might
  have (our definition must work \emph{for all} types \hs{t}); the only
  functions we can call on \hs{x} must also be polymorphic, and hence also
  incapable of branching. The type of \hs{id} states that it returns a value of
  type \hs{t}; without knowing what that type is, the only type-correct value it
  can return is the argument \hs{x}. Hence the \emph{type} of \hs{id} tells us
  everything about its behaviour, with this style of reasoning known as
  \emph{theorems for free}~\cite{wadler1989theorems}. Haskell definitions are
  commonly made polymorphic like this, to prevent incorrect implementations
  passing the type checker, e.g. \hs{fst :: (Nat, Nat) -> Nat} might return the
  wrong element, but \hs{fst :: (t1, t2) -> t1} can't.

\item{Type classes}: Along with their various extensions, type classes are
  interfaces which specify a set of operations over a type or other type-level
  object (like a \emph{type constructor}, e.g. \hs{List}). Many type classes
  also specify a set of \emph{laws} which their operations should obey but,
  lacking a simple mechanism to enforce this, laws are usually considered as
  documentation. As a simple example, we can define a type class \hs{Semigroup}
  with the following operation and an associativity law:

  \begin{center}
    \begin{haskell}
op :: forall t. Semigroup t => t -> t -> t
    \end{haskell}

    $\forall \text{\hs{x y z}}, \text{\hs{op x (op y z)}} =
                                \text{\hs{op (op x y) z}}$
  \end{center}

  The notation \hs{Semigroup t =>} is a \emph{type class constraint}, which
  restricts the possible types \hs{t} to only those which implement
  \hs{Semigroup}. \footnote{Alternatively, we can consider \hs{Semigroup t} as
    the type of ``implementations of \hs{Semigroup} for \hs{t}'', in which case
  \hs{=>} has a similar role to \hs{->} and we can consider \hs{op} to take
  \emph{four} parameters: a type \hs{t}, an implementation of \hs{Semigroup t}
  and two values of type \hs{t}. As with parameteric polymorphism, this extra
  \hs{Semigroup t} parameter is not available at the value level. Even if it
  were, we could not alter our behaviour by inspecting it, since Haskell only
  allows types to implement each type class in at most one way, so there would
  be no information to branch on.} There are many \emph{instances} of
  \hs{Semigroup} (types which may be substituted for \hs{t}), e.g. \hs{Integer}
  with \hs{op} performing addition. Many more examples can be found in the
  \emph{typeclassopedia} \cite{yorgey2009typeclassopedia}. This ability to
  constrain types, and the existence of laws, helps us reason about code
  generically, rather than repeating the same arguments for each particular pair
  of \hs{t} and \hs{op}.

\item{Equational}: Haskell uses equations at the value level, for definitions;
  at the type level, for coercions; at the documentation level, for typeclass
  laws; and at the compiler level, for ad-hoc rewrite rules. This provides us
  with many \emph{sources} of equations, as well as many possible \emph{uses}
  for any equations we might discover. Along with their support in existing
  tools such as SMT solvers, this makes equational conjectures a natural target
  for theory exploration.

\item{Modularity}: Haskell's module system allows definitions to be kept
  private. This mechanism allows modules to provide more guarantees than are
  available just in their types, by constraining the ways that values can be
  constructed. For example, the following module represents email addresses as a
  pair of \hs{String}s, one for the user part and one for the host:

  \begin{haskell}
-- Exports appear between the parentheses
module Email (Email(), at, render) where

data Email = E String String

render :: Email -> String
render (E user host) = user ++ "@" ++ host

-- if/then/else is sugar for pattern-matching a Bool
at :: String -> String -> Maybe Email
at user host = if user == "" || host == ""
                  then Nothing
                  else Just (E user host)
  \end{haskell}

  An \hs{Email} value can be constructed by passing any two \hs{String}s to
  \hs{E}, but \hs{E} is private (not exported). The \hs{at} function is
  exported, but only passes its arguments to \hs{E} iff they are not empty.\footnote{\hs{Maybe t} is a
    safer alternative to the \textsc{Null} construct of other
    languages~\cite{hoare2009null}. It is defined as
    \hs{data Maybe t = Nothing | Just t} and can be understood as an optional
    value, or a computation which may fail, or as a list with at most one
    element, or as a degenerate search tree with no
    backtracking~\cite{wadler1985replace}} Since this module never calls \hs{E}
  with empty \hs{String}s, and other modules must use \hs{at}, we're guaranteed
  that \emph{all} \hs{Email} values will have non-empty \hs{String}s. Such
  ``smart constructors'' can guarantee \emph{any} decidable property, at the
  cost of performing run-time checks on each invocation.\footnote{We can
    guarantee non-emptiness ``by construction'', without run-time checks or
    \hs{Maybe} wrappers, by changing the type to require at least one element.
    \hs{String} is equivalent to \hs{List Char}, with \hs{""} represented as
    \hs{Nil}. Changing \hs{Nil} to require a \hs{Char} would eliminate empty
    \hs{String}s, e.g. \hs{data NonEmpty t = Nil t | Cons t (NonEmpty t)}. We
    could also use a pair like \hs{data NonEmpty t = NE t (List t)} instead.
    Such precise types are often more desirable than smart constructors, but are
    less general since ad-hoc representations need to be invented to enforce
    each guarantee.}

\end{description}

Together, these features make Haskell code highly structured, amenable to
logical analysis and subject to many algebraic laws. However, as mentioned with
regards to type classes, Haskell itself is incapable of expressing or enforcing
these laws (at least, without difficulty \cite{lindley2014hasochism}). This
reduces the incentive to manually discover, state and prove theorems about
Haskell code, e.g. in the style of interactive theorem proving, as these results
may be invalidated by seemingly innocuous code changes. This puts Haskell in a
rather special position with regards to the discovery of interesting theorems;
namely that many discoveries may be available with very little work, simply
because the code's authors are focused on \emph{software} development rather
than \emph{proof} development. The same cannot be said, for example, of ITP
systems; although our reasoning capabilities may be stronger in an ITP setting,
much of the ``low hanging fruit'' will have already been found through the
user's dedicated efforts, and hence theory exploration would be less likely to
discover unexpected properties.

Other empirical advantages to studying Haskell, compared to other programming
languages or theorem proving systems, include:

\begin{itemize}
\item The large amount of Haskell code which is freely available online, e.g. in
  repositories like \href{http://hackage.haskell.org}{Hackage}, with which we
  can experiment.

\item The existence of theory exploration systems such as \hspec{}, \qspec{} and
  \textsc{Speculate}.

\item Related tooling we can re-use such as counterexample finders (\qcheck{},
  \textsc{SmallCheck}, \textsc{SmartCheck}, \textsc{LeanCheck},
  \textsc{Hedgehog}, etc.), theorem provers
  (e.g. \textsc{Hip}~\cite{rosen2012proving}), and other testing and
  term-generating systems like \textsc{MuCheck}~\cite{le2014mucheck},
  \textsc{MagicHaskeller}~\cite{katayama2011magichaskeller} and
  \textsc{Djinn}~\cite{augustsson2005djinn}.

\item The remarkable amount of infrastructure which exists for working with
  Haskell code, including package managers, compilers, interpreters, parsers,
  static analysers, etc.
\end{itemize}

\begin{figure}
  \begin{equation*}
    \begin{split}
      expr\    \rightarrow\ & \CVar\ id                          \\
                         |\ & \CLit\ literal                     \\
                         |\ & \CApp\ expr\ expr                  \\
                         |\ & \CLam\ \mathcal{L}\ expr           \\
                         |\ & \CLet\ bind\ expr                  \\
                         |\ & \CCase\ expr\ \mathcal{L}\ [alt]   \\
                         |\ & \CType                             \\
      id\      \rightarrow\ & \CLocal\       \mathcal{L}         \\
                         |\ & \CGlobal\      \mathcal{G}         \\
                         |\ & \CConstructor\ \mathcal{D}         \\
      literal\ \rightarrow\ & \CLitNum\ \mathcal{N}              \\
                         |\ & \CLitStr\ \mathcal{S}              \\
      alt\     \rightarrow\ & \CAlt\ altcon\ expr\ [\mathcal{L}] \\
      altcon\  \rightarrow\ & \CDataAlt\ \mathcal{D}             \\
                         |\ & \CLitAlt\ literal                  \\
                         |\ & \CDefault                          \\
      bind\    \rightarrow\ & \CNonRec\ binder                   \\
                         |\ & \CRec\ [binder]                    \\
      binder   \rightarrow\ & \CBind\ \mathcal{L}\ expr
    \end{split}
  \end{equation*}
  Where:
  \begin{tabular}[t]{l @{ $=$ } l}
    $\mathcal{S}$ & string literals    \\
    $\mathcal{N}$ & numeric literals   \\
    $\mathcal{L}$ & local identifiers  \\
    $\mathcal{G}$ & global identifiers \\
    $\mathcal{D}$ & constructor identifiers
  \end{tabular}

  \caption{Simplified syntax of GHC Core in BNF style. $[]$ and $(,)$ denote repetition and grouping, respectively.}
  \label{fig:coresyntax}
\end{figure}

Further evidence of Haskell's suitability for theory exploration is given by the
fact that the state-of-the-art implementation for Isabelle/HOL, the
\textsc{Hipster}~\cite{Hipster} system, is actually implemented by translating
to Haskell and invoking \hspec{}~\cite{claessen2013automating}.

Whilst our systems and experiments use normal Haskell code, for simplicity we
perform some of our analyses on an intermediate representation of the
\textsc{GHC} compiler, known as \emph{GHC Core}, rather than the relatively
large and complex syntax of Haskell proper. Core is based on \fc{}, which is
described in detail in~\cite[Appendix C]{sulzmann2007system}.

Core contains explicit type annotations and coercions, which we omit as they
have no effect on runtime behaviour. The resulting sub-set of Core\footnote{As
  of GHC version 7.10.2.} is shown in Figure~\ref{fig:coresyntax}; for brevity,
we also omit several other forms of literal (machine words of various sizes,
individual characters, etc.), since their treatment is similar to those of
strings and numerals. We use quoted strings to denote names and literals,
e.g. \hs{Local "foo"}, \hs{Global "bar"}, \hs{Constructor "Baz"}, \hs{LitStr
  "quux"} and \hs{LitNum "42"}, and require only that they can be compared for
equality.

\begin{figure}
  \begin{subfigure}[plus]{\textwidth}
    \begin{small}
      \underline{\texttt{plus}}
      \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Local "y")))
                       (Alt (DataAlt "S") (App (Var (Constructor "S"))
                                               (App (App (Var (Global "plus"))
                                                         (Var (Local  "x")))
                                                    (Var (Local "y"))))
                                          "x")))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[mult]{\textwidth}
    \begin{small}
      \underline{\texttt{mult}}
      \begin{verbatim}
Lam "a" (Lam "y" (Case (Var (Local "a"))
                       "b"
                       (Alt (DataAlt "Z") (Var (Constructor "Z")))
                       (Alt (DataAlt "S") (App (App (Var (Global "plus"))
                                                    (Var (Local  "y")))
                                               (App (App (Var (Global "mult"))
                                                         (Var (Local  "x")))
                                                    (Var (Local  "y"))))
                                          "x")))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[odd]{\textwidth}
    \begin{small}
      \underline{\texttt{odd}}
      \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "False")))
              (Alt (DataAlt "S") (App (Var (Global "even"))
                                      (Var (Local  "n")))
                                 "n"))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \begin{subfigure}[even]{\textwidth}
    \begin{small}
      \underline{\texttt{even}}
      \begin{verbatim}
Lam "a" (Case (Var (Local "a"))
              "b"
              (Alt (DataAlt "Z") (Var (Constructor "True")))
              (Alt (DataAlt "S") (App (Var (Global "odd"))
                                      (Var (Local  "n")))
                                 "n"))
      \end{verbatim}
    \end{small}
  \end{subfigure}
  \caption{Translations of functions in Figure \ref{fig:haskellexample} into the
    Core syntax of Figure \ref{fig:coresyntax}. Notice the introduction of
    explicit $\lambda$ abstractions (\texttt{Lam}) and the use of \texttt{Case}
    to represent piecewise definitions. Fresh variables are chosen arbitrarily
    as \texttt{"a"}, \texttt{"b"}, etc.}
  \label{fig:coreexample}
\end{figure}

GHC's translation from Haskell to Core is routine: Figure~\ref{fig:coreexample}
shows the Core representation of functions from Figure~\ref{fig:haskellexample}.
Although the Core is more verbose, we can see that similar structure in the
Haskell definitions gives rise to similar structure in the Core; for example,
the definitions of \hs{odd} and \hs{even} are identical in both languages,
except for the particular identifiers used. It is this close correspondence
which allows us to analyse Core expressions in place of their more complicated
Haskell source.

Note that we exclude representations for type-level entities, including datatype
definitions like that of \hs{Nat}. GHC can represent these, but we are mostly
concerned with dynamic behaviour, which excludes types (they are \emph{erased}
during compilation). For simplicity we also avoid using constructors directly,
choosing instead to wrap them with normal functions (e.g. \hs{s x = S x}), since
this makes our domain more symmetric and reduces cross-language differences.

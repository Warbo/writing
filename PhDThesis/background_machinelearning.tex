\section{Machine Learning}

Machine learning (ML) is a sub-field of artificial intelligence (AI) which
emphasises the use of statistical methods to find patterns and optimise
functions, in contrast with symbolic reasoning approaches like automated theorem
proving (commonly referred to as ``Good Old-Fashioned AI'').

The machine learning umbrella spans many techniques and applications, but one
common distinction is between \emph{supervised} and \emph{unsupervised} tasks.
In supervised learning we allow the system to perform some behaviour, then an
external process assesses whether it behaved well or poorly (akin to Justice
Potter Stewart's test for obscenity: ``I know it when I see it''). The system's
parameters are optimised to try and score highly on the test. Common supervised
learning tasks include function approximation using input/output examples (e.g.
classifying scanned images of numerals) where the test is the proportion of
example inputs which got the correct output, and reinforcement learning where
the system chooses from a set of ``actions'', receives a pair of ``observation''
and ``reward'' (test score) in response, and must accumulate the most reward.

Unsupervised learning does not have the same testable ``success'' criteria as
supervised learning. Instead, the goal is to find patterns and form models of
the target domain, which is more ambitious and open-ended than supervised
learning but also much harder to define, measure and optimise. Examples of this
are prediction and compression tasks such as auto-encoders
(learned compressor/decompressor pairs, rewarded by how well they preserve data
through a round-trip) and adversarial or co-evolution tasks (coupled systems
where each is rewarded by exploiting mis-prediction in the others).

\subsection{Feature Extraction}

One major difficulty when applying statistical machine learning algorithms to
\emph{languages}, such as Haskell, is the appearance of recursive
structures. This can lead to nested expressions of arbitrary depth, which are
difficult to compare in numerical ways. One common approach to this problem is
to represent such structures as \emph{sequences}. \emph{Recurrent neural
  networks} (RNNs) are a popular choice for processing sequences, especially
when combined with mechanisms such as \emph{long short-term memory} (LSTM) for
preserving information across long sequences \cite{hochreiter1997long}. Such
systems have been used, for example, to parse and execute computer programs
\cite{zaremba2014learning}. However, learning to parse sequences seems
inefficient considering that we already have correctly-parsed ASTs.

Whilst neural networks have been applied directly to recursive structures
\cite{goller1996learning}, including using LSTM \cite{zhu2015long}, a more
popular approach is to use \emph{kernel methods}
\cite{bakir2007predicting}. These are promising as a more principled alternative
to our current hand-crafted translation of ASTs to vectors.

\subsubsection{Distributed Representations}

A \emph{distributed representation} does not isolate each semantic feature (for
example, presence of absence of some lexeme) to separate, independent
``computing elements'' (e.g. memory locations, such as the elements of a feature
vector). Instead, features are represented as \emph{patterns} across multiple
elements: each feature uses many elements and each element contributes to many
features~\cite{hinton1984distributed}. Such encodings provide simple mechanisms
to represent \emph{combinations} of features (combining their patterns
element-wise), \emph{partial} features (presence of the pattern across a subset
of its elements) and \emph{generalisation} (patterns which are present as
sub-patterns of multiple features).

% Distributed representations may be learned from data, for example by
% \emph{auto-encoding}; or they may be calculated ``from whole cloth'' using
% techniques like hashing. We are particularly interested in distributed
% representations for storing structured data (like syntax trees) in fixed-length
% feature vectors.

\input{background_clustering.tex}

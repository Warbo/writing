\subsection{Interestingness}

Whilst there are many reasonably objective benchmarks for mathematical tasks
such as automated theorem proving, the precision/recall analysis shown
in~\cite{claessen2013automating}, and described further in $\S$\ref{sec:te}, is
the only quantitative comparison of these recent MTE tools we have found in the
literature. Our Theory Exploration Benchmark, described in
section~\ref{sec:benchmark}, is essentially an extension of this approach, to a
larger and more diverse set of examples. The suitability of the TIP theorem
proving benchmark for our purposes, detailed in $\S$\ref{sec:tip}, is not
coincidental, since its developers are also those of the IsaCoSy and QuickSpec
tools we have tested. This goes some way to ensuring that our demonstration is a
faithful representation of the task these tools were intended to solve; our
independent repurposing of this problem set, in a way it was not designed for,
reduces the risk that the benchmark is tailor-made for these tools (or that the
tools over-fit to these particular problems).

% TODO Include Colton/Bundy et al

% TODO Include something about intrinsic motivation, compression progress,
% variance among experts (decision tree stuff as well as more established
% splitting functions stuff)

% TODO Make sure we mention the recent reinforcement learning task that includes
% a constantly-changing "TV" in the 3D world, shown in some TwoMinutePapers
% videos

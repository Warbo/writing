\section{Theory Exploration}
\label{sec:theoryexploration}

% TODO Work this in
\iffalse
Theory exploration is the task of taking a \emph{signature} of definitions in
some formal system (for example a programming language) and automatically
generating a set of formal statements (properties) involving those
definitions. These may be conjectures or theorems (proven either by sending
conjectures to an automated theorem prover, or by having the generating
procedure proceed in logically sound steps); in either case, these statements
must also be ``interesting'' in some way. It is beyond the scope of this paper
to define what makes a mathematical statement ``interesting'', but this problem
has been tackled extensively in the literature~\cite{colton2000notion}.
\fi
% END TOTO

In this work we consider the application of \emph{(automated) theory
  exploration} to generate conjectures about code. Existing tools are able to
\emph{prove} those conjectures, and hence output \emph{novel} theorems without
guidance from the user. The method of conjecture generation is a key
characteristic of any theory exploration system, although all existing
implementations rely on brute force enumeration to some degree.

We focus on \qspec{}~\cite{QuickSpec}, which conjectures equations about Haskell
code (these may be fed into another tool, such as \hspec{}, for proving). We
have used version 1 of \qspec{} in our experiments, due to its availability,
stability and tooling integration; hence references to \qspec{} are to version 1
unless stated otherwise. At the time of writing there is a \qspec{} version 2
available, which has a much improved generation procedure which can be
significantly faster than its predecessor. Whilst \qspec{} 2 has advanced the
state of the art in theory exploration, our preliminary experience has found
that it still suffers the scaling issues we identify in this work; indeed, we
even encountered memory exhaustion from some of the examples included with the
\qspec{} 2 source code! Since \qspec{} 2 has less integration with the tooling
we have used, we leave a more thorough analysis of it (and related systems such
as \speculate{}) for future work, pending the necessary infrastructure changes
that would require.

\begin{figure}
  \centering
  \begin{minted}{haskell}
    -- A datatype with two constructors (Note that -- introduces a comment)
    data Bool = True | False

    -- A recursive datatype (S requires a Nat as argument)
    data Nat = Z | S Nat

    -- A function turning a Bool into a Bool
    not :: Bool -> Bool
    not True  = False
    not False = True

    -- A pair of mutually recursive functions

    odd :: Nat -> Bool
    odd    Z  = False
    odd (S n) = even n

    even :: Nat -> Bool
    even    Z  = True
    even (S n) = odd n
  \end{minted}
  \caption{Haskell datatypes for booleans and natural numbers, followed by some
    simple function definitions (with type annotations).}
  \label{fig:haskellteexample}
\end{figure}

\qspec{} (version 1) is written in the Haskell programming language, and works
with definitions which are also written in Haskell. A thorough description of
Haskell, including its suitability for theory exploration, can be found
in~\ref{sec:haskell}. For illustrative purposes, some simple Haskell definitions
are shown in Figure~\ref{fig:haskellteexample}. The following procedure is used to
generate conjectures, which are both plausible (due to testing on many examples)
and potentially interesting (due to being mutually irreducible):

\begin{enumerate}
\item Given a typed signature $\Sigma$ and set of variables $V$, \qspec{}
  generates a list $terms$ containing the constants (including functions) from
  $\Sigma$, the variables from $V$ and type-correct function applications
  $f(x)$, where $f$ and $x$ are elements of $terms$ \iffalse TODO: A little
  awkward; maybe use the above notation? \fi. To ensure the list is finite,
  function applications are only nested up to a specified depth (by default, 3).
\item The elements of $terms$ are grouped into equivalence classes, based on
  their type.
\item Each variable is instantiated to a particular value, generated randomly by
  \qcheck{}.
\item For each class, the members are compared (using a pre-specified function,
  such as equality \hs{==}) to see if these instantiations have caused an
  observable difference between members. If so, the class is split up to
  separate such distinguishable members.
\item The previous steps of variable instantiation and comparison are repeated
  until the classes stabilise (i.e. no differences have been observed for some
  specified number of repetitions).
\item A set of equations are then conjectured, relating each class's members.
\end{enumerate}

\iffalse
% TODO: Compare the above to this; merge/swap anything that's better
\begin{enumerate}
\item Given a typed signature $\Sigma$ and set of variables $V$, \qspec{}
  generates a list $terms$ containing the constants (including functions) from
  $\Sigma$, the variables from $V$ and type-correct function applications
  \hs{f x}, where \hs{f} and \hs{x} are elements of $terms$. To ensure the list
  is finite, function applications are only nested up to a specified depth (by
  default, 3).
\item The elements of $terms$ are grouped into equivalence classes, based on
  their type.
\item The equivalence of terms in each class is tested using \qcheck{}:
  variables are instantiated to particular values, generated randomly, and the
  resulting closed expressions are evaluated and compared for equality.
\item If a class is found to have non-equal members, it is split up to separate
  those members.
\item The previous steps of testing and splitting are repeated until the classes
  stabilise (i.e. no differences have been observed for some specified number of
  repetitions).
\item For each class, one member is selected and equations are conjectured that
  it is equal to each of the other members.
\item A congruence closure algorithm is applied to these equations, to discard
  any which are implied by the others.
\end{enumerate}
\fi

Such conjectures can be used in several ways: they can be simplified for direct
presentation to the user (for example via the congruence closure algorithm
\iffalse\cite{TODO}\fi), sent to a more rigorous system like \hspec{} or
\hipster{} for proving, or even serve as a background theory for an
automated theorem prover \cite{claessen2013automating}.

As an example, we can consider a simple signature containing the expressions
from Figure \ref{fig:haskellteexample}:

\begin{align*}
  \Sigma_{\texttt{Nat}} = \{\texttt{Z}, \texttt{S}, \texttt{plus}, \texttt{mult}, \texttt{odd}, \texttt{even}\}
\end{align*}

Together with a set of variables, say $V_{\texttt{Nat}} = \{a, b, c\}$,
\qspec{}'s enumeration will resemble the following:

\begin{align*}
  terms_{\texttt{Nat}} = [& \texttt{Z},\ \texttt{S},\ \texttt{plus},\ \texttt{mult},\ \texttt{odd},\ \texttt{even},\ a,\ b,\ c,\ \texttt{S Z},\ \texttt{S}\ a,\ \texttt{S}\ b, \\
                     & \texttt{S}\ c,\ \texttt{plus Z},\ \texttt{plus}\ a,\ \dots ]
\end{align*}

Notice that functions such as \hs{plus} and \hs{mult} are valid terms, despite
not being applied to any arguments. In addition, Haskell curries functions, so
these binary functions will be treated as unary functions which return unary
functions. This is required as the construction of $terms$ applies functions to
one argument at a time.

\begin{figure}
  % To reproduce, run 'quickSpec nat' in haskell_example/src/QuickSpecExample.hs
  \begin{haskell}
                      plus a b = plus b a
                      plus a Z = a
             plus a (plus b c) = plus b (plus a c)
                      mult a b = mult b a
                      mult a Z = Z
             mult a (mult b c) = mult b (mult a c)
                  plus a (S b) = S (plus a b)
                  mult a (S b) = plus a (mult a b)
             mult a (plus b b) = mult b (plus a a)
                     odd (S a) = even a
                odd (plus a a) = odd Z
               odd (times a a) = odd a
                    even (S a) = odd a
               even (plus a a) = even Z
              even (times a a) = even a
    plus (mult a b) (mult a c) = mult a (plus b c)
  \end{haskell}
  \caption{Equations conjectured by \qspec{} for the functions in Figure
    \ref{fig:haskellteexample}; after simplification.}
  \label{fig:qspecresult}
\end{figure}

These terms will be grouped into five classes, one each for \hs{Nat},
\hs{Nat -> Nat}, \hs{Nat -> Nat -> Nat}, \hs{Nat -> Bool} and \hs{Bool}. As the
variables $a$, $b$ and $c$ are instantiated to various randomly-generated
numbers, these equivalence classes will be divided, until eventually the
equations in Figure \ref{fig:qspecresult} are conjectured.

Although complete, this enumeration approach is wasteful: many terms are
unlikely to appear in theorems, which requires careful choice by the user of
what to include in the signature. Here we know that addition and multiplication
are closely related, and hence obey many algebraic laws.

\qspec{} (and \hspec{}) is also compatible with Haskell's existing testing
infrastructure, such that an invocation of \texttt{cabal test} can run these
tools alongside more traditional QA tools like \qcheck{}, \textsc{HUnit} and
\textsc{Criterion}.

In fact, there are similarities between the way a TE system like \qspec{} can
generalise from checking \emph{particular} properties to \emph{inventing} new
ones, and the way counterexample finders like \qcheck{} can generalise from
testing \emph{particular} expressions to \emph{inventing} expressions to
test. One of our aims is to understand the implications of this generalisation,
the lessons that each can learn from the other's approach to term generation,
and the consequences for testing and QA in general.

% TODO
%%%%% THE BELOW SEEMS TO BE INTERESTINGNESS?

%\begin{description}
%\item{Interestingness} Various alternative interestingness criteria have been
%  proposed, which we survey in \S \ref{sec:relatedwork}. Augmenting or replacing
%  the criteria may be useful, for example to distinguish useful relationships
%  from incidental coincidences; or to prevent surprising, insightful equations
%  from being discarded because they can be simplified.
%\end{description}

% Theory exploration is similar to \emph{experimental mathematics}

% - Relation to Science
%  - Testable/falsifiable hypotheses are like evaluable terms (or, more generally, conjectures which can be decided, using a reasonable amount of resources).
% - Relation to AI tasks: exploring surroundings, etc.

% - Statistics is another area that's less straightforward than normal numerical computing, since there is subjectivity and judgement involved in the answering of questions.

% Theory formation: Alison? Others.
% Theory exploration: Buchberger, Moa in Isabelle, Koen in Haskell. Others?
% Theorem proving: Well-trodden: first-order ATP, higher-order ITP, functional programming
% Communication: Latex, Wikis, APIs, communicating with aliens

% \section{Exploration in Theorem Proving}
% \label{sec:examples}

% Before exploring abstract definitions of interestingness, we can first consider
% some scenarios which arise during formal proof where we are forced to generate
% conjectures. An analysis of these situations, and the subsequent theorems they
% produce, will contribute towards an empirical justification for what is
% interesting (at least from a utilitarian point of view) and inform our later
% exploration of the literature.

% \subsection{Generalisation}

% \providecommand{\coq}[1]{\lstinline[language=ML]|#1|}

% When we \emph{generalise} a statement $S$, we obtain a new statement $S'$ of
% which $S$ is a special case. Although it seems counterintuitive, a generalised
% statement can sometimes be \emph{easier} to prove than the original. This arises
% often in inductive proofs, since the specific obligations which arise in the
% proof may be incompatible with the available inductive hypotheses.

% However, we cannot blindly generalise \emph{all} obligations we encounter, since
% \emph{over-generalising} results in obligations which are so strong that they
% are unprovable, or even false. We must therefore rely on heuristics to guide the
% generation of generalised conjectures, and hence perform a kind of exploration.

% An informative example is given by Boyer and Moore of the associativity of
% multiplication in ACL2 \cite{boyer1983proof}:

% $$(x * y) * z = x * (y * z)$$

% During the course of the proof, the following obligation arises:

% \begin{equation}
%   \tag{conc3}
%   (y + (x * y)) * z = (y * z) + ((x * y) * z)
%   \label{eq:conc3}
% \end{equation}

% ACL2 automatically generalises \eqref{eq:conc3} by replacing the repeated
% sub-term $x * y$ with a fresh variable $w$:

% \begin{equation}
%   \tag{conc4}
%   (y + w) * z = (y * z) + (w * z)
%   \label{eq:conc4}
% \end{equation}

% This generalised form is clearly the distributivity law for multiplication and
% addition, which can be proved separately to the original goal of
% associativity. It would not be controversial to claim that this distributivity
% law is interesting in its own right (relative to associativity, at least), in
% addition to its usefulness in making this proof go through.

% % TODO: Describe the ACL2 heuristics

% Generalisation also occurs frequently when reasoning about \emph{tail-recursive}
% definitions \cite{kapur2003automatic}. \footnote{A tail-recursive function can
%   be executed in constant space using a loop, whereas recursion in non-tail
%   positions may require a growing number of stack frames or nested closures. See
%   \S \ref{sec:auxiliarylemmas} for example definitions of each type.}

% \subsection{Analogy}

% One way to characterise the interestingness of a statement is by \emph{analogy}
% to existing interesting statements. By finding lemmas analogous to those of a
% different theory, we may be able to re-use tactics and other forms of
% meta-programming across both.

% Existing theory exploration systems have been successfully applied to this
% problem, however the use of pure exploration misses opportunities to
% \emph{focus} the search, since we know which lemmas are used in those theories
% where a technique succeeded. If we can find an analogy to map from such solved
% problems to our unsolved goal, we can infer the approximate form of the lemmas
% we require, and target these specifically.

% The approach taken by \textsc{ACL2(ml)} is to find lemmas which may be relevant
% to solving a goal $G$ by making analogies via unsupervised clustering
% \cite{Heras.Komendantskaya.Johansson.ea:2013}. These clusters are used in two
% ways:

% \begin{itemize}
% \item First, we use the cluster $C_G$ containing $G$ to identify analogous
%   theorems.

% \item For each theorem $T \in C_G \setminus \{G\}$, we consider those symbols
%   $S_T$ which occur in $T$ but not in $G$. Our analogous lemmas are those used
%   to prove $T$, mutated such that symbols $s \in S_T$ are replaced by members of
%   the cluster $C_s$ containing $s$.
% \end{itemize}

% The running examples for demonstrating \textsc{ACL2(ml)} are equivalence
% theorems for tail-recursive and non-tail-recursive calculations, as well as the
% effect of repeating certain list operations:

% \begin{itemize}

%   \item $\forall n, \texttt{natp}(n) \rightarrow \texttt{fact-tail}(n) = \texttt{fact}(n)$ where \texttt{natp} is the predicate that $n$ is a natural number, whilst \texttt{fact-tail} and \texttt{fact} are tail-recursive and non-tail-recursive implementations of factorial, respectively.

%   \item $\forall n, \texttt{natp}(n) \rightarrow \texttt{power-tail}(n) = \texttt{power}(n)$,  where \texttt{power-tail} and \texttt{power} calculate powers of 2.

%   \item $\forall n, \texttt{natp}(n) \rightarrow \texttt{fib-tail}(n) = \texttt{fib}(n)$,  where \texttt{fib-tail} and \texttt{fib} calculate fibonacci numbers.

%   \item $\forall x, \texttt{nat-listp}(x) \rightarrow \texttt{sort}(\texttt{sort}(x)) = \texttt{sort}(x)$, for list-of-natural-numbers predicate \texttt{nat-listp} and list-sorting function \texttt{sort}.

%   \item $\forall x, \texttt{true-listp}(x) \rightarrow \texttt{rev}(\texttt{rev}(x)) = x$, where \texttt{true-listp} ensures that $x$ is a valid singly-linked list structure and \texttt{rev} is list reversal.

%   \item $\forall x, \texttt{true-listp}(x) \rightarrow \texttt{int}(x, x) = x$, where \texttt{int} is the intersection of lists (i.e. a list of elements common to each).

% \end{itemize}

% \subsection{Auxiliary Lemmas} \label{sec:auxiliarylemmas}

% One consideration when generating conjectures is the difference between
% theorems, lemmas, corollaries, etc. From a logical point of view, these are all
% equivalent, and hence most proof assistants do not distinguish between
% them. However, their \emph{intention} may be different: in a sense, theorems are
% the interesting results; whilst lemmas are useful results, required for proving
% the theorems.

% Some systems, like Coq, allow users to \emph{label} each statement as being a
% \coq{Theorem}, a \coq{Lemma}, etc. despite their internal representations being
% the same. This shows us immediately that lemmas outnumber theorems; in the Coq
% standard library there are over five times as many lemmas as theorems
% \footnote{The latest version as of writing is \texttt{coq-8.4pl6} which, when
%   excluding comments, includes 1492 occurences of \coq{Theorem} and 7594 of
%   \coq{Lemma} in its \texttt{theories/} directory.}.

% % TODO: Analyse them

% % TODO: Theory exploration as lemma generation; give example from a HipSpec paper

% We can find a need for auxiliary lemmas, once again, in the context of
% tail-recursive functions. Consider proving the (pointwise) equality of the
% following Coq functions, defined for the Peano naturals \coq{Z} and \coq{S}:

% \begin{coqblock}
% Inductive Nat : Set := Z : Nat
%                      | S : Nat -> Nat.

% Fixpoint plus      (n m : Nat) := match n with
%                                       | Z    => m
%                                       | S n' => S (plus n' m)
%                                   end.

% Fixpoint plus_tail (n m : Nat) := match n with
%                                       | Z    => m
%                                       | S n' => plus_tail n' (S m)
%                                   end.
% \end{coqblock}

% \begin{haskell}
% Haskell equivalent:

% plus :: Nat -> Nat -> Nat
% plus      n  Z    = n
% plus      n (S m) = S (plus n m)

% plus_tail :: Nat -> Nat -> Nat
% plus_tail n  Z    = n
% plus_tail n (S m) = plus_tail (S n) m
% \end{haskell}

% Both of these functions implement addition, but the \coq{plus_tail} variant is
% tail-recursive. However, if we want to \emph{prove} that the definitions are
% (pointwise) equal, we run into difficulties. In particular, when the inductive
% step requires us to prove \coq{plus (S n) m = plus n (S m)} (which seems
% reasonable), we cannot make this go through using another inductive argument.

% \begin{coqblock}
% (* Solve equalities by beta-normalising both sides *)
% Ltac triv := try (simpl; reflexivity).

% (* Prove equivalence of plus and plus_tail *)
% Theorem equiv : forall n m, plus n m = plus_tail n m.
%   induction n; triv. (* Base case is trivial *)

%   (* Inductive case: plus (S n) m = plus_tail (S n) m *)
%   intro m.

%   (* Beta-reduce the right-hand-side (justification is trivial) *)
%   replace (plus_tail (S n) m) with (plus_tail n (S m)); triv.

%   (* Use induction hypothesis to replace plus_tail with plus *)
%   rewrite <- (IHn (S m)).
% \end{coqblock}

% Specifically, the \emph{conclusion} of a second inductive hypothesis is exactly
% the equation we need:

% \begin{coqblock}
% IHn' : (forall x, plus n' x = plus_tail n' x) -> plus (S n') m = plus n' (S m)
% \end{coqblock}

% Yet we cannot provide it with the argument it needs, as our original induction
% hypothesis is \emph{too specific} (i.e. it has too many \coq{S} constructors):

% \begin{coqblock}
% IHn : forall x, plus (S n') x = plus_tail (S n') x
% \end{coqblock}

% We are forced to abandon the proof, despite such a reasonable-looking
% intermediate goal.

% In fact, if we attempt to prove that goal \emph{separately}, we can use a
% straightforward argument by induction; even though it is actually
% \emph{stronger} due to the absence of the \coq{IHn} assumption. Using this
% separate result as a lemma, the pointwise equality is proven easily.

% \begin{coqblock}
% Lemma gen n m : plus (S n) m = plus n (S m).
%   induction n; triv. (* Base case is trivial *)

%   (* Move all S constructors outside *)
%   simpl. rewrite <- IHn. simpl.

%   (* Trivial *)
%   reflexivity.
% Defined.
% \end{coqblock}

% \begin{coqblock}
%   rewrite (gen n m).
%   reflexivity.
% Defined.
% \end{coqblock}

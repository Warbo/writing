\subsection{Automated Mathematical Discovery and Invention}

Automated logical and mathematical reasoning has been pursued since at least the
time of mechanical calculators like the
Pascaline~\cite{ocagne93:_le_calcul_simpl}. A recurring theme in these efforts
is the separation between those undertaken by mathematicians like Pascal and
Babbage~\cite{bowden}, and those of engineers such as
M\"uller~\cite[p. 65]{lindgren}. This pattern continues today, with the tasks we
are concerned with (automatically constructing and evaluating concepts,
conjectures, theorems, axioms, examples, etc.) being divided into two main
fields: Mathematical Theory Exploration (MTE)~\cite{buchberger:06} (also
sometimes prefaced with ``Computer-Aided'', ``Automated'' or
``Algorithm-Supported''), which is championed by mathematicians such as
Buchberger~\cite{buchberger}; and Automated Theory Formation
(ATF)~\cite{lenat:77,colton:book}, pursued by AI researchers including Lenat.
Other related terms include ``Automated Mathematical
Discovery''~\cite{epstein:91,colton2000notion,esarm2008}, ``Concept Formation in
Discovery Systems''~\cite{haase}, and ``Automated Theorem
Discovery''~\cite{roy}.

Such a plethora of terminology can mask similarities and shared goals between
these fields. Even notable historical differences, such as the emphasis of MTE
on user-interaction and mathematical domains, in contrast to the full automation
and more general applications targeted by ATF, are disappearing in recent
implementations.

\subsection{Theory Formation}

An important historical implementation of ATF is Lenat's AM (Automated
Mathematician) system. Unlike prior work, such as
Meta-Dendral~\cite{buchanan:75} and those described in~\cite{winston}, AM aims
to be a general purpose mathematical discovery system, designed to both
construct new concepts and conjecture relationships between them. AM is a
rule-based system which represents knowledge using a frame-like scheme, enlarges
its knowledge base via a collection of heuristic rules, and controls the firing
of these rules via an agenda mechanism.

The performance of AM was evaluated based on its generality (performance in new
domains) and how finely-tuned various aspects of the program are (the agenda,
the interaction of the heuristics, etc). Most of this evaluation was
qualitative, and has subsequently been criticised~\cite[chap.~13]{colton:book}.
In their case study in methodology, Ritchie and Hanna found a large discrepancy
between the theoretical claims made of AM and the implemented
program~\cite{ritchie1984case}; for example, AM ``invented'' natural numbers
from sets, but did so using a heuristic specifically designed to make this
connection.

The successor of AM is Eurisko, a discovery system intended to be useful in more
general domains than just mathematical theory formation. Despite claiming some
early successes % FIXME Traveller TCS
work on Eurisko was put on hold due to a lack of machine-readable data about
real world systems for it to work with % FIXME Find something quotable
. This lead to the Cyc project, in an attempt to encode such knowledge in a
semantically rich and consistent form. Cyc is an ongoing project, with parts of
its database made available via OpenCyc.

\subsection{Theory Exploration}

The prototypical implementation of MTE is the Theorema system of Buchberger and
colleagues~\cite{buchberger,buchberger2016theorema}, which also places a strong
emphasis on user interface and output presentation. Theory exploration in the
Theorema system involves the user formalising their definitions in a consistent,
layered approach; such that reasoning algorithms can exploit this structure in
subsequent proofs, calculations, etc. The potential of this strategy was
evaluated by illustrating the automated synthesis of Buchberger's own Gr\"obner
bases algorithm~\cite{buchberger:04}.

A similar ``layering'' approach is found in the \textsc{IsaScheme} system of
Monta{\~n}o-Rivas \etal{}~\cite{Montano-Rivas.McCasland.Dixon.ea:2012}, which
has also been quantitatively compared against \textsc{IsaCoSy} and
\textsc{HipSpec} using precision/recall analysis~\cite{claessen2013automating}.
The name comes from its embedding in the Isabelle proof assistant and its use of
``schemes'': higher-order formulae which can be used to generate new concepts
and conjectures. Variables within a scheme are instantiated automatically and
this drives the invention process. For example, the concept of ``repetition''
can be encoded as a scheme, and instantiated with existing encodings of zero,
successor and addition to produce a definition of multiplication. The same
scheme can be instantiated with this new multiplication function to produce
exponentiation.

\textsc{IsaCoSy} (Isabelle Conjecture Synthesis) is written for the Isabelle
proof assistant, mostly in
StandardML~\cite{Johansson.Dixon.Bundy:conjecture-generation}. It conjectures
equations by enumerating expressions involving a given set of (typed) constants
and free variables (a \emph{signature}). A constraint solver forbids certain
(sub)expressions from being synthesised, and these constraints are extended
whenever a new property is conjectured, to avoid generating any special-cases of
this property in the future. Conjectures are tested by looking for
counterexamples and, if none are found, sent to \textsc{IsaPlanner} which
attempts to prove them.

\textsc{QuickSpec} (the conjecture generation component of \textsc{HipSpec})
emerged from work on the \textsc{QuickCheck} software testing framework for the
Haskell programming language~\cite{claessen2011quickcheck}. As well as
conjecturing properties of Haskell definitions for \textsc{HipSpec}, it has been
applied to Isabelle definitions via the \textsc{Hipster} tool~\cite{Hipster}.
Like \textsc{IsaCoSy}, \textsc{QuickSpec} takes a signature and enumerates
well-typed terms. It collects together those of the same type into equivalence
classes, assuming them to be equal, then uses \textsc{QuickCheck} to find
counterexamples to this assumption by randomly instantiating the variables and
comparing the resulting closed expressions. Any equivalence class whose elements
don't compare equal are split up, and the process is repeated with new random
values.

After 500 rounds of testing, any expressions still sharing the same equivalence
class are conjectured to be equal for all values of their variables. Rather than
using a constraint system to prevent redundancies (such as special-cases of
other properties), \textsc{QuickSpec} instead passes its output through a
congruence closure algorithm to achieve the same effect.

\textsc{QuickSpec} has since evolved to version 2~\cite{smallbone2017quick},
which replaces the distinct enumeration and testing steps with a single,
iterative algorithm similar to that of \textsc{IsaCoSy}. Generated conjectures
are fed into a Knuth-Bendix completion algorithm to form a corresponding set of
rewrite rules. As expressions are enumerated, they are simplified using these
rules and discarded if equal to a known expression. If not, \textsc{QuickCheck}
tests whether the new expression can be distinguished from the known expressions
through random testing: those which can are added to the set of known
expressions. Those which cannot be distinguished are conjectured to be equal,
and the rewrite rules are updated.

\textsc{QuickSpec} has also inspired another MTE tool for Haskell called
\textsc{Speculate}~\cite{braquehais2017speculate}, which operates in a similar
way but also makes use of the laws of total orders and Boolean algebra to
conjecture \emph{in}equalities and conditional relations between expressions.

Another notable MTE implementation, distinct from those based in Isabelle and
Haskell, is the \textsc{MATHsAiD} project (Mechanically Ascertaining Theorems
from Hypotheses, Axioms and Definitions)~\cite{roy}. Unlike the tools above,
which generate \emph{conjectures} that may later be sent to automated provers,
\textsc{MATHsAiD} directly generates \emph{theorems}, by making logically valid
inferences from a given set of axioms and definitions. Evaluation of the
interestingness of these theorems was performed qualitatively by the system's
developer, which highlights how these tools could benefit from the availability
of an objective, repeatable, quantitative method of evaluation and comparison.

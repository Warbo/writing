\section{Background}
\label{sec:background}

\subsection{Theory Exploration}
\label{sec:theoryexploration}

Theory exploration is the task of taking a \emph{signature} of definitions in
some formal system (for example a programming language) and automatically
generating a set of formal statements (properties) involving those
definitions. These may be conjectures or theorems (proven either by sending
conjectures to an automated theorem prover, or by having the generating
procedure proceed in logically sound steps); in either case, these statements
must also be ``interesting'' in some way. It is beyond the scope of this paper
to define what makes a mathematical statement ``interesting'', but this problem
has been tackled extensively in the literature~\cite{colton2000notion}.

We will follow the tradition of prior practitioners and use an existing
corpus of theorems as a \emph{ground truth} against which to compare results.
Prior work has used libraries of the \textsc{Isabelle/HOL} proof assistant as a
ground
truth~\cite{Montano-Rivas.McCasland.Dixon.ea:2012,johansson2009isacosy,claessen2013automating},
but these only contained a few definitions and theorems each. To analyse
behaviour on a larger scale, as required to study the effects of clustering, we
have chosen the \emph{Tons of Inductive Problems} (TIP) problem set for
automated theorem provers~\cite{claessen2015tip}.  Reasons for choosing TIP
include:

\begin{itemize}
\item Each problem in TIP clearly distinguishes between the statement to be
  proved and the definitions involved, such that the two can be easily and
  meaningfully separated to act as inputs and desired outputs in a TE setting.
  This would not be the case for problem sets like
  SMT-COMP~\cite{barrett2005smt}, where many problems involve uninterpreted
  functions whose behaviour is \emph{implicit} in the logical structure of the
  theorem statement but not separately \emph{defined}.
\item The TIP problem set includes a tool to translate from the language it
  uses to define problems (a variant of SMTLIB) into various other languages
  including Haskell and Isabelle, for which TE tools already exist.
\item TIP problems are relevant to the domains we are interested in, including
  pure functional programming, higher-order functions and inductively defined
  datatypes. This makes TIP more attractive than problem sets for first-order
  languages/logics such as TPTP~\cite{sutcliffe2009tptp}.
\item Many of TIP's problem statements re-use the same definitions, which makes
  it more useful as a ground-truth than a mixture of unrelated problems. This is
  because non-membership in the ground truth will be treated as being
  ``uninteresting''.
\end{itemize}

All existing theory exploration systems rely on brute force enumeration to some
degree. We focus on \qspec{}~\cite{QuickSpec}, which conjectures equations about
Haskell code. These may be fed into another tool, such as \hspec{}, for
subsequent proving. We have used version 1 of \qspec{} in our experiments, due
to its availability, stability and tooling integration, and we will drop the
version number from now on. At the time of writing there is a \qspec{} version 2
available, which has a much improved generation procedure that is significantly
faster than its predecessor. However, it still scales poorly in our experience,
and has less integration with the tooling we have used. We hence leave analysis
of \qspec{} 2 as future work, pending the necessary infrastructure changes that
would require.

\qspec{} (version 1) is written in the Haskell programming language, and works
with definitions which are also written in Haskell. Some simple Haskell
definitions are shown in Figure~\ref{fig:haskellexample}. The following
procedure is used to generate conjectures, which are both plausible (due to
testing on many examples) and potentially interesting (due to being mutually
irreducible):

\iffalse % TODO{2019-03-15}: Make this more formal?
 V \in Var
 F \in Fun
 T \in Term
 T ::= V | F | T1 T2

 Term ::= VAR | Const | Fun (Term)
or
 Term t ::= x | f | t t'
\fi


\begin{figure}
  \centering
  \begin{minted}{haskell}
    -- A datatype with two constructors
    data Bool = True | False

    -- A recursive datatype (S requires a Nat as argument)
    data Nat = Z | S Nat

    -- A function turning a Bool into a Bool
    not :: Bool -> Bool
    not True  = False
    not False = True

    -- A pair of mutually recursive functions

    odd :: Nat -> Bool
    odd    Z  = False
    odd (S n) = even n

    even :: Nat -> Bool
    even    Z  = True
    even (S n) = odd n
  \end{minted}
  \caption{Haskell datatypes for booleans and natural numbers, followed by some
    simple function definitions (with type annotations).}
  \label{fig:haskellexample}
\end{figure}

\begin{enumerate}
\item Given a typed signature $\Sigma$ and set of variables $V$, \qspec{}
  generates a list $terms$ containing the constants (including functions) from
  $\Sigma$, the variables from $V$ and type-correct function applications
  \hs{f x}, where \hs{f} and \hs{x} are elements of $terms$. To ensure the list
  is finite, function applications are only nested up to a specified depth (by
  default, 3).
\item The elements of $terms$ are grouped into equivalence classes, based on
  their type.
\item The equivalence of terms in each class is tested using \qcheck{}:
  variables are instantiated to particular values, generated randomly, and the
  resulting closed expressions are evaluated and compared for equality.
\item If a class is found to have non-equal members, it is split up to separate
  those members.
\item The previous steps of testing and splitting are repeated until the classes
  stabilise (i.e. no differences have been observed for some specified number of
  repetitions).
\item For each class, one member is selected and equations are conjectured that
  it is equal to each of the other members.
\item A congruence closure algorithm is applied to these equations, to discard
  any which are implied by the others.
\end{enumerate}

As an example, giving the definitions from Figure~\ref{fig:haskellexample} to
\qspec{}, along with suitable comparison functions, random data generators and
variables \hs{n :: Nat} and \hs{b :: Bool}, will give rise to an equivalence
class for each type \hs{Nat}, \hs{Bool}, \hs{Nat -> Nat}, \hs{Nat -> Bool} and
\hs{Bool -> Bool}. Testing will find unequal terms in some of these classes
(such as the \hs{Bool} class containing \hs{True}, \hs{False}, \hs{b},
\hs{not b}, \hs{odd n} and \hs{even n}, which are all mutually unequal), split
them up, and repeat until no more splits are found. Equations are then generated
between elements of non-singleton classes, such as $\{\text{\hs{odd n}},
\text{\hs{not (even n)}}, \text{\hs{not (not (odd n))}}, \ldots\}$, and reduced
to a minimal set (for example, discarding \hs{odd n == not (not (odd n))} as an
instance of the more general \hs{b == not (not b)}). The remaining equations
(including \hs{not (not b) == b} and \hs{not (odd n) == even n}, etc.) are
output, either for the user to peruse or for another system to process, like the
\textsc{HipSpec} automated theorem prover.

Note that we must be able to generate values of a type in order to include
variables of that type. Also, only equivalence classes whose element type can be
compared are split and conjectured about in this way; in particular, functions
(like \hs{odd} and \hs{even}) are first-class values, so they will be put in an
equivalence class, but they will not be subject to testing and comparison since
Haskell has no generic notion of function equality~\footnote{In
  fact \qspec{} compares expressions using a total ordering rather than equality;
  this is even more restrictive than requiring a notion of equality, but reduces
  the number of required comparisons}. \qspec{} can hence conjecture that two
functions are pointwise-equal (e.g. \hs{f x == g x}) but it cannot conjecture
that the functions themselves are equal (e.g. \hs{f == g}).

Although complete, this enumeration approach is wasteful: many terms are
unlikely to appear in theorems, which requires careful choice by the user of
what to include in the signature. For example, we know that addition and
multiplication are closely related, and hence obey many algebraic laws. Our
machine learning technique aims to predict these kinds of relations between
functions, so we can create small signatures which can be explored quickly, yet
have the potential to give rise to many equations.

\iffalse TODO{2019-03-15} In fact, there are similarities between the way a TE
system like \qspec{} can generalise from checking \emph{particular} properties
to \emph{inventing} new ones, and the way counterexample finders like \qcheck{}
can generalise from testing \emph{particular} expressions to \emph{inventing}
expressions to test. One of our aims is to understand the implications of this
generalisation, the lessons that each can learn from the other's approach to
term generation, and the consequences for testing and QA in general.  \fi

\subsection{Clustering}
\label{sec:clustering}

Our approach to scaling up \qspec{} takes inspiration from two sources. The first is relevance filtering, which makes expensive algorithms used in theorem proving more practical by only considering clauses deemed ``relevant'' to the problem \cite{meng2009lightweight}.

Relevance is determined by comparing clauses to the target theorem, but theory exploration does not have such a distinguished term. Instead, we are interested in relationships between \emph{all} terms in a signature, and hence we need a different algorithm for considering the relevance of \emph{all terms} to \emph{all other terms}.

A natural fit for this task is \emph{clustering}, which attempts to group similar inputs together in an unsupervised way. Based on their success in discovering relationships and patterns between expressions in Coq and ACL2 (in the ML4PG and ACL2(ml) tools respectively), we hypothesise that clustering methods can fulfil the role of relevance filters for theory exploration: intelligently breaking up large signatures into smaller ones more amenable to brute force enumeration, such that related expressions are explored together.

Due to its use by ML4PG and ACL2(ml), we use \emph{k-means} clustering, implemented in the Weka tool \cite{Holmes.Donkin.Witten:1994} by Lloyd's algorithm \cite{lloyd1982least}, with randomly-selected input elements as the initial clusters. Rather than relying on the user to provide the number of clusters $k$, we use the ``rule of thumb'' given in \cite[pp. 365]{mardia1979multivariate} of clustering $n$ data points into $k = \lceil \sqrt{\frac{n}{2}} \rceil$ clusters.

\subsection{The Clustering Problem}
\label{sec:clustering}

% TODO{2019-03-15}

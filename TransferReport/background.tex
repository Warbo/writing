\section{Background}
\label{sec:background}

\subsection{Haskell}
\label{sec:haskell}

\begin{figure}
  \begin{equation*}
    \begin{split}
      expr\    \rightarrow\ & \texttt{Var}\ id                                       \\
                         |\ & \texttt{Lit}\ literal                                  \\
                         |\ & \texttt{App}\ expr\ expr                               \\
                         |\ & \texttt{Lam}\ \mathcal{L}\ expr                        \\
                         |\ & \texttt{Let}\ bind\ expr                               \\
                         |\ & \texttt{Case}\ expr\ \mathcal{L}\ \left[ alt \right]   \\
      id\      \rightarrow\ & \texttt{Local}\ \mathcal{L}                            \\
                         |\ & \texttt{Global}\ \mathcal{G}                           \\
      literal\ \rightarrow\ & \texttt{LitNum}\ \mathcal{N}                           \\
                         |\ & \texttt{LitStr}\ \mathcal{S}                           \\
      alt\     \rightarrow\ & ( altcon,\ [\mathcal{L}],\ expr )                      \\
      altcon\  \rightarrow\ & \texttt{DataAlt}\ \mathcal{G}                          \\
                         |\ & \texttt{LitAlt}\ literal                               \\
                         |\ & \texttt{Default}                                       \\
      bind\    \rightarrow\ & \texttt{NonRec}\ \mathcal{L}\ expr                     \\
                         |\ & \texttt{Rec}\ [ ( \mathcal{L},\ expr ) ]
    \end{split}
  \end{equation*}
  Where:
  \begin{tabular}[t]{l @{ $=$ } l}
    $\mathcal{S}$ & string literals    \\
    $\mathcal{N}$ & numeric literals   \\
    $\mathcal{L}$ & local identifiers  \\
    $\mathcal{G}$ & global identifiers
  \end{tabular}

  \caption{Simplified syntax of GHC Core in BNF style. $[]$ and $(,)$ denote repetition and grouping, respectively.}
  \label{fig:coresyntax}
\end{figure}

We decided to focus our theory exploration research on Haskell as it has mature, state-of-the-art implementations (\qspec{} \citep{QuickSpec} and \hspec{} \citep{claessen2013automating}). This is evident from the fact that the state-of-the-art equivalent for Isabelle/HOL, the \textsc{Hipster} \citep{Hipster} system, is actually implemented by translating to Haskell and invoking \hspec{}.

Haskell is a programming language which is well-suited to research; indeed, this was a goal of the language's creators \citep{marlow2010haskell}. Like most members of the \emph{functional programming} paradigm, Haskell is essentially a variant of $\lambda$-calculus; in fact an intermediate representation of the \textsc{GHC} compiler, known as \emph{GHC Core}, is based on a variant of the polymorphic $\lambda$-calculus known as \fc{}. For simplicity, we will focus on this Core language rather than the relatively large and complex syntax of Haskell proper. For a full treatment of \fc{} and its use in GHC, see \citep[Appendix C]{sulzmann2007system}.

The sub-set of Core we consider is shown in figure \ref{fig:coresyntax}; compared to the full language \footnote{As of GHC version 7.10.2, the latest at the time of writing.} our major changes are to erase types and use a custom representation of names. There are also several other forms of literal (machine words of various sizes, individual characters, etc.) which we omit for brevity, as their treatment is similar to those of strings and numerals.

\subsection{QuickCheck}
\label{sec:quickcheck}

Although unit testing is the de facto industry standard for quality assurance in non-critical systems, the level of confidence it provides is rather low, and totally inadequate for many (e.g. life-) critical systems. To see why, consider the following Haskell function, along with some unit tests (see \ref{sec:haskell} for more details on Haskell):

\begin{lstlisting}[language=Haskell, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth]
factorial 0 = 1
factorial n = n * factorial (n-1)

fact_base      = factorial 0 == factorial 1
fact_increases = factorial 3 <= factorial 4
fact_div       = factorial 4 == factorial 5 `div` 5
\end{lstlisting}

The intent of the function is to map an input $n$ to an output $n!$. The tests check a few properties of the implementation, including the base case, that the function is monotonically increasing, and a relationship between adjacent outputs. However, these tests will \emph{not} expose a serious problem with the implementation: it diverges on half of its possible inputs!

All of Haskell's built-in numeric types allow negative numbers, which this implementation doesn't take into account. Whilst this is a rather trivial example, it highlights a common problem: unit tests are insufficient to expose incorrect assumptions. In this case, our assumption that numbers are positive has caused a bug in the implementation \emph{and} limited the tests we've written.

If we do manage to spot this error, we might capture it in a \emph{regression test} and update the definition of \hs{factorial} to handle negative numbers, e.g. by taking their absolute value:

\begin{lstlisting}[language=Haskell, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth]
factorial 0 = 1
factorial n = let nPos = abs n
               in nPos * factorial (nPos - 1)

fact_neg = factorial 1 == factorial (-1)
\end{lstlisting}

However, this is \emph{still} not enough, since this function will also accept fractional values\footnote{Since we only use generic numeric operations, the function will be polymorphic with a type of the form \hs{forall t. Num t => t -> t}, where \hs{Num t} constrains the type variable \hs{t} to be numeric. In fact, Haskell will infer extra constraints such as \hs{Eq t} since we have used \hs{==} in the unit tests.}, which will also cause it to diverge. Clearly, by choosing what to test we are biasing the test suite towards those cases we've already taken into account, whilst neglecting the problems we did not expect.

Haskell offers a partial solution to this problem in the form of \emph{property checking}. Tools such as \qcheck{} separate tests into three components: a \emph{property} to check, which unlike a unit test may contain \emph{free variables}; a source of values to instantiate these free variables; and a stopping criterion. Here is how we might restate our unit tests as properties:

\begin{lstlisting}[language=Haskell, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth]
fact_base        = factorial 0 == factorial 1
fact_increases n = factorial n <= factorial (n+1)
fact_div       n = factorial n == factorial (n+1) `div` (n+1)
fact_neg       n = factorial n == factorial (-n)
\end{lstlisting}

The free variables (all called \hs{n} in this case) are abstracted as function parameters; these parameters are implicitly \emph{universally quantified}, i.e. we've gone from a unit test asserting $factorial(3) \leq factorial(4)$ to a property asserting $\forall n, factorial(n) \leq factorial(n+1)$. Notice that unit tests like \hs{fact_base} are valid properties; they just assert rather weak statements.

To check these properties, \qcheck{} treats closed terms (like \hs{fact_base}) just like unit tests: pass if they evaluate to \hs{True}, fail otherwise. For open terms, a random selection of values are generated and passed in via the function parameter; the results are then treated in the same way as closed terms. The default stopping criterion for \qcheck{} (for each test) is when a single generated test fails, or when 100 generated tests pass.

The ability to state \emph{universal} properties in this way avoids some of the bias we encountered with unit tests. In the \hs{factorial} example, this manifests in two ways:

\begin{itemize}
  \item \qcheck{} cannot test polymorphic functions; they must be \emph{monomorphised} first (instantiated to a particular concrete type). This is a technical limitation, since \qcheck{} must know which type of values to generate, but in our example it would bring the issue with fractional values to our attention.

  \item The generators used by \qcheck{} depend only on the \emph{type} of value they are generating: since \hs{Int} includes positive and negative values, the \hs{Int} generator will output both. This will expose the problem with negative numbers, which we weren't expecting.
\end{itemize}

Property checking is certainly an improvement over unit testing, but the problem of tests being biased towards expected cases remains, since we are manually specifying the properties to be checked.

We can reduce this bias further through the use of \emph{theory exploration} tools, such as \qspec{} and \hspec{}. These programs \emph{discover} properties of a ``theory'' (e.g. a library), through a combination of brute-force enumeration, random testing and (in the case of \hspec{}) automated theorem proving.\footnote{See section \ref{sec:atp} for more information on automated theorem proving.}

\subsection{Theory Exploration}
\label{sec:theoryexploration}

\begin{figure}
  \begin{equation*}
    \begin{split}
      names\       =\ & \{ N_1, \dots, N_n \}                                                                  \\
      types\       =\ & \{ T_1, \dots, T_m \} \cup \{a \rightarrow b \mid a \in types \wedge b \in types \} \\
      exprs\       =\ & names \cup \{ f\ x \mid f \in exprs \wedge x \in exprs \wedge f : a \rightarrow b \wedge x : a \} \\
      classes\     =\ & \{c \mid \forall x y \in c, x \in exprs \wedge y \in exprs \wedge x =_{QC} y \} \\
      conjectures\ =\ & \bigcup_{c \in classes}\{ \ulcorner a = b \urcorner \mid a \in c \wedge b \in c \}
    \end{split}
  \end{equation*}

  \caption{General model of \qspec{} conjecture generation, given a signature $\Sigma$ containing names $N_1$ to $N_n$ and types $T_1$ to $T_m$. We use $a =_{QC} b$ to denote that $a$ and $b$ are indistinguishable by \qcheck{}, and use $\ulcorner \dots \urcorner$ for quasiquotation.}
  \label{fig:qspecmodel}
\end{figure}

In this work we focus on the problem of \emph{(automated) theory exploration}, which includes the ability to \emph{generate} conjectures about code, to \emph{prove} those conjectures, and hence output \emph{novel} theorems without guidance from the user. In \citep{warburtonscaling} we identify the method of conjecture generation as a key characteristic of any theory exploration system, and in particular lament the existing reliance on brute force methods.

We focus on \qspec{} \citep{QuickSpec}, which discovers equations about Haskell code. These are found through the process modelled in figure \ref{fig:qspecmodel}: \iffalse TODO: Figure just shows the syntax, it does not model any process \fi given a typed signature $\Sigma$, \qspec{} enumerates all type-correct combinations of terms from $\Sigma$ up to some depth; it then groups them into equivalence classes using the \qcheck{} counterexample finder, then conjectures equations relating the members of these classes.

The resulting set of $conjectures$ can be used in several ways: it can be simplified for direct presentation to the user, sent to a more rigorous system like \hspec{} and \textsc{Hipster} for proving, or even serve as a background theory for an automated theorem prover \citep{claessen2013automating}.

\qspec{} (and \hspec{}) is also compatible with Haskell's existing testing infrastructure, such that an invocation of \texttt{cabal test} can run these tools alongside more traditional QA tools like \qcheck{}, \textsc{HUnit} and \textsc{Criterion}.

In fact, there are similarities between the way a TE system like \qspec{} can generalise from checking \emph{particular} properties to \emph{inventing} new ones, and the way counterexample finders like \qcheck{} can generalise from testing \emph{particular} expressions to \emph{inventing} expressions to test. One of our aims is to understand the implications of this generalisation, the lessons that each can learn from the other's approach to term generation, and the consequences for testing and QA in general.

We attempt to mitigate the cost of running \qspec{}, by providing a machine learning layer analogous to that of premise selection in theorem proving. Of particular concern is the set of expressions $exprs$, which grows exponentially as the size of the expressions increases, and hence slows down the rest of the algorithm. Although complete, the current enumeration approach is wasteful: many combinations are unlikely to appear in theorems, which requires careful choice by the user of what to include in the signature. The key idea of premise selection is to make such choices automatically, by only including those expressions which are considered \emph{relevant} to the problem.

\iffalse TODO: Not immediately clear how the problem of reducing the set of expressions relates to premise selection \fi
\iffalse TODO: This detailed comparison with premise selection is opaque without definiing/explaining what premise selection is \fi

\subsection{Clustering and Feature Extraction}
\label{sec:featureextraction}

Our approach to scaling up these Haskell theory exploration tools takes inspiration from two sources. The first is premise selection, which makes expensive algorithms used in theorem proving more practical by limiting the size of their inputs. We describe this approach in more details in \S \ref{sec:relevance}. Premise selection is a practical tool, used in production systems like the \emph{Sledgehammer} component of the Isabelle/HOL theorem prover.

Despite the idea's promise, we cannot simply invoke existing premise selection algorithms in our theory exploration setting. The reason is that theory exploration has no distinct \emph{goal} to compare expressions against. Instead, we are interested in relationships between \emph{any} components of a theory, and hence we must consider the relevance of \emph{everything} to \emph{everything else}. \iffalse TODO: every term in the given signature? \fi A natural fit for such problems is \emph{clustering}, which attempts to group similar inputs together in an unsupervised way. See \S \ref{sec:clustering} for more information.

Our hypothesis is that clustering methods, as found in systems like ML4PG and ACL2(ml), can be used as relevance filters to break up large signatures into smaller sets more amenable to brute force enumeration.

\iffalse TODO: Signatures or terms? \fi

\iffalse TODO: I would play up the clustering intelligence. Clustering is not just about breaking up, it is about discovering significant patterns in data. By forgetting this, you make motivation for your work sound too ``small'' \fi

Both of these approaches, premise selection and clustering, use \emph{machine learning} (ML) algorithms to analyse (logical or software) expressions, and hence rely on \emph{feature extraction} to transform the data into a suitable representation and remove irrelevant details. This has two benefits:

\begin{itemize}
  \item \emph{Feature vectors} (ordered sets of features) are chosen to be more compact than the data they're extracted from: feature extraction is \emph{lossy compression}. This reduces the size of the machine learning problem, improving efficiency (e.g. running time).
  \item We avoid learning irrelevant details, such as the text encoding system used, improving \emph{data} efficiency (the number of samples required to spot a pattern).
\end{itemize}

Another benefit of feature extraction is to \emph{normalise} the input data to a fixed-size representation. Many ML algorithms only work with inputs of a uniform size; feature extraction allows us to use these algorithms in domains where the size of each input is not known, may vary or may even be unbounded.

As an example, say we want to learn relationships between the following program fragments:

\begin{haskell}
data Maybe a = Nothing | Just a

data Either a b = Left a | Right b
\end{haskell}

We might hope our algorithm discovers relationships like:

\begin{itemize}
  \item Both are valid Haskell code.
  \item Both describe datatypes.
  \item Both datatypes have two constructors.
  \item \hs{Either} is a generalisation of \hs{Maybe} (we can define \hs{Maybe a = Either () a} and \hs{Nothing = Left ()}).
  \item There is a symmetry in \hs{Either}: \hs{Either a b} is equivalent to \hs{Either b a} if we swap occurences of \hs{Left} and \hs{Right}.
  \item It is trivial to satisfy \hs{Maybe} (using \hs{Nothing}).
  \item It is not trivial to satisfy \hs{Either}; we require an \hs{a} or a \hs{b}.
\end{itemize}

However, this is too optimistic. Without our domain-knowledge of Haskell, an ML algorithm cannot impose any structure on these fragments, and will treat them as strings of bits. Our high-level hopes are obscured by low-level details: the desirable patterns of Haskell types are mixed with undesirable patterns of ASCII bytes, of letter frequency in English words, and so on.

In theory we could throw more computing resources and data at a problem, but available hardware and corpora are always limited. Instead, feature extraction lets us narrow the ML problem to what we, with our domain knowlege, consider important.

There is no \emph{fundamental} difference between raw representations and features: the identity function is a valid feature extractor. Likewise, there is no crisp distinction between feature extraction and machine learning: a sufficiently-powerful learner doesn't require feature extraction, and a sufficiently-powerful feature extractor doesn't require any learning! \footnote{Consider a classification problem, to assign a label $l \in L$ to each input. If we only extract a single feature $f \in L$, we have solved the classification problem without using a separate learning step.}

Rather, the terms are distinguished for purely \emph{practical} reasons: by separating feature extraction from learning, we can distinguish straightforward, fast data transformation (feature extraction) from complex, slow statistical analysis (learning). This allows for modularity, separation of concerns, and in particular allows ``off-the-shelf'' ML to be re-used across a variety of different domains.

In our case, we will use a novel feature extraction algorithm, described in \S \ref{sec:recurrentclustering}, to transform expressions in Haskell Core into a fixed size representation, suitable for clustering via the standard \emph{k-means} algorithm.

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.75, level/.style={sibling distance=60mm/#1}]
      % Tree
      \node {\texttt{Lam}}
        child {node {\texttt{"x"}}}
        child {node {\texttt{App}}
          child {node {\texttt{Var}}
            child {node {\texttt{Global}}
              child {node {\texttt{"base:Prelude.not"}}}}}
          child {node {\texttt{Var}}
            child {node {\texttt{Local}}
              child {node {\texttt{"x"}}}}}};

      % Arrow
      %\node[single arrow,
      %      draw=black,
      %      fill=black!10,
      %      minimum height=2cm,
      %      shape border rotate=0] at (0,-1) {};
      %\draw[-latex] (A.east) -- (B.west);

      % Feature vector

  \end{tikzpicture}

  \caption{}

  \label{fig:featureextractionpic}
\end{figure}

\iffalse
\subsection{Clustering}
% TODO
\fi
